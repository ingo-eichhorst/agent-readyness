---
phase: 03-scoring-model
plan: 02
type: tdd
wave: 2
depends_on: ["03-01"]
files_modified:
  - internal/scoring/scorer.go
  - internal/scoring/scorer_test.go
autonomous: true

must_haves:
  truths:
    - "C1 category score correctly extracts and scores all 6 C1 metrics from AnalysisResult"
    - "C3 category score correctly extracts and scores all 5 C3 metrics from AnalysisResult"
    - "C6 category score correctly extracts and scores all 5 C6 metrics including unavailable coverage handling"
    - "Missing coverage (CoveragePercent == -1) excludes coverage sub-score and redistributes weight"
    - "Coupling maps are aggregated to averages before scoring"
    - "Custom config thresholds are used when provided to Scorer"
  artifacts:
    - path: "internal/scoring/scorer.go"
      provides: "Score method, scoreC1, scoreC3, scoreC6, extractors"
      exports: ["Score"]
  key_links:
    - from: "internal/scoring/scorer.go"
      to: "pkg/types/types.go"
      via: "type assertions on C1Metrics, C3Metrics, C6Metrics"
      pattern: "raw\\.\\(\\*types\\.C[136]Metrics\\)"
    - from: "internal/scoring/scorer.go"
      to: "internal/scoring/config.go"
      via: "reads MetricThresholds from ScoringConfig categories"
      pattern: "s\\.Config\\.C[136]\\.Metrics"
---

<objective>
Implement the category scoring methods that extract raw metrics from AnalysisResult and compute per-category scores with sub-score breakdowns.

Purpose: This bridges the gap between raw analyzer output (C1Metrics, C3Metrics, C6Metrics) and the scored result. Each category scorer must correctly extract the right field from the metrics struct, handle edge cases (missing coverage, empty coupling maps, zero-division), and produce sub-scores that roll up into the category score.

Output: `Scorer.Score()` method and `scoreC1`, `scoreC3`, `scoreC6` methods with full test coverage against known metric inputs.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/03-scoring-model/03-RESEARCH.md
@.planning/phases/03-scoring-model/03-01-SUMMARY.md
@pkg/types/types.go
@pkg/types/scoring.go
@internal/scoring/config.go
@internal/scoring/scorer.go
</context>

<feature>
  <name>Category Scorers: C1, C3, C6 metric extraction and scoring</name>
  <files>
    internal/scoring/scorer.go
    internal/scoring/scorer_test.go
  </files>
  <behavior>
    The `Scorer.Score(results []*types.AnalysisResult) (*types.ScoredResult, error)` method:
    1. Iterates over AnalysisResult slice
    2. For each result, dispatches to scoreC1/scoreC3/scoreC6 based on Category field
    3. Collects CategoryScores
    4. Computes composite via computeComposite (from Plan 01)
    5. Classifies tier via classifyTier (from Plan 01)
    6. Returns ScoredResult

    **scoreC1(ar *types.AnalysisResult) types.CategoryScore:**
    Extracts `*types.C1Metrics` from `ar.Metrics["c1"]` via type assertion.
    Maps metrics to config metric names:
    - "cyclomatic_complexity_avg" -> m.CyclomaticComplexity.Avg
    - "func_length_avg" -> m.FunctionLength.Avg
    - "file_size_avg" -> m.FileSize.Avg
    - "afferent_coupling_avg" -> average of values in m.AfferentCoupling map (0 if empty)
    - "efferent_coupling_avg" -> average of values in m.EfferentCoupling map (0 if empty)
    - "duplication_rate" -> m.DuplicationRate

    For coupling maps: sum all values / len(map). If map is nil or empty, raw value = 0.

    **scoreC3(ar *types.AnalysisResult) types.CategoryScore:**
    Extracts `*types.C3Metrics` from `ar.Metrics["c3"]`.
    Maps:
    - "max_dir_depth" -> float64(m.MaxDirectoryDepth)
    - "module_fanout_avg" -> m.ModuleFanout.Avg
    - "circular_deps" -> float64(len(m.CircularDeps))
    - "import_complexity_avg" -> m.ImportComplexity.Avg
    - "dead_exports" -> float64(len(m.DeadExports))

    **scoreC6(ar *types.AnalysisResult) types.CategoryScore:**
    Extracts `*types.C6Metrics` from `ar.Metrics["c6"]`.
    Maps:
    - "test_to_code_ratio" -> m.TestToCodeRatio
    - "coverage_percent" -> m.CoveragePercent (mark Available=false if == -1)
    - "test_isolation" -> m.TestIsolation
    - "assertion_density_avg" -> m.AssertionDensity.Avg
    - "test_file_ratio" -> float64(m.TestFileCount) / float64(m.SourceFileCount) (0 if SourceFileCount == 0)

    Each scoreXX method:
    1. Extracts metrics struct via type assertion, returns zero-score CategoryScore on failure
    2. Builds a map[string]float64 of metric name -> raw value
    3. For each MetricThresholds in config category, looks up raw value by name
    4. Calls Interpolate(thresholds.Breakpoints, rawValue) to get sub-score
    5. Creates SubScore with RawValue, Score, Weight, Available
    6. Calls categoryScore(subScores) for the weighted average
    7. Returns CategoryScore with Name, Score, Weight (from config), SubScores

    Test cases:
    - Healthy codebase: low complexity, good coverage -> high scores (>8)
    - Poor codebase: high complexity, no tests -> low scores (<4)
    - Missing coverage: CoveragePercent == -1 -> coverage sub-score excluded, other weights redistributed
    - Empty coupling maps -> raw value 0 -> high coupling score
    - Zero source files -> test_file_ratio = 0 (not division by zero)
    - Full round-trip: Score() with 3 AnalysisResults -> ScoredResult with composite and tier
    - Custom config: scoreC1 with non-default MetricThresholds verifies that s.Config.C1.Metrics breakpoints are used (not hardcoded defaults). Create a ScoringConfig with modified C1 complexity breakpoints (e.g., [{1,1},{5,5},{10,10}]) and verify the interpolated score differs from what DefaultConfig would produce for the same raw value.
  </behavior>
  <implementation>
    Add to internal/scoring/scorer.go:

    1. `Score(results []*types.AnalysisResult) (*types.ScoredResult, error)` -- public method
    2. `scoreC1(ar *types.AnalysisResult) types.CategoryScore` -- private
    3. `scoreC3(ar *types.AnalysisResult) types.CategoryScore` -- private
    4. `scoreC6(ar *types.AnalysisResult) types.CategoryScore` -- private
    5. Helper: `avgMapValues(m map[string]int) float64` -- averages int map values, returns 0 for nil/empty
    6. Helper: `findMetric(metrics []MetricThresholds, name string) *MetricThresholds` -- finds metric config by name
    7. Helper: `scoreMetrics(catConfig CategoryConfig, rawValues map[string]float64, unavailable map[string]bool) ([]types.SubScore, float64)` -- generic scoring for any category

    The `scoreMetrics` helper is key to avoiding duplication across scoreC1/C3/C6:
    - Takes the CategoryConfig, a map of raw values, and a set of unavailable metric names
    - For each metric in config: if unavailable, set Available=false and skip; else interpolate and add to sub-scores
    - Compute weighted average of available sub-scores
    - Return sub-scores and category score

    Write RED tests first against these exact inputs, then implement GREEN.
  </implementation>
</feature>

<verification>
- `cd /Users/ingo/agent-readyness && go test ./internal/scoring/... -v` -- all tests pass including new category scorer tests
- `cd /Users/ingo/agent-readyness && go build ./...` -- compiles cleanly
- Test covers: healthy metrics -> high score, poor metrics -> low score, missing coverage handling, empty coupling maps, zero-division guards, full Score() round-trip, custom config wiring
</verification>

<success_criteria>
- `Scorer.Score()` accepts `[]*types.AnalysisResult` and returns `*types.ScoredResult` with all categories scored
- C1 coupling maps are averaged correctly (not summed, not maxed)
- C6 coverage == -1 results in coverage sub-score being excluded (Available=false) with weight redistributed
- C6 test_file_ratio handles zero source files without panic
- Full round-trip test: 3 AnalysisResults in -> ScoredResult with composite score and tier rating out
- Custom config test: Scorer initialized with modified breakpoints produces different scores than DefaultConfig for same raw values, proving config wiring through s.Config.C1.Metrics
- No import of internal/analyzer -- only uses pkg/types
</success_criteria>

<output>
After completion, create `.planning/phases/03-scoring-model/03-02-SUMMARY.md`
</output>
