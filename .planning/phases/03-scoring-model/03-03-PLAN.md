---
phase: 03-scoring-model
plan: 03
type: execute
wave: 3
depends_on: ["03-02"]
files_modified:
  - internal/pipeline/pipeline.go
  - internal/pipeline/pipeline_test.go
  - internal/output/terminal.go
  - cmd/scan.go
  - cmd/root.go
autonomous: true

must_haves:
  truths:
    - "User sees per-category scores (1-10) and composite score with tier rating after running scan"
    - "User sees per-metric sub-score breakdown when running scan with --verbose"
    - "User can override scoring thresholds with --config path.yaml"
    - "Scores are consistent without --config (built-in defaults apply automatically)"
  artifacts:
    - path: "internal/pipeline/pipeline.go"
      provides: "Scoring stage between analyze and output"
      contains: "scoring.Scorer"
    - path: "internal/output/terminal.go"
      provides: "Score rendering with tier badge and sub-score breakdown"
      contains: "RenderScores"
    - path: "cmd/scan.go"
      provides: "--config flag for YAML threshold override"
      contains: "config"
  key_links:
    - from: "internal/pipeline/pipeline.go"
      to: "internal/scoring/scorer.go"
      via: "Scorer.Score() call in pipeline Run"
      pattern: "scorer\\.Score"
    - from: "internal/pipeline/pipeline.go"
      to: "internal/output/terminal.go"
      via: "passes ScoredResult to output renderer"
      pattern: "RenderScores.*ScoredResult"
    - from: "cmd/scan.go"
      to: "internal/scoring/config.go"
      via: "LoadConfig for --config flag"
      pattern: "scoring\\.LoadConfig"
---

<objective>
Wire the scoring model into the pipeline and add score rendering to terminal output, completing Phase 3.

Purpose: The scoring types and math exist but are not connected to the running application. This plan adds the scoring stage to the pipeline (between analyze and output), renders scores in the terminal, and adds the --config flag for YAML threshold overrides.

Output: Running `ars scan <dir>` displays per-category scores, composite score, and tier rating. `--verbose` shows sub-score breakdowns. `--config` allows custom thresholds.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/03-scoring-model/03-RESEARCH.md
@.planning/phases/03-scoring-model/03-01-SUMMARY.md
@.planning/phases/03-scoring-model/03-02-SUMMARY.md
@internal/pipeline/pipeline.go
@internal/output/terminal.go
@cmd/scan.go
@cmd/root.go
@internal/scoring/scorer.go
@internal/scoring/config.go
@pkg/types/scoring.go
</context>

<tasks>

<task type="auto">
  <name>Task 1: Pipeline scoring stage and config flag</name>
  <files>
    internal/pipeline/pipeline.go
    internal/pipeline/pipeline_test.go
    internal/scoring/config.go
    cmd/scan.go
    cmd/root.go
  </files>
  <action>
    **Pipeline changes (internal/pipeline/pipeline.go):**
    1. Add `scorer *scoring.Scorer` field to Pipeline struct
    2. Add `scored *types.ScoredResult` field to Pipeline struct
    3. Update `New()` to accept a `*scoring.ScoringConfig` parameter and create `scoring.Scorer{Config: cfg}`
    4. In `Run()`, add Stage 3.5 between analyze (Stage 3) and output (Stage 4):
       ```
       // Stage 3.5: Score results
       scored, err := p.scorer.Score(p.results)
       if err != nil {
           return fmt.Errorf("scoring: %w", err)
       }
       p.scored = scored
       ```
    5. Pass `p.scored` to the output render function (update RenderSummary call signature or add separate RenderScores call)

    **Config loading (internal/scoring/config.go):**
    Add `LoadConfig(path string) (*ScoringConfig, error)` function:
    - If path is empty, return DefaultConfig()
    - Read file, unmarshal YAML into a fresh ScoringConfig (NOT into DefaultConfig -- YAML should fully replace, not merge, to avoid partial override confusion)
    - Actually, per research: unmarshal INTO DefaultConfig so missing fields keep defaults. This is the standard Go pattern. Use `cfg := DefaultConfig(); yaml.Unmarshal(data, cfg)`.
    - Add `gopkg.in/yaml.v3` to go.mod: `go get gopkg.in/yaml.v3@v3.0.1`
    - Add yaml struct tags to Breakpoint, MetricThresholds, CategoryConfig, ScoringConfig, TierConfig

    **Pipeline test (internal/pipeline/pipeline_test.go):**
    Add test that verifies the pipeline scoring stage runs:
    - Create a minimal pipeline with DefaultConfig
    - Verify that after Run(), the scored result is populated (not nil)
    - Verify scored result contains expected categories

    **CLI changes (cmd/scan.go):**
    1. Add `var configPath string` package variable
    2. In `init()`, add flag: `scanCmd.Flags().StringVar(&configPath, "config", "", "path to scoring config YAML file")`
    3. In RunE, load config: `cfg, err := scoring.LoadConfig(configPath)` (handle error)
    4. Pass cfg to `pipeline.New(cmd.OutOrStdout(), verbose, cfg)`

    **cmd/root.go** -- only if verbose flag is defined there, ensure it's accessible. Check current code.

    Import scoring package in pipeline.go and cmd/scan.go.
  </action>
  <verify>
    - `cd /Users/ingo/agent-readyness && go build ./...` compiles cleanly
    - `cd /Users/ingo/agent-readyness && go test ./internal/pipeline/... -v` passes including new scoring stage test
    - `cd /Users/ingo/agent-readyness && go test ./internal/scoring/... -v` passes (config_test should test LoadConfig)
  </verify>
  <done>
    Pipeline runs scoring stage after analysis, verified by automated test. `--config` flag accepted. LoadConfig returns defaults when no path given, parses YAML when path provided. Pipeline test confirms scored result is populated after Run().
  </done>
</task>

<task type="auto">
  <name>Task 2: Terminal score rendering and verbose sub-score breakdown</name>
  <files>
    internal/output/terminal.go
  </files>
  <action>
    Update `internal/output/terminal.go` to render scoring results.

    **Option A (preferred):** Add a new function `RenderScores(w io.Writer, scored *types.ScoredResult, verbose bool)` called from the pipeline after RenderSummary. This keeps scoring output separate from metric output.

    **Score output format (non-verbose):**
    ```
    [blank line]
    Agent Readiness Score
    ════════════════════════════════════════
      C1: Code Health           7.2 / 10
      C3: Architecture          8.1 / 10
      C6: Testing               6.5 / 10
    ────────────────────────────────────────
      Composite Score:          7.1 / 10
      Rating:                   Agent-Assisted
    ```

    Color coding for category scores:
    - >= 8.0: green
    - >= 6.0: yellow
    - < 6.0: red

    Color coding for composite score: same thresholds.

    Tier badge colors:
    - Agent-Ready: green + bold
    - Agent-Assisted: yellow + bold
    - Agent-Limited: red + bold
    - Agent-Hostile: red + bold

    **Verbose sub-score breakdown (appended after each category line when --verbose):**
    ```
      C1: Code Health           7.2 / 10
        Complexity avg:      8.5  ->  7.0  (25%)
        Func length avg:     22.3 ->  8.1  (20%)
        File size avg:       180  ->  8.5  (15%)
        Afferent coupling:   3.2  ->  7.5  (15%)
        Efferent coupling:   2.1  ->  8.0  (10%)
        Duplication rate:    4.2% ->  6.5  (15%)
    ```

    For unavailable sub-scores (e.g., coverage when -1):
    ```
        Coverage:            n/a         (30%, excluded)
    ```

    Use double-line (=) separator for the score section header to visually distinguish it from the metric sections above which use single-line (-).

    Update RenderSummary signature or add RenderScores as a separate exported function. If separate, update pipeline.go to call both RenderSummary and RenderScores.

    Format sub-score raw values: use %.1f for floats, %.0f for percentages, %d for counts. Align columns for readability.
  </action>
  <verify>
    - `cd /Users/ingo/agent-readyness && go build ./...` compiles
    - `cd /Users/ingo/agent-readyness && go test ./... -count=1` all tests pass
  </verify>
  <done>
    Running `ars scan <dir>` shows per-category scores (1-10), composite score, and tier rating in a clearly formatted section. `--verbose` adds per-metric sub-score breakdown showing raw value, interpolated score, and weight percentage. Unavailable metrics shown as "n/a (excluded)".
  </done>
</task>

</tasks>

<verification>
End-to-end verification:
1. `cd /Users/ingo/agent-readyness && go build ./...` -- compiles cleanly
2. `cd /Users/ingo/agent-readyness && go test ./... -count=1` -- all tests pass
3. `cd /Users/ingo/agent-readyness && go test ./internal/pipeline/... -v` -- pipeline scoring stage test passes
4. Verify SCORE-01 through SCORE-06 requirements:
   - SCORE-01: Per-category 1-10 scores displayed (check)
   - SCORE-02: Composite with weighted average and tier (check)
   - SCORE-03: Tier rating assigned (check)
   - SCORE-04: Piecewise linear interpolation used (verified in Plan 01 via Interpolate tests)
   - SCORE-05: Verbose shows per-metric breakdown (check)
   - SCORE-06: Thresholds configurable via --config YAML (check)
</verification>

<success_criteria>
- `go build ./...` compiles cleanly with scoring wired into pipeline
- `go test ./... -count=1` all tests pass including new pipeline scoring stage test
- Pipeline test verifies scored result is non-nil and contains expected categories after Run()
- `--config path/to/config.yaml` overrides default thresholds
- No config file -> defaults used seamlessly
- All existing tests continue to pass
- Pipeline error handling: scoring errors produce warnings, do not crash
</success_criteria>

<output>
After completion, create `.planning/phases/03-scoring-model/03-03-SUMMARY.md`
</output>
