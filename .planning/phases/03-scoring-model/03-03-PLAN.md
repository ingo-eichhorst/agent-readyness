---
phase: 03-scoring-model
plan: 03
type: execute
wave: 3
depends_on: ["03-02"]
files_modified:
  - internal/pipeline/pipeline.go
  - internal/output/terminal.go
  - cmd/scan.go
  - cmd/root.go
autonomous: true

must_haves:
  truths:
    - "Running ars scan shows per-category scores (1-10) and composite score with tier rating"
    - "Running ars scan --verbose shows per-metric sub-score breakdown for each category"
    - "Scoring thresholds are configurable via --config flag pointing to a YAML file"
    - "Default scoring works with no config file (built-in defaults)"
  artifacts:
    - path: "internal/pipeline/pipeline.go"
      provides: "Scoring stage between analyze and output"
      contains: "scoring.Scorer"
    - path: "internal/output/terminal.go"
      provides: "Score rendering with tier badge and sub-score breakdown"
      contains: "RenderScores"
    - path: "cmd/scan.go"
      provides: "--config flag for YAML threshold override"
      contains: "config"
  key_links:
    - from: "internal/pipeline/pipeline.go"
      to: "internal/scoring/scorer.go"
      via: "Scorer.Score() call in pipeline Run"
      pattern: "scorer\\.Score"
    - from: "internal/pipeline/pipeline.go"
      to: "internal/output/terminal.go"
      via: "passes ScoredResult to output renderer"
      pattern: "RenderScores.*ScoredResult"
    - from: "cmd/scan.go"
      to: "internal/scoring/config.go"
      via: "LoadConfig for --config flag"
      pattern: "scoring\\.LoadConfig"
---

<objective>
Wire the scoring model into the pipeline and add score rendering to terminal output, completing Phase 3.

Purpose: The scoring types and math exist but are not connected to the running application. This plan adds the scoring stage to the pipeline (between analyze and output), renders scores in the terminal, and adds the --config flag for YAML threshold overrides.

Output: Running `ars scan <dir>` displays per-category scores, composite score, and tier rating. `--verbose` shows sub-score breakdowns. `--config` allows custom thresholds.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/03-scoring-model/03-RESEARCH.md
@.planning/phases/03-scoring-model/03-01-SUMMARY.md
@.planning/phases/03-scoring-model/03-02-SUMMARY.md
@internal/pipeline/pipeline.go
@internal/output/terminal.go
@cmd/scan.go
@cmd/root.go
@internal/scoring/scorer.go
@internal/scoring/config.go
@pkg/types/scoring.go
</context>

<tasks>

<task type="auto">
  <name>Task 1: Pipeline scoring stage and config flag</name>
  <files>
    internal/pipeline/pipeline.go
    internal/scoring/config.go
    cmd/scan.go
    cmd/root.go
  </files>
  <action>
    **Pipeline changes (internal/pipeline/pipeline.go):**
    1. Add `scorer *scoring.Scorer` field to Pipeline struct
    2. Add `scored *types.ScoredResult` field to Pipeline struct
    3. Update `New()` to accept a `*scoring.ScoringConfig` parameter and create `scoring.Scorer{Config: cfg}`
    4. In `Run()`, add Stage 3.5 between analyze (Stage 3) and output (Stage 4):
       ```
       // Stage 3.5: Score results
       scored, err := p.scorer.Score(p.results)
       if err != nil {
           return fmt.Errorf("scoring: %w", err)
       }
       p.scored = scored
       ```
    5. Pass `p.scored` to the output render function (update RenderSummary call signature or add separate RenderScores call)

    **Config loading (internal/scoring/config.go):**
    Add `LoadConfig(path string) (*ScoringConfig, error)` function:
    - If path is empty, return DefaultConfig()
    - Read file, unmarshal YAML into a fresh ScoringConfig (NOT into DefaultConfig -- YAML should fully replace, not merge, to avoid partial override confusion)
    - Actually, per research: unmarshal INTO DefaultConfig so missing fields keep defaults. This is the standard Go pattern. Use `cfg := DefaultConfig(); yaml.Unmarshal(data, cfg)`.
    - Add `gopkg.in/yaml.v3` to go.mod: `go get gopkg.in/yaml.v3@v3.0.1`
    - Add yaml struct tags to Breakpoint, MetricThresholds, CategoryConfig, ScoringConfig, TierConfig

    **CLI changes (cmd/scan.go):**
    1. Add `var configPath string` package variable
    2. In `init()`, add flag: `scanCmd.Flags().StringVar(&configPath, "config", "", "path to scoring config YAML file")`
    3. In RunE, load config: `cfg, err := scoring.LoadConfig(configPath)` (handle error)
    4. Pass cfg to `pipeline.New(cmd.OutOrStdout(), verbose, cfg)`

    **cmd/root.go** -- only if verbose flag is defined there, ensure it's accessible. Check current code.

    Import scoring package in pipeline.go and cmd/scan.go.
  </action>
  <verify>
    - `cd /Users/ingo/agent-readyness && go build ./...` compiles cleanly
    - `cd /Users/ingo/agent-readyness && go test ./internal/pipeline/... -v` passes
    - `cd /Users/ingo/agent-readyness && go test ./internal/scoring/... -v` passes (config_test should test LoadConfig)
  </verify>
  <done>
    Pipeline runs scoring stage after analysis. `--config` flag accepted. LoadConfig returns defaults when no path given, parses YAML when path provided.
  </done>
</task>

<task type="auto">
  <name>Task 2: Terminal score rendering and verbose sub-score breakdown</name>
  <files>
    internal/output/terminal.go
  </files>
  <action>
    Update `internal/output/terminal.go` to render scoring results.

    **Option A (preferred):** Add a new function `RenderScores(w io.Writer, scored *types.ScoredResult, verbose bool)` called from the pipeline after RenderSummary. This keeps scoring output separate from metric output.

    **Score output format (non-verbose):**
    ```
    [blank line]
    Agent Readiness Score
    ════════════════════════════════════════
      C1: Code Health           7.2 / 10
      C3: Architecture          8.1 / 10
      C6: Testing               6.5 / 10
    ────────────────────────────────────────
      Composite Score:          7.1 / 10
      Rating:                   Agent-Assisted
    ```

    Color coding for category scores:
    - >= 8.0: green
    - >= 6.0: yellow
    - < 6.0: red

    Color coding for composite score: same thresholds.

    Tier badge colors:
    - Agent-Ready: green + bold
    - Agent-Assisted: yellow + bold
    - Agent-Limited: red + bold
    - Agent-Hostile: red + bold

    **Verbose sub-score breakdown (appended after each category line when --verbose):**
    ```
      C1: Code Health           7.2 / 10
        Complexity avg:      8.5  ->  7.0  (25%)
        Func length avg:     22.3 ->  8.1  (20%)
        File size avg:       180  ->  8.5  (15%)
        Afferent coupling:   3.2  ->  7.5  (15%)
        Efferent coupling:   2.1  ->  8.0  (10%)
        Duplication rate:    4.2% ->  6.5  (15%)
    ```

    For unavailable sub-scores (e.g., coverage when -1):
    ```
        Coverage:            n/a         (30%, excluded)
    ```

    Use double-line (═) separator for the score section header to visually distinguish it from the metric sections above which use single-line (─).

    Update RenderSummary signature or add RenderScores as a separate exported function. If separate, update pipeline.go to call both RenderSummary and RenderScores.

    Format sub-score raw values: use %.1f for floats, %.0f for percentages, %d for counts. Align columns for readability.
  </action>
  <verify>
    - `cd /Users/ingo/agent-readyness && go build ./...` compiles
    - `cd /Users/ingo/agent-readyness && go test ./... -count=1` all tests pass
    - `cd /Users/ingo/agent-readyness && go run ./main.go scan .` produces output with score section showing category scores, composite, and tier
    - `cd /Users/ingo/agent-readyness && go run ./main.go scan --verbose .` shows sub-score breakdown under each category
  </verify>
  <done>
    Running `ars scan <dir>` shows per-category scores (1-10), composite score, and tier rating in a clearly formatted section. `--verbose` adds per-metric sub-score breakdown showing raw value, interpolated score, and weight percentage. Unavailable metrics shown as "n/a (excluded)".
  </done>
</task>

</tasks>

<verification>
End-to-end verification:
1. `cd /Users/ingo/agent-readyness && go build -o ars ./main.go && ./ars scan .`
   - Should show file discovery, metric sections (C1, C3, C6), AND scoring section with composite + tier
2. `./ars scan --verbose .`
   - Should show sub-score breakdowns under each category in the scoring section
3. `./ars scan --config nonexistent.yaml .`
   - Should produce clear error about missing config file
4. `go test ./... -count=1`
   - All tests pass
5. Verify SCORE-01 through SCORE-06 requirements:
   - SCORE-01: Per-category 1-10 scores displayed (check)
   - SCORE-02: Composite with weighted average and tier (check)
   - SCORE-03: Tier rating assigned (check)
   - SCORE-04: Piecewise linear interpolation used (check from Plan 01)
   - SCORE-05: Verbose shows per-metric breakdown (check)
   - SCORE-06: Thresholds configurable via --config YAML (check)
</verification>

<success_criteria>
- `ars scan <dir>` displays scores section with C1, C3, C6 scores and composite + tier
- `ars scan --verbose <dir>` shows sub-score breakdowns per metric per category
- `--config path/to/config.yaml` overrides default thresholds
- No config file -> defaults used seamlessly
- All existing tests continue to pass
- Pipeline error handling: scoring errors produce warnings, do not crash
</success_criteria>

<output>
After completion, create `.planning/phases/03-scoring-model/03-03-SUMMARY.md`
</output>
