---
phase: 04-recommendations-and-output
plan: 01
type: tdd
wave: 1
depends_on: []
files_modified:
  - internal/recommend/recommend.go
  - internal/recommend/recommend_test.go
autonomous: true

must_haves:
  truths:
    - "Recommendation engine produces ranked list of up to 5 improvements sorted by composite score impact"
    - "Each recommendation includes current value, target value, estimated score improvement, effort level, and agent-readiness framed summary"
    - "Impact estimation uses actual score simulation (not approximation) by recomputing composite with improved metric"
    - "Effort estimation combines gap size with metric-specific difficulty"
  artifacts:
    - path: "internal/recommend/recommend.go"
      provides: "Recommendation type, Generate function, effort estimation, agent impact descriptions"
      exports: ["Recommendation", "Generate"]
    - path: "internal/recommend/recommend_test.go"
      provides: "Tests for recommendation generation, ranking, effort estimation, edge cases"
      min_lines: 80
  key_links:
    - from: "internal/recommend/recommend.go"
      to: "internal/scoring"
      via: "scoring.Interpolate for what-if simulation"
      pattern: "scoring\\.Interpolate"
    - from: "internal/recommend/recommend.go"
      to: "pkg/types/scoring.go"
      via: "types.ScoredResult input"
      pattern: "types\\.ScoredResult"
---

<objective>
Build the recommendation engine that analyzes scored results and generates Top 5 improvement recommendations ranked by composite score impact, using TDD.

Purpose: This is the core business logic that turns raw scores into actionable guidance. TDD ensures the ranking algorithm, impact estimation, and effort classification are correct before wiring into the pipeline.
Output: Working, tested `internal/recommend` package with `Generate()` function.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-recommendations-and-output/04-RESEARCH.md
@internal/scoring/scorer.go
@internal/scoring/config.go
@pkg/types/scoring.go
</context>

<feature>
  <name>Recommendation Engine</name>
  <files>internal/recommend/recommend.go, internal/recommend/recommend_test.go</files>
  <behavior>
    Generate(scored *types.ScoredResult, cfg *scoring.ScoringConfig) []Recommendation

    Core behavior:
    - For each metric sub-score that is Available and Score < 9.0, compute improvement potential
    - "Improvement potential" = simulate what composite score would be if that metric improved to the next breakpoint tier's target value
    - Impact = simulated_composite - current_composite
    - Sort by impact descending, return top 5
    - Skip metrics already scoring >= 9.0 (excellent, little room to improve)
    - Skip unavailable metrics (no data to improve)

    Recommendation struct fields:
    - Rank (int, 1-based)
    - Category (string, e.g., "C1")
    - MetricName (string, e.g., "complexity_avg")
    - CurrentValue (float64, raw metric value)
    - CurrentScore (float64, 1-10 metric score)
    - TargetValue (float64, next breakpoint value that improves score)
    - TargetScore (float64, what metric score would be at target)
    - ScoreImprovement (float64, estimated composite improvement)
    - Effort (string, "Low"/"Medium"/"High")
    - Summary (string, agent-readiness framed description)
    - Action (string, concrete improvement action)

    Impact simulation:
    - Deep-copy the categories from ScoredResult
    - Replace the target metric's sub-score with the simulated improved score
    - Recompute category score (weighted avg of sub-scores)
    - Recompute composite score (weighted avg of categories, normalized by active weight sum)
    - Delta = new_composite - old_composite

    Target value selection:
    - Find the metric's breakpoints in the config
    - Find the next breakpoint that would give a higher score than current
    - Use that breakpoint's Value as the target

    Effort estimation:
    - Based on score gap (targetScore - currentScore) combined with metric difficulty multiplier
    - Score gap < 1.0 point: "Low"
    - Score gap < 2.5 points: "Medium"
    - Score gap >= 2.5 points: "High"
    - Metrics like complexity_avg and duplication_rate get a +1 level bump (inherently harder to fix)

    Agent-readiness framing:
    - Use a map of metric names to agent-impact descriptions (see research agentImpact map)
    - Summary format: "Reduce {metric_display_name} from {current} to {target} -- {agent_impact}"
    - Action: concrete advice like "Refactor functions with complexity > X" or "Add tests to reach Y% coverage"

    Edge cases:
    - Empty ScoredResult (no categories) -> return empty slice
    - All metrics >= 9.0 -> return empty slice
    - Fewer than 5 improvable metrics -> return fewer than 5
    - Nil config -> use DefaultConfig
  </behavior>
  <implementation>
    1. Define Recommendation struct and agentImpact/actionTemplates maps
    2. Implement simulateComposite helper that deep-copies categories, patches one sub-score, recomputes category then composite
    3. Implement findTargetBreakpoint that finds the next-better breakpoint for a metric
    4. Implement effortLevel that combines gap size with metric difficulty
    5. Implement Generate that iterates all sub-scores, computes impact for each, sorts, ranks, returns top 5
    6. Reuse scoring.Interpolate for score computation, scoring categoryScore pattern for recomputation
  </implementation>
</feature>

<verification>
```bash
cd /Users/ingo/agent-readyness && go test ./internal/recommend/ -v -count=1
```
All tests pass. Tests cover: basic ranking, impact calculation accuracy, effort estimation, edge cases (empty input, all-excellent scores), top-5 capping.
</verification>

<success_criteria>
- `Generate()` returns recommendations sorted by ScoreImprovement descending
- Impact values are accurate (verified by manually computing expected composite deltas in tests)
- Effort levels reflect gap size and metric difficulty
- Summaries use agent-readiness framing (not generic code quality language)
- Edge cases handled without panics
</success_criteria>

<output>
After completion, create `.planning/phases/04-recommendations-and-output/04-01-SUMMARY.md`
</output>
