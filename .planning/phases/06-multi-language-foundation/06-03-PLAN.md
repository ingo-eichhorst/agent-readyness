---
phase: 06-multi-language-foundation
plan: 03
type: execute
wave: 2
depends_on: ["06-01"]
files_modified:
  - internal/analyzer/c2_semantics.go
  - internal/analyzer/c2_go.go
  - internal/analyzer/c2_go_test.go
  - internal/scoring/config.go
  - internal/scoring/config_test.go
  - internal/scoring/scorer.go
  - internal/scoring/scorer_test.go
  - pkg/types/types.go
autonomous: true

must_haves:
  truths:
    - "C2 Go analyzer reports interface{}/any usage rate for Go projects"
    - "C2 Go analyzer measures naming consistency (CamelCase exports, camelCase unexports)"
    - "C2 Go analyzer detects magic numbers outside const blocks"
    - "C2 Go analyzer produces a scored C2 category in the output"
    - "ScoringConfig uses map-based categories instead of hardcoded C1/C3/C6 fields"
    - "Existing C1/C3/C6 scoring still works identically after refactoring to map-based config"
  artifacts:
    - path: "internal/analyzer/c2_go.go"
      provides: "Go-specific C2 metrics using go/ast"
      contains: "C2GoAnalyzer"
    - path: "internal/analyzer/c2_semantics.go"
      provides: "C2 analyzer dispatcher"
      contains: "C2Analyzer"
    - path: "internal/scoring/config.go"
      provides: "Map-based ScoringConfig with C2 breakpoints"
      contains: "Categories map"
    - path: "pkg/types/types.go"
      provides: "C2Metrics and C2LanguageMetrics types"
      contains: "C2Metrics"
  key_links:
    - from: "internal/analyzer/c2_semantics.go"
      to: "internal/analyzer/c2_go.go"
      via: "C2Analyzer dispatches to C2GoAnalyzer for Go targets"
      pattern: "goAnalyzer\\.Analyze"
    - from: "internal/scoring/scorer.go"
      to: "internal/scoring/config.go"
      via: "Scorer looks up C2 category from map-based config"
      pattern: "Categories\\[.C2.\\]"
---

<objective>
Implement C2 semantic explicitness analysis for Go (using existing go/ast), and refactor the scoring system from hardcoded category fields to a map-based design that supports any number of categories.

Purpose: Delivers the first C2 analysis capability (Go) and makes the scoring system extensible for all 7 future categories (C1-C7).
Output: Working C2 Go analyzer, map-based ScoringConfig, C2 scoring with breakpoints.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-multi-language-foundation/06-RESEARCH.md
@.planning/phases/06-multi-language-foundation/06-01-SUMMARY.md

@internal/analyzer/c1_codehealth.go
@internal/scoring/config.go
@internal/scoring/scorer.go
@internal/scoring/config_test.go
@internal/scoring/scorer_test.go
@pkg/types/types.go
@internal/parser/parser.go
</context>

<tasks>

<task type="auto">
  <name>Task 1: Refactor ScoringConfig to map-based categories and add C2 types</name>
  <files>
    internal/scoring/config.go
    internal/scoring/config_test.go
    internal/scoring/scorer.go
    internal/scoring/scorer_test.go
    pkg/types/types.go
  </files>
  <action>
1. Add C2 metric types to `pkg/types/types.go`:
```go
// C2Metrics holds Semantic Explicitness metric results.
type C2Metrics struct {
    PerLanguage map[Language]*C2LanguageMetrics
    Aggregate   *C2LanguageMetrics // LOC-weighted aggregate
}

// C2LanguageMetrics holds C2 metrics for a single language.
type C2LanguageMetrics struct {
    TypeAnnotationCoverage float64 // % of functions/params with type annotations (0-100)
    NamingConsistency      float64 // % of identifiers following convention (0-100)
    MagicNumberRatio       float64 // magic numbers per 1000 LOC
    TypeStrictness         float64 // 0 or 1: strict mode on/off (Python mypy, TS strict)
    NullSafety             float64 // % of pointer/nullable usages with safety checks (0-100)
    TotalFunctions         int     // total functions analyzed
    TotalIdentifiers       int     // total identifiers checked for naming
    MagicNumberCount       int     // raw count of magic numbers
    LOC                    int     // lines of code for this language
}
```

2. Refactor `internal/scoring/config.go`:
   - Change `ScoringConfig` from hardcoded `C1`, `C3`, `C6` fields to: `Categories map[string]CategoryConfig`
   - Keep `Tiers []TierConfig` unchanged
   - Update `DefaultConfig()` to use the map: `Categories: map[string]CategoryConfig{"C1": {...}, "C2": {...}, "C3": {...}, "C6": {...}}`
   - Add C2 category config with 5 metrics and breakpoints from RESEARCH.md:
     - type_annotation_coverage (weight 0.30): {0:1, 30:3, 50:6, 80:8, 100:10}
     - naming_consistency (weight 0.25): {0:1, 70:3, 85:6, 95:8, 100:10}
     - magic_number_ratio (weight 0.20): {0:10, 5:8, 15:6, 30:3, 50:1} (inverted: lower is better)
     - type_strictness (weight 0.15): {0:3, 1:10} (binary)
     - null_safety (weight 0.10): {0:1, 30:3, 50:6, 80:8, 100:10}
   - C2 weight: 0.10

3. Refactor `internal/scoring/scorer.go`:
   - Change `Score()` method: instead of switch on ar.Category with hardcoded scoreC1/C3/C6, look up category config from `s.Config.Categories[ar.Category]` and call a generic `scoreCategory(catConfig, ar)` method
   - The `scoreC1`, `scoreC3`, `scoreC6` methods extract raw values from metrics -- refactor these to be registered extractors. Create a `metricExtractors map[string]func(*types.AnalysisResult) map[string]float64` that maps category name to a function returning raw metric values.
   - Add `extractC2` function that extracts raw values from C2Metrics aggregate (type_annotation_coverage, naming_consistency, magic_number_ratio, type_strictness, null_safety)
   - Keep `extractC1`, `extractC3`, `extractC6` functions (renamed from scoreC1/scoreC3/scoreC6) that return `map[string]float64` of raw values
   - The generic `scoreCategory` function: looks up config by category name, calls extractor, computes sub-scores via Interpolate, returns CategoryScore
   - Handle unavailable metrics (e.g., coverage_percent == -1 for C6) by passing an unavailable map from the extractor
   - `computeComposite` and `classifyTier` remain unchanged

4. Update `LoadConfig()` to work with map-based structure. YAML unmarshal should populate the map.

5. Update scoring tests:
   - All existing tests must pass with identical results
   - Add test for C2 scoring with sample C2Metrics
   - Test that map-based config lookup works correctly
   - Test that DefaultConfig() has C1, C2, C3, C6 entries
  </action>
  <verify>
`cd /Users/ingo/agent-readyness && go test ./internal/scoring/... -v` -- all tests pass including new C2 scoring tests
  </verify>
  <done>ScoringConfig is map-based. C2 category config exists with breakpoints. Scorer dispatches by category name via extractor pattern. All existing scoring tests pass.</done>
</task>

<task type="auto">
  <name>Task 2: Implement C2 Go analyzer</name>
  <files>
    internal/analyzer/c2_semantics.go
    internal/analyzer/c2_go.go
    internal/analyzer/c2_go_test.go
  </files>
  <action>
1. Create `internal/analyzer/c2_semantics.go`:
   - `C2Analyzer` struct with `goAnalyzer *C2GoAnalyzer` field (Python/TS analyzers added in Plan 04)
   - Implement `GoAwareAnalyzer` interface (has SetGoPackages + Analyze)
   - `Name()` returns "C2: Semantic Explicitness"
   - `Analyze(targets []*types.AnalysisTarget)`: iterate targets, dispatch to language-specific analyzer. For Go, call goAnalyzer.Analyze(). For Python/TypeScript, skip for now (they will be added in Plan 04).
   - Aggregate per-language C2 results weighted by LOC proportion
   - Return AnalysisResult with Category: "C2" and Metrics: map[string]interface{}{"c2": metrics}

2. Create `internal/analyzer/c2_go.go`:
   - `C2GoAnalyzer` struct with `pkgs []*parser.ParsedPackage` field
   - `Analyze(target *types.AnalysisTarget) (*types.C2LanguageMetrics, error)` method

   Implement these C2-GO metrics using go/ast (NOT Tree-sitter):

   **C2-GO-01: interface{}/any usage rate**
   - Walk AST looking for empty `*ast.InterfaceType` (Methods.List is nil or empty) and `*ast.Ident` with Name=="any"
   - Count total type references vs any/interface{} references
   - NullSafety metric: ratio of safe usages (100 - anyUsagePercent)

   **C2-GO-02: Naming consistency**
   - For exported identifiers (ast.IsExported): check CamelCase starting with uppercase
   - For unexported identifiers: check camelCase starting with lowercase
   - Skip single-letter variables, `_` blank identifier, and common acronyms (ID, URL, HTTP)
   - Score: percentage of identifiers following convention

   **C2-GO-03: Magic numbers**
   - Find `*ast.BasicLit` with Kind == token.INT or token.FLOAT
   - Exclude: inside const declarations, values 0/1/-1, array/slice indices
   - Report: magic numbers per 1000 LOC

   **C2-GO-04: Nil safety patterns**
   - Count pointer dereferences (UnaryExpr with Star, or SelectorExpr on pointer types)
   - Count nil checks (BinaryExpr with nil operand using == or !=)
   - Simple heuristic: percentage of pointer usages that have preceding nil checks in the same function
   - Do NOT build full data flow analysis -- simple ratio is sufficient

   **TypeAnnotationCoverage:** Always 100 for Go (statically typed)
   **TypeStrictness:** Always 1 for Go (compile-time type checking)

   Combine metrics into C2LanguageMetrics struct.

3. Write tests `internal/analyzer/c2_go_test.go`:
   - Test against the project's own codebase (parse . and analyze)
   - Verify C2LanguageMetrics fields are populated (TypeAnnotationCoverage == 100, NamingConsistency > 0, MagicNumberRatio >= 0)
   - Test edge cases: empty interface{} detection, any keyword detection
  </action>
  <verify>
`cd /Users/ingo/agent-readyness && go test ./internal/analyzer/... -v -run C2` -- C2 tests pass. Then `go test ./... 2>&1` -- all tests pass.
  </verify>
  <done>C2 Go analyzer produces metrics for interface{}/any usage, naming consistency, magic numbers, and nil safety. C2Analyzer dispatches to Go implementation. Tests verify real Go code analysis.</done>
</task>

</tasks>

<verification>
1. `go build ./...` succeeds
2. `go test ./...` all tests pass
3. ScoringConfig uses `Categories map[string]CategoryConfig`
4. C2 analyzer produces scored results for Go projects
5. Existing C1/C3/C6 scoring unchanged (verified by tests)
6. `go build -o /tmp/ars-test ./main.go && /tmp/ars-test scan .` shows C2 scores alongside C1/C3/C6
</verification>

<success_criteria>
- C2 Go analysis produces meaningful metrics for Go codebases
- ScoringConfig is map-based and extensible for C4/C5/C7
- All existing scoring tests pass with identical results
- C2 scores appear in `ars scan` output
</success_criteria>

<output>
After completion, create `.planning/phases/06-multi-language-foundation/06-03-SUMMARY.md`
</output>
