---
phase: 09-c4-documentation-quality-html-reports
plan: 02
type: execute
wave: 2
depends_on: ["09-01"]
files_modified:
  - internal/llm/client.go
  - internal/llm/client_test.go
  - internal/llm/cost.go
  - internal/llm/prompts.go
  - internal/analyzer/c4_documentation.go
  - pkg/types/types.go
  - cmd/scan.go
  - go.mod
autonomous: false
user_setup:
  - service: anthropic
    why: "LLM-based documentation quality evaluation"
    env_vars:
      - name: ANTHROPIC_API_KEY
        source: "Anthropic Console -> API Keys"

must_haves:
  truths:
    - "User can run ars scan --enable-c4-llm and see LLM-evaluated content quality"
    - "User sees cost estimation before LLM analysis runs"
    - "User must confirm before LLM analysis proceeds"
    - "LLM analysis uses sampling to limit API calls (max 100 files)"
    - "LLM analysis gracefully degrades if API errors occur"
  artifacts:
    - path: "internal/llm/client.go"
      provides: "LLM client abstraction with Anthropic SDK"
      exports: ["Client", "NewClient", "EvaluateContent"]
    - path: "internal/llm/cost.go"
      provides: "Cost estimation for LLM analysis"
      exports: ["EstimateCost", "CostEstimate"]
    - path: "internal/llm/prompts.go"
      provides: "Evaluation prompts for C4 content quality"
      contains: "readmeClarityPrompt"
    - path: "cmd/scan.go"
      provides: "CLI flag --enable-c4-llm"
      contains: "--enable-c4-llm"
  key_links:
    - from: "internal/analyzer/c4_documentation.go"
      to: "internal/llm/client.go"
      via: "calls LLM client for content evaluation"
      pattern: "llmClient.Evaluate"
    - from: "cmd/scan.go"
      to: "internal/llm/cost.go"
      via: "shows cost estimate before confirmation"
      pattern: "EstimateCost"
---

<objective>
Add LLM client abstraction and C4 content quality evaluation with cost estimation and user confirmation.

Purpose: Enable opt-in LLM-based documentation quality assessment (README clarity, example quality, completeness, cross-reference coherence). This covers requirements C4-08 through C4-14 and CLI-01, CLI-06.

Output: Working LLM integration that can be enabled with --enable-c4-llm, shows costs before running, and adds LLM-based scores to C4 metrics.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/09-c4-documentation-quality-+-html-reports/09-RESEARCH.md
@.planning/phases/09-c4-documentation-quality-+-html-reports/09-01-SUMMARY.md
@internal/analyzer/c4_documentation.go
@cmd/scan.go
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create LLM client package with Anthropic SDK integration</name>
  <files>
    go.mod
    internal/llm/client.go
    internal/llm/cost.go
    internal/llm/prompts.go
  </files>
  <action>
1. Add Anthropic SDK dependency:
   ```bash
   go get github.com/anthropics/anthropic-sdk-go
   ```

2. Create internal/llm/client.go:
   ```go
   package llm

   type Client struct {
       client *anthropic.Client
       model  anthropic.Model
   }

   func NewClient(apiKey string) (*Client, error) {
       if apiKey == "" {
           return nil, fmt.Errorf("ANTHROPIC_API_KEY not set")
       }
       c := anthropic.NewClient(option.WithAPIKey(apiKey))
       return &Client{
           client: c,
           model:  anthropic.ModelClaude3_5HaikuLatest,  // Cost-effective for evaluation
       }, nil
   }

   // Evaluation holds LLM evaluation result
   type Evaluation struct {
       Score     int     // 1-10
       Reasoning string
   }

   // EvaluateContent runs LLM judge on content with given prompt
   func (c *Client) EvaluateContent(ctx context.Context, systemPrompt, content string) (Evaluation, error) {
       // Use prompt caching for system prompt (rubric)
       // Parse JSON response: {"score": N, "reason": "..."}
       // Handle rate limits with exponential backoff (3 retries)
   }
   ```

3. Create internal/llm/cost.go:
   ```go
   type CostEstimate struct {
       InputTokens  int
       OutputTokens int
       MinCost      float64  // USD
       MaxCost      float64  // USD
       FilesCount   int
   }

   // EstimateCost calculates expected cost before running analysis
   // Haiku pricing: $0.25/MTok input, $1.25/MTok output
   // With caching: writes 1.25x, reads 0.1x (after first call)
   func EstimateCost(readmeWordCount int, sampleFileCount int) CostEstimate {
       rubricTokens := 2000  // System prompt cached after first call
       perFileInputTokens := 500
       perFileOutputTokens := 100

       totalInput := rubricTokens + (sampleFileCount * perFileInputTokens)
       totalOutput := sampleFileCount * perFileOutputTokens

       // First call: cache write (1.25x), subsequent: cache read (0.1x)
       // Simplified: assume mostly cache reads for multi-file analysis
       inputCost := float64(totalInput) / 1_000_000 * 0.25
       outputCost := float64(totalOutput) / 1_000_000 * 1.25

       return CostEstimate{
           InputTokens:  totalInput,
           OutputTokens: totalOutput,
           MinCost:      inputCost + outputCost,
           MaxCost:      (inputCost + outputCost) * 1.5,  // Buffer for variance
           FilesCount:   sampleFileCount,
       }
   }
   ```

4. Create internal/llm/prompts.go with evaluation prompts:
   - readmeClarityPrompt: Rate 1-10 for purpose clarity, quickstart, structure, examples
   - exampleQualityPrompt: Rate 1-10 for runnability, clarity, best practices
   - completenessPrompt: Rate 1-10 for missing sections
   - crossRefCoherencePrompt: Rate 1-10 for consistent terminology, valid links

   Each prompt must return JSON: {"score": N, "reason": "..."}

5. Implement prompt caching using Anthropic SDK cache_control:
   ```go
   System: []anthropic.TextBlockParam{
       {
           Type: anthropic.F(anthropic.TextBlockParamTypeText),
           Text: anthropic.F(systemPrompt),
           CacheControl: anthropic.F(anthropic.CacheControlEphemeralParam{
               Type: anthropic.F(anthropic.CacheControlEphemeralTypeEphemeral),
           }),
       },
   },
   ```
  </action>
  <verify>
go build ./internal/llm/... compiles without errors
  </verify>
  <done>
LLM client package exists with Anthropic SDK integration, cost estimation, and evaluation prompts.
  </done>
</task>

<task type="auto">
  <name>Task 2: Extend C4Analyzer for LLM metrics and add CLI flag</name>
  <files>
    internal/analyzer/c4_documentation.go
    pkg/types/types.go
    cmd/scan.go
    internal/pipeline/pipeline.go
  </files>
  <action>
1. Extend C4Metrics in pkg/types/types.go:
   ```go
   type C4Metrics struct {
       // ... existing static fields ...

       // LLM metrics (only populated if --enable-c4-llm)
       LLMEnabled        bool
       ReadmeClarity     int      // 1-10 scale
       ExampleQuality    int      // 1-10 scale
       Completeness      int      // 1-10 scale
       CrossRefCoherence int      // 1-10 scale
       LLMCostUSD        float64  // Actual cost incurred
       LLMTokensUsed     int
       LLMFilesSampled   int
   }
   ```

2. Update C4Analyzer constructor:
   ```go
   type C4Analyzer struct {
       tsParser  *parser.TreeSitterParser
       llmClient *llm.Client  // nil if LLM not enabled
   }

   func NewC4Analyzer(tsParser *parser.TreeSitterParser, llmClient *llm.Client) *C4Analyzer {
       return &C4Analyzer{tsParser: tsParser, llmClient: llmClient}
   }
   ```

3. Implement LLM analysis in C4Analyzer.Analyze():
   - Check if llmClient != nil
   - Call LLM evaluation functions for README clarity, example quality, etc.
   - Implement sampling strategy: max 100 files for cost control
   - Use errgroup with limited concurrency (5 parallel) for API calls
   - Handle errors gracefully: log warning, continue with static metrics only

4. Add CLI flag to cmd/scan.go:
   ```go
   var enableC4LLM bool

   func init() {
       scanCmd.Flags().BoolVar(&enableC4LLM, "enable-c4-llm", false,
           "Enable LLM-based C4 content quality evaluation (requires ANTHROPIC_API_KEY)")
   }
   ```

5. Update pipeline.go to:
   - Check enableC4LLM flag
   - If true, create LLM client: llm.NewClient(os.Getenv("ANTHROPIC_API_KEY"))
   - Pass llmClient to NewC4Analyzer()
   - Show cost estimate and prompt for confirmation BEFORE analysis starts

6. Implement confirmation flow in cmd/scan.go RunE:
   ```go
   if enableC4LLM {
       apiKey := os.Getenv("ANTHROPIC_API_KEY")
       if apiKey == "" {
           return fmt.Errorf("--enable-c4-llm requires ANTHROPIC_API_KEY environment variable")
       }

       // Estimate cost based on project size
       estimate := llm.EstimateCost(readmeWordCount, sampleFileCount)
       fmt.Printf("\nLLM Analysis Cost Estimation\n")
       fmt.Printf("Files to analyze: %d (sampled)\n", estimate.FilesCount)
       fmt.Printf("Estimated cost: $%.3f - $%.3f\n\n", estimate.MinCost, estimate.MaxCost)
       fmt.Printf("Continue? (yes/no): ")

       var response string
       fmt.Scanln(&response)
       if response != "yes" && response != "y" {
           fmt.Println("LLM analysis cancelled. Running static analysis only.")
           enableC4LLM = false
       }
   }
   ```
  </action>
  <verify>
go build ./... compiles without errors
go run ./cmd/ars scan --help shows --enable-c4-llm flag
  </verify>
  <done>
C4Analyzer accepts optional LLM client. CLI has --enable-c4-llm flag. Cost estimation and confirmation flow implemented.
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>LLM integration with cost estimation and user confirmation</what-built>
  <how-to-verify>
1. Set ANTHROPIC_API_KEY environment variable (get from Anthropic Console)
2. Run: `go run ./cmd/ars scan --enable-c4-llm .`
3. Verify cost estimation is shown before confirmation prompt
4. Type "yes" to proceed
5. Verify LLM metrics appear in C4 output (README clarity, example quality, etc.)
6. Verify actual cost is reported in output
7. Run without --enable-c4-llm and verify LLM metrics are absent (static only)
  </how-to-verify>
  <resume-signal>Type "approved" if LLM integration works correctly, or describe issues</resume-signal>
</task>

</tasks>

<verification>
1. `go build ./...` compiles successfully
2. `go test ./internal/llm/...` passes (mock tests)
3. With ANTHROPIC_API_KEY set: `go run ./cmd/ars scan --enable-c4-llm .` shows LLM metrics
4. Without flag: `go run ./cmd/ars scan .` shows only static C4 metrics
5. Cost estimation appears before confirmation prompt
</verification>

<success_criteria>
- LLM client package exists with Anthropic SDK integration
- Prompt caching implemented for cost reduction
- Cost estimation accurate within 50% of actual
- User confirmation required before LLM analysis
- LLM metrics populate C4 results when enabled
- Graceful degradation if LLM errors occur
- --enable-c4-llm flag documented in help
</success_criteria>

<output>
After completion, create `.planning/phases/09-c4-documentation-quality-+-html-reports/09-02-SUMMARY.md`
</output>
