---
phase: 10-c7-agent-evaluation
plan: 02
type: execute
wave: 2
depends_on: ["10-01"]
files_modified:
  - internal/agent/scorer.go
  - internal/agent/scorer_test.go
  - internal/analyzer/c7_agent.go
  - internal/analyzer/c7_agent_test.go
  - pkg/types/types.go
  - internal/scoring/config.go
  - internal/pipeline/pipeline.go
  - cmd/scan.go
autonomous: true

must_haves:
  truths:
    - "User can run ars scan --enable-c7 and see C7 agent evaluation scores"
    - "User sees cost estimation and must confirm before C7 evaluation runs"
    - "C7 handles agent errors, timeouts, and failures gracefully without crashing"
    - "User without claude CLI installed gets a clear error when requesting C7"
    - "C7 scores appear in JSON output and HTML reports"
  artifacts:
    - path: "internal/agent/scorer.go"
      provides: "Rubric-based LLM scoring for agent responses"
      exports: ["Scorer", "NewScorer", "ScoreResult"]
    - path: "internal/analyzer/c7_agent.go"
      provides: "C7Analyzer implementing pipeline.Analyzer"
      exports: ["C7Analyzer", "NewC7Analyzer"]
    - path: "pkg/types/types.go"
      provides: "C7Metrics type definition"
      contains: "C7Metrics struct"
    - path: "cmd/scan.go"
      provides: "--enable-c7 flag with cost estimation"
      contains: "enableC7"
  key_links:
    - from: "internal/analyzer/c7_agent.go"
      to: "internal/agent/executor.go"
      via: "Executor.ExecuteTask calls"
      pattern: "executor\\.ExecuteTask"
    - from: "internal/agent/scorer.go"
      to: "internal/llm/client.go"
      via: "llm.Client.EvaluateContent"
      pattern: "llmClient\\.EvaluateContent"
    - from: "cmd/scan.go"
      to: "internal/analyzer/c7_agent.go"
      via: "pipeline C7 analyzer registration"
      pattern: "C7Analyzer|enableC7"
---

<objective>
Complete C7 agent evaluation: LLM-based scoring of agent responses, C7Analyzer integration with pipeline, --enable-c7 CLI flag with cost estimation, and comprehensive error handling.

Purpose: Delivers the novel C7 metric that differentiates ARS from static analysis tools by measuring genuine agent behavior.
Output: Working --enable-c7 flag that executes agent tasks and produces C7 scores in all output formats.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/10-c7-agent-evaluation/10-CONTEXT.md
@.planning/phases/10-c7-agent-evaluation/10-RESEARCH.md
@.planning/phases/10-c7-agent-evaluation/10-01-SUMMARY.md
@internal/llm/client.go (LLM client pattern)
@internal/llm/cost.go (cost estimation pattern)
@internal/analyzer/c4_documentation.go (analyzer pattern with LLM integration)
@internal/analyzer/c5_temporal.go (repo-level analyzer pattern)
@cmd/scan.go (CLI flag pattern with confirmation)
@internal/pipeline/pipeline.go (analyzer registration)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create scorer and C7 metrics types</name>
  <files>
    internal/agent/scorer.go
    internal/agent/scorer_test.go
    pkg/types/types.go
  </files>
  <action>
Add to pkg/types/types.go (append after C4Metrics):

```go
// C7Metrics holds Agent Evaluation metric results.
type C7Metrics struct {
    Available            bool              // false if claude CLI not found or user declined
    IntentClarity        int               // 0-100 score
    ModificationConfidence int             // 0-100 score
    CrossFileCoherence   int               // 0-100 score
    SemanticCompleteness int               // 0-100 score
    OverallScore         float64           // average of 4 task scores (0-100)
    TaskResults          []C7TaskResult    // detailed per-task results
    TotalDuration        float64           // seconds
    TokensUsed           int               // estimated total tokens
    CostUSD              float64           // estimated cost
}

// C7TaskResult holds results for a single C7 evaluation task.
type C7TaskResult struct {
    TaskID     string  // e.g., "intent_clarity"
    TaskName   string  // e.g., "Intent Clarity"
    Score      int     // 0-100
    Status     string  // completed, timeout, error
    Duration   float64 // seconds
    Reasoning  string  // scoring rationale from LLM judge
}
```

Create internal/agent/scorer.go:

```go
package agent

import (
    "context"
    "github.com/ingo/agent-readyness/internal/llm"
)

// Scorer evaluates agent task responses using LLM-as-a-judge.
type Scorer struct {
    llmClient *llm.Client
}

// NewScorer creates a scorer with the given LLM client.
func NewScorer(client *llm.Client) *Scorer {
    return &Scorer{llmClient: client}
}

// ScoreResult holds the scoring output for a task response.
type ScoreResult struct {
    Score     int    // 0-100
    Reasoning string // explanation from LLM judge
}

// Score evaluates an agent's response against a task's rubric.
func (s *Scorer) Score(ctx context.Context, task Task, response string) (ScoreResult, error) {
    // Build the scoring prompt based on task ID
    rubric := getRubric(task.ID)
    content := fmt.Sprintf("Task: %s\n\nAgent Response:\n%s", task.Prompt, response)

    eval, err := s.llmClient.EvaluateContent(ctx, rubric, content)
    if err != nil {
        return ScoreResult{}, err
    }

    // Scale from 1-10 to 0-100
    return ScoreResult{
        Score:     eval.Score * 10,
        Reasoning: eval.Reasoning,
    }, nil
}

// getRubric returns the LLM scoring prompt for a task.
func getRubric(taskID string) string {
    rubrics := map[string]string{
        "intent_clarity": `You are evaluating an AI coding agent's response to a code understanding task.

The agent was asked to find and explain a main entry point function.

Score the response from 1-10 based on these criteria:
- Correct identification (40%): Did the agent find the right function and file?
- Accuracy of explanation (40%): Is the explanation correct, clear, and specific?
- Use of codebase context (20%): Did the agent reference actual code details?

Consider:
- Score 8-10: Correct function found, accurate explanation, references specific code
- Score 5-7: Function found but explanation has minor issues or lacks specifics
- Score 3-4: Wrong function or significant explanation errors
- Score 1-2: Failed to find function or completely wrong explanation

Respond with JSON only: {"score": N, "reason": "brief explanation"}`,

        "modification_confidence": `You are evaluating an AI coding agent's response to a code modification task.

The agent was asked to propose input validation for a function.

Score the response from 1-10 based on these criteria:
- Correctness of change (50%): Is the proposed validation appropriate and correct?
- Appropriate scope (30%): Is the change well-scoped (not too broad or too narrow)?
- Follows patterns (20%): Does it match existing codebase patterns?

Consider:
- Score 8-10: Correct validation, well-scoped, matches existing patterns
- Score 5-7: Reasonable validation but minor issues with scope or patterns
- Score 3-4: Validation has significant issues or wrong approach
- Score 1-2: Proposed change would break code or is completely wrong

Respond with JSON only: {"score": N, "reason": "brief explanation"}`,

        "cross_file_coherence": `You are evaluating an AI coding agent's response to a code tracing task.

The agent was asked to trace data flow across multiple files.

Score the response from 1-10 based on these criteria:
- Completeness of trace (50%): Did the agent follow the full data path?
- Accuracy (30%): Are the files, functions, and flow described correctly?
- Efficiency (20%): Did the agent avoid unnecessary detours or confusion?

Consider:
- Score 8-10: Complete trace, all files/functions correct, clear presentation
- Score 5-7: Most of the trace correct but missing steps or minor errors
- Score 3-4: Major gaps in trace or significant errors
- Score 1-2: Failed to trace or completely wrong flow

Respond with JSON only: {"score": N, "reason": "brief explanation"}`,

        "semantic_completeness": `You are evaluating an AI coding agent's response to a pattern-matching task.

The agent was asked to propose error handling that matches existing patterns.

Score the response from 1-10 based on these criteria:
- Functional correctness (40%): Would the proposed error handling work?
- Pattern matching (40%): Does it actually match patterns found in the codebase?
- Edge case handling (20%): Does it consider edge cases appropriately?

Consider:
- Score 8-10: Correct error handling, clearly matches existing patterns, good edge cases
- Score 5-7: Reasonable error handling but pattern matching could be better
- Score 3-4: Error handling has issues or doesn't match patterns
- Score 1-2: Proposed handling would fail or ignores existing patterns

Respond with JSON only: {"score": N, "reason": "brief explanation"}`,
    }

    if rubric, ok := rubrics[taskID]; ok {
        return rubric
    }
    return rubrics["intent_clarity"] // fallback
}
```

Create internal/agent/scorer_test.go:
- TestGetRubric_AllTasksHaveRubrics: Verify getRubric returns non-empty for all task IDs
- TestScoreResult_Fields: Verify ScoreResult has Score and Reasoning fields
  </action>
  <verify>
go build ./internal/agent/... ./pkg/types/...
go test ./internal/agent/... -v
  </verify>
  <done>
C7Metrics added to types.go with all required fields
Scorer implemented with rubrics for all 4 tasks
Tests pass for scorer
  </done>
</task>

<task type="auto">
  <name>Task 2: Create C7Analyzer and integrate with pipeline</name>
  <files>
    internal/analyzer/c7_agent.go
    internal/analyzer/c7_agent_test.go
    internal/scoring/config.go
    internal/pipeline/pipeline.go
  </files>
  <action>
Create internal/analyzer/c7_agent.go following C4/C5 analyzer patterns:

```go
package analyzer

import (
    "context"
    "fmt"
    "time"

    "github.com/ingo/agent-readyness/internal/agent"
    "github.com/ingo/agent-readyness/internal/llm"
    "github.com/ingo/agent-readyness/pkg/types"
)

// C7Analyzer implements the pipeline.Analyzer interface for C7: Agent Evaluation.
type C7Analyzer struct {
    llmClient *llm.Client
    enabled   bool // only runs if explicitly enabled
}

// NewC7Analyzer creates a C7Analyzer. It's disabled by default.
func NewC7Analyzer() *C7Analyzer {
    return &C7Analyzer{enabled: false}
}

// Enable activates C7 analysis with the given LLM client.
func (a *C7Analyzer) Enable(client *llm.Client) {
    a.llmClient = client
    a.enabled = true
}

// Name returns the analyzer display name.
func (a *C7Analyzer) Name() string {
    return "C7: Agent Evaluation"
}

// Analyze runs C7 agent evaluation.
func (a *C7Analyzer) Analyze(targets []*types.AnalysisTarget) (*types.AnalysisResult, error) {
    if !a.enabled {
        return &types.AnalysisResult{
            Name:     "C7: Agent Evaluation",
            Category: "C7",
            Metrics:  map[string]interface{}{"c7": &types.C7Metrics{Available: false}},
        }, nil
    }

    if len(targets) == 0 {
        return nil, fmt.Errorf("no targets provided")
    }
    rootDir := targets[0].RootDir

    // Check for Claude CLI
    if err := agent.CheckClaudeCLI(); err != nil {
        return &types.AnalysisResult{
            Name:     "C7: Agent Evaluation",
            Category: "C7",
            Metrics:  map[string]interface{}{"c7": &types.C7Metrics{Available: false}},
        }, nil
    }

    // Create isolated workspace
    workDir, cleanup, err := agent.CreateWorkspace(rootDir)
    if err != nil {
        return nil, fmt.Errorf("create workspace: %w", err)
    }
    defer cleanup()

    // Execute tasks
    executor := agent.NewExecutor(workDir)
    scorer := agent.NewScorer(a.llmClient)
    tasks := agent.AllTasks()

    metrics := &types.C7Metrics{
        Available:   true,
        TaskResults: make([]types.C7TaskResult, 0, len(tasks)),
    }

    ctx := context.Background()
    startTime := time.Now()
    totalTokens := 0

    for i, task := range tasks {
        // Progress reporting could go here (future enhancement)
        _ = i

        taskStart := time.Now()
        result := executor.ExecuteTask(ctx, task)
        taskDuration := time.Since(taskStart).Seconds()

        var scoreResult agent.ScoreResult
        if result.Status == "completed" && result.Response != "" {
            scoreResult, _ = scorer.Score(ctx, task, result.Response)
            totalTokens += estimateResponseTokens(result.Response)
        }

        taskResult := types.C7TaskResult{
            TaskID:    task.ID,
            TaskName:  task.Name,
            Score:     scoreResult.Score,
            Status:    string(result.Status),
            Duration:  taskDuration,
            Reasoning: scoreResult.Reasoning,
        }
        metrics.TaskResults = append(metrics.TaskResults, taskResult)

        // Set individual scores
        switch task.ID {
        case "intent_clarity":
            metrics.IntentClarity = scoreResult.Score
        case "modification_confidence":
            metrics.ModificationConfidence = scoreResult.Score
        case "cross_file_coherence":
            metrics.CrossFileCoherence = scoreResult.Score
        case "semantic_completeness":
            metrics.SemanticCompleteness = scoreResult.Score
        }
    }

    metrics.TotalDuration = time.Since(startTime).Seconds()
    metrics.TokensUsed = totalTokens

    // Calculate overall score (average of completed tasks)
    completedCount := 0
    totalScore := 0
    for _, tr := range metrics.TaskResults {
        if tr.Status == "completed" && tr.Score > 0 {
            totalScore += tr.Score
            completedCount++
        }
    }
    if completedCount > 0 {
        metrics.OverallScore = float64(totalScore) / float64(completedCount)
    }

    // Estimate cost (Sonnet pricing for Claude CLI)
    // ~10k tokens per task (agent execution), ~500 tokens per scoring call
    metrics.CostUSD = float64(metrics.TokensUsed+len(tasks)*500) / 1_000_000 * 5.0 // ~$5/MTok blended

    return &types.AnalysisResult{
        Name:     "C7: Agent Evaluation",
        Category: "C7",
        Metrics:  map[string]interface{}{"c7": metrics},
    }, nil
}

func estimateResponseTokens(response string) int {
    return len(response) / 4 // ~4 chars per token
}
```

Update internal/scoring/config.go:
- Add C7 to DefaultConfig() Categories map with weight 0.10:
```go
"C7": {Weight: 0.10, Enabled: true},
```
- Adjust other weights proportionally to sum to 1.0 (reduce C1/C3/C6 slightly)

Update internal/pipeline/pipeline.go:
- Add c7Analyzer field to Pipeline struct
- Add SetC7Enabled(client *llm.Client) method
- In Run(), add c7Analyzer to analyzers list if enabled
- Wire metrics extraction for C7 in extractMetrics or equivalent

Create internal/analyzer/c7_agent_test.go:
- TestC7Analyzer_DisabledByDefault: Verify Analyze returns Available:false when not enabled
- TestC7Analyzer_NoCLI: Mock or skip - verify graceful handling when CLI not found
  </action>
  <verify>
go build ./...
go test ./internal/analyzer/... -run C7 -v
go test ./internal/scoring/... -v
  </verify>
  <done>
C7Analyzer created following C4/C5 patterns
Scoring config updated with C7 weight
Pipeline wired to include C7 when enabled
Tests pass
  </done>
</task>

<task type="auto">
  <name>Task 3: Add CLI flag with cost estimation and confirmation</name>
  <files>
    cmd/scan.go
    internal/llm/cost.go
  </files>
  <action>
Update internal/llm/cost.go - add C7 cost estimation:

```go
// EstimateC7Cost calculates expected cost for C7 agent evaluation.
// Uses Sonnet pricing since Claude CLI uses Sonnet by default.
// 4 tasks, ~10k tokens each for agent execution, ~500 tokens for scoring.
func EstimateC7Cost() CostEstimate {
    // Agent execution: 4 tasks * 10k tokens = 40k tokens
    // Scoring: 4 tasks * 500 tokens = 2k tokens
    // Total: ~42k tokens
    agentTokens := 40000
    scoringTokens := 2000
    totalTokens := agentTokens + scoringTokens

    // Sonnet pricing: $3/MTok input, $15/MTok output
    // Assume 70% input, 30% output for agent; 80% input, 20% output for scoring
    inputTokens := int(float64(agentTokens)*0.7) + int(float64(scoringTokens)*0.8)
    outputTokens := totalTokens - inputTokens

    inputCost := float64(inputTokens) / 1_000_000 * 3.0
    outputCost := float64(outputTokens) / 1_000_000 * 15.0

    minCost := inputCost + outputCost
    maxCost := minCost * 2.0 // Higher variance for agent tasks

    return CostEstimate{
        InputTokens:  inputTokens,
        OutputTokens: outputTokens,
        MinCost:      minCost,
        MaxCost:      maxCost,
        FilesCount:   4, // 4 tasks
    }
}
```

Update cmd/scan.go:

1. Add new flag variable:
```go
var enableC7 bool
```

2. In init(), add flag:
```go
scanCmd.Flags().BoolVar(&enableC7, "enable-c7", false, "enable C7 agent evaluation using Claude Code CLI (requires claude CLI installed)")
```

3. In RunE, before pipeline.Run(), add C7 handling (similar to C4 LLM pattern):

```go
// Handle C7 agent evaluation if enabled
if enableC7 {
    // Check Claude CLI availability first
    if err := agent.CheckClaudeCLI(); err != nil {
        return fmt.Errorf("--enable-c7 requires Claude Code CLI to be installed\n%s", err)
    }

    // LLM client needed for scoring (uses ANTHROPIC_API_KEY)
    apiKey := os.Getenv("ANTHROPIC_API_KEY")
    if apiKey == "" {
        return fmt.Errorf("--enable-c7 requires ANTHROPIC_API_KEY environment variable for scoring\nGet your API key from: https://console.anthropic.com/")
    }

    // Show cost estimate and get confirmation
    estimate := llm.EstimateC7Cost()
    fmt.Fprintf(cmd.OutOrStdout(), "\nC7 Agent Evaluation Cost Estimate\n")
    fmt.Fprintf(cmd.OutOrStdout(), "==================================\n")
    fmt.Fprintf(cmd.OutOrStdout(), "Tasks to run: 4 (intent clarity, modification confidence, cross-file coherence, semantic completeness)\n")
    fmt.Fprintf(cmd.OutOrStdout(), "Estimated cost: %s\n", estimate.FormatCost())
    fmt.Fprintf(cmd.OutOrStdout(), "Estimated duration: 5-20 minutes (depends on codebase size)\n\n")
    fmt.Fprintf(cmd.OutOrStdout(), "This will:\n")
    fmt.Fprintf(cmd.OutOrStdout(), "  1. Run Claude Code headless against your codebase\n")
    fmt.Fprintf(cmd.OutOrStdout(), "  2. Send agent responses to Anthropic API for scoring\n")
    fmt.Fprintf(cmd.OutOrStdout(), "  3. Use your ANTHROPIC_API_KEY for both operations\n\n")
    fmt.Fprintf(cmd.OutOrStdout(), "Continue? (yes/no): ")

    reader := bufio.NewReader(os.Stdin)
    response, _ := reader.ReadString('\n')
    response = strings.TrimSpace(strings.ToLower(response))

    if response != "yes" && response != "y" {
        fmt.Fprintf(cmd.OutOrStdout(), "C7 evaluation cancelled. Running other analyzers only.\n\n")
        enableC7 = false
    } else {
        // Create LLM client for scoring
        if llmClient == nil {
            var err error
            llmClient, err = llm.NewClient(apiKey)
            if err != nil {
                return fmt.Errorf("failed to create LLM client: %w", err)
            }
        }
        p.SetC7Enabled(llmClient)
        fmt.Fprintf(cmd.OutOrStdout(), "\n")
    }
}
```

4. Add import for agent package:
```go
import "github.com/ingo/agent-readyness/internal/agent"
```
  </action>
  <verify>
go build ./cmd/...
ars scan --help | grep -A2 "enable-c7"
  </verify>
  <done>
--enable-c7 flag added with cost estimation and confirmation flow
CLI checks for claude CLI and ANTHROPIC_API_KEY before running
Graceful error messages for missing dependencies
  </done>
</task>

</tasks>

<verification>
All checks pass:
- go build ./...
- go test ./... -v
- ars scan --help shows --enable-c7 flag
- ars scan . (without --enable-c7) completes without C7 (Available: false in JSON)
- Types include C7Metrics in JSON output schema
</verification>

<success_criteria>
1. ars scan --enable-c7 shows cost estimation and prompts for confirmation
2. Without claude CLI: clear error message with installation instructions
3. Without ANTHROPIC_API_KEY: clear error message
4. With all dependencies: C7 tasks execute and produce scores
5. Timeouts and errors handled gracefully (score 0 for failed tasks, continue to next)
6. C7 scores appear in JSON output under c7 key
7. All tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/10-c7-agent-evaluation/10-02-SUMMARY.md`
</output>
