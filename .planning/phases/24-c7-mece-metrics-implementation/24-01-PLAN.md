---
phase: 24-c7-mece-metrics-implementation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - internal/agent/metrics/metric.go
  - internal/agent/metrics/registry.go
  - internal/agent/metrics/m1_consistency.go
  - internal/agent/metrics/m2_comprehension.go
  - internal/agent/metrics/m3_navigation.go
  - internal/agent/metrics/m4_identifiers.go
  - internal/agent/metrics/m5_documentation.go
autonomous: true

must_haves:
  truths:
    - "5 distinct MECE metrics exist with non-overlapping evaluation scopes"
    - "Each metric defines timeout, sample count, and sample selection logic"
    - "Metrics use deterministic heuristic-based sample selection"
  artifacts:
    - path: "internal/agent/metrics/metric.go"
      provides: "Metric interface and common types"
      exports: ["Metric", "MetricResult", "Sample", "SampleResult"]
    - path: "internal/agent/metrics/registry.go"
      provides: "Metric registration and lookup"
      exports: ["AllMetrics", "GetMetric"]
    - path: "internal/agent/metrics/m1_consistency.go"
      provides: "Task Execution Consistency metric"
      contains: "type M1Consistency struct"
    - path: "internal/agent/metrics/m2_comprehension.go"
      provides: "Code Behavior Comprehension metric"
      contains: "type M2Comprehension struct"
    - path: "internal/agent/metrics/m3_navigation.go"
      provides: "Cross-File Navigation metric"
      contains: "type M3Navigation struct"
    - path: "internal/agent/metrics/m4_identifiers.go"
      provides: "Identifier Interpretability metric"
      contains: "type M4Identifiers struct"
    - path: "internal/agent/metrics/m5_documentation.go"
      provides: "Documentation Accuracy Detection metric"
      contains: "type M5Documentation struct"
  key_links:
    - from: "internal/agent/metrics/registry.go"
      to: "all 5 metric implementations"
      via: "AllMetrics() returns slice"
      pattern: "M1Consistency|M2Comprehension|M3Navigation|M4Identifiers|M5Documentation"
---

<objective>
Create the 5 MECE metric implementations for C7 agent evaluation.

Purpose: Replace C7's single overall_score with 5 distinct, research-grounded metrics that each test one isolated agent capability. This enables granular understanding of agent strengths/weaknesses.

Output: internal/agent/metrics/ package with Metric interface and 5 implementations (M1-M5).
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/24-c7-mece-metrics-implementation/24-CONTEXT.md
@.planning/phases/24-c7-mece-metrics-implementation/24-RESEARCH.md

# Existing patterns to follow
@internal/agent/types.go
@internal/agent/tasks.go
@internal/agent/executor.go
@internal/agent/scorer.go
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Metric interface and common types</name>
  <files>
    internal/agent/metrics/metric.go
    internal/agent/metrics/registry.go
  </files>
  <action>
Create the `internal/agent/metrics/` package with the Metric interface.

In `metric.go`:
```go
// Package metrics provides 5 MECE agent evaluation metrics for C7.
package metrics

import (
    "context"
    "time"
    "github.com/ingo/agent-readyness/pkg/types"
)

// Metric defines a single MECE agent evaluation capability.
type Metric interface {
    ID() string                    // e.g., "task_execution_consistency"
    Name() string                  // e.g., "Task Execution Consistency"
    Description() string           // What this metric measures
    Timeout() time.Duration        // Per-metric timeout
    SampleCount() int              // Number of samples to evaluate (1-5)
    SelectSamples(targets []*types.AnalysisTarget) []Sample
    Execute(ctx context.Context, workDir string, samples []Sample, executor Executor) MetricResult
}

// Sample represents a code sample selected for metric evaluation.
type Sample struct {
    FilePath       string  // Absolute path to file
    FunctionName   string  // Optional: specific function/method
    StartLine      int     // Optional: line range start
    EndLine        int     // Optional: line range end
    SelectionScore float64 // Score used for deterministic selection
    Description    string  // Why this sample was selected
}

// SampleResult holds the outcome of evaluating one sample.
type SampleResult struct {
    Sample   Sample
    Score    int           // 1-10 scale
    Response string        // Agent's response
    Duration time.Duration // How long this sample took
    Error    string        // Empty if successful
}

// MetricResult holds the complete outcome of a metric evaluation.
type MetricResult struct {
    MetricID    string
    MetricName  string
    Score       int           // 1-10 aggregate score
    Samples     []SampleResult
    TokensUsed  int
    Duration    time.Duration
    Error       string        // Empty if successful
}

// Executor abstracts Claude CLI execution for testability.
type Executor interface {
    ExecutePrompt(ctx context.Context, workDir, prompt, tools string, timeout time.Duration) (response string, err error)
}
```

In `registry.go`:
```go
package metrics

// allMetrics holds singleton instances of each metric.
var allMetrics = []Metric{
    NewM1Consistency(),
    NewM2Comprehension(),
    NewM3Navigation(),
    NewM4Identifiers(),
    NewM5Documentation(),
}

// AllMetrics returns all 5 MECE metrics.
func AllMetrics() []Metric {
    return allMetrics
}

// GetMetric returns a metric by ID, or nil if not found.
func GetMetric(id string) Metric {
    for _, m := range allMetrics {
        if m.ID() == id {
            return m
        }
    }
    return nil
}
```

Note: registry.go will have compile errors until Task 2 creates the metric implementations - that's expected.
  </action>
  <verify>
File exists: `ls internal/agent/metrics/metric.go`
Contains interface: `grep "type Metric interface" internal/agent/metrics/metric.go`
  </verify>
  <done>
Metric interface defined with all required methods. MetricResult, Sample, SampleResult types defined. Registry skeleton created (will complete after Task 2).
  </done>
</task>

<task type="auto">
  <name>Task 2: Implement all 5 MECE metric evaluators</name>
  <files>
    internal/agent/metrics/m1_consistency.go
    internal/agent/metrics/m2_comprehension.go
    internal/agent/metrics/m3_navigation.go
    internal/agent/metrics/m4_identifiers.go
    internal/agent/metrics/m5_documentation.go
  </files>
  <action>
Implement all 5 metrics following the patterns from 24-RESEARCH.md. Each metric:
- Implements the Metric interface
- Has a constructor function (NewM1Consistency, etc.)
- Uses deterministic heuristic-based sample selection
- Has a specific prompt and rubric for Claude evaluation

**M1: Task Execution Consistency** (`m1_consistency.go`)
- ID: "task_execution_consistency"
- Measures: Reproducibility of task completion across multiple runs
- Method: Execute same simple task 3 times, measure variance
- Sample selection: Pick 1 function with moderate complexity (5-15 cyclomatic)
- Timeout: 180s (3 runs of 60s each)
- Sample count: 1 (but runs 3 times internally)
- Scoring:
  - 10: <5% variance across runs
  - 7: 5-15% variance
  - 4: 15-30% variance
  - 1: >30% variance or failures

**M2: Code Behavior Comprehension** (`m2_comprehension.go`)
- ID: "code_behavior_comprehension"
- Measures: Agent's understanding of what code does (semantics, not syntax)
- Method: Select complex functions, ask agent to explain behavior
- Sample selection: Pick 3 functions sorted by complexity * (1/sqrt(LOC))
- Timeout: 120s
- Sample count: 3
- Scoring per sample: Accuracy of behavioral explanation
  - 10: Correctly explains all paths including edge cases
  - 7: Correct main path, minor gaps
  - 4: Partial understanding
  - 1: Fundamentally wrong

**M3: Cross-File Navigation** (`m3_navigation.go`)
- ID: "cross_file_navigation"
- Measures: Ability to trace dependencies across files
- Method: Select import chains, ask agent to trace data flow
- Sample selection: Pick 2 files with highest import count that aren't test files
- Timeout: 120s
- Sample count: 2
- Scoring: Completeness and accuracy of dependency trace
  - 10: Complete trace with all files identified
  - 7: Most files found, minor gaps
  - 4: Only direct dependencies
  - 1: Cannot navigate beyond single file

**M4: Identifier Interpretability** (`m4_identifiers.go`)
- ID: "identifier_interpretability"
- Measures: Ability to infer meaning from identifier names
- Method: Present identifiers in isolation, score semantic interpretation
- Sample selection: Pick 5 exported identifiers sorted by name length (longer = more semantic content)
- Timeout: 60s
- Sample count: 5
- Scoring: Accuracy of purpose inference from name alone
  - 10: Correctly interprets purpose
  - 7: Mostly correct
  - 4: Needs context
  - 1: Misinterprets

**M5: Documentation Accuracy Detection** (`m5_documentation.go`)
- ID: "documentation_accuracy_detection"
- Measures: Ability to detect comment/code mismatches
- Method: Present code with comments, ask agent to identify discrepancies
- Sample selection: Pick 3 documented functions (have comments) sorted by comment density
- Timeout: 60s
- Sample count: 3
- Scoring: Detection of mismatches
  - 10: Identifies all mismatches or correctly states none exist
  - 7: Identifies most
  - 4: Only obvious ones
  - 1: Cannot reliably detect

For each metric implementation:
1. Struct with config fields (sampleCount, timeout)
2. Constructor that sets defaults
3. SelectSamples() using deterministic heuristics from targets
4. Execute() that builds prompts, calls executor, parses responses, aggregates score
5. Include the LLM-as-judge rubric as a constant

Sample selection must be DETERMINISTIC:
- Sort candidates by a computed score
- Take top N
- No random.Shuffle or similar

Use the existing `types.AnalysisTarget` structure for sample selection - each target has Files with path, language, LOC, and complexity information available.
  </action>
  <verify>
All files created: `ls internal/agent/metrics/m*.go | wc -l` should be 5
Package compiles: `go build ./internal/agent/metrics/...`
All metrics registered: `grep -c "NewM" internal/agent/metrics/registry.go` should be 5
  </verify>
  <done>
All 5 MECE metrics implemented with deterministic sample selection, specific prompts, and scoring rubrics. Package compiles without errors. AllMetrics() returns all 5 metrics.
  </done>
</task>

</tasks>

<verification>
```bash
# Package structure exists
ls -la internal/agent/metrics/

# All expected files present
ls internal/agent/metrics/metric.go internal/agent/metrics/registry.go
ls internal/agent/metrics/m1_consistency.go
ls internal/agent/metrics/m2_comprehension.go
ls internal/agent/metrics/m3_navigation.go
ls internal/agent/metrics/m4_identifiers.go
ls internal/agent/metrics/m5_documentation.go

# Package compiles
go build ./internal/agent/metrics/...

# Metric interface has required methods
grep -E "(ID|Name|Description|Timeout|SampleCount|SelectSamples|Execute)" internal/agent/metrics/metric.go

# All 5 metrics are registered
go run -exec "echo" ./internal/agent/metrics 2>&1 || go build ./internal/agent/metrics
```
</verification>

<success_criteria>
- internal/agent/metrics/ package exists with metric.go, registry.go, and 5 m*.go files
- Metric interface defines ID, Name, Description, Timeout, SampleCount, SelectSamples, Execute
- All 5 metrics implement the interface and are registered in AllMetrics()
- Sample selection is deterministic (no random elements)
- Each metric has distinct evaluation scope (MECE)
- Package compiles: `go build ./internal/agent/metrics/...` succeeds
</success_criteria>

<output>
After completion, create `.planning/phases/24-c7-mece-metrics-implementation/24-01-SUMMARY.md`
</output>
