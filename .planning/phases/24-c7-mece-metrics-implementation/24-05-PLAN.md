---
phase: 24-c7-mece-metrics-implementation
plan: 05
type: execute
wave: 3
depends_on: ["24-01", "24-02", "24-03", "24-04"]
files_modified:
  - internal/analyzer/c7_agent/agent.go
autonomous: true

must_haves:
  truths:
    - "C7 analyzer uses parallel MECE metric execution"
    - "Progress display shows real-time metric status"
    - "C7Metrics result includes all 5 MECE scores"
    - "Backward compatibility maintained with legacy task execution"
  artifacts:
    - path: "internal/analyzer/c7_agent/agent.go"
      provides: "Integrated C7 analyzer with MECE metrics"
      contains: "RunMetricsParallel"
      min_lines: 150
  key_links:
    - from: "internal/analyzer/c7_agent/agent.go"
      to: "internal/agent/parallel.go"
      via: "RunMetricsParallel"
      pattern: "agent\\.RunMetricsParallel"
    - from: "internal/analyzer/c7_agent/agent.go"
      to: "internal/agent/progress.go"
      via: "NewC7Progress"
      pattern: "agent\\.NewC7Progress"
    - from: "internal/analyzer/c7_agent/agent.go"
      to: "internal/agent/metrics"
      via: "AllMetrics"
      pattern: "metrics\\.AllMetrics"
---

<objective>
Integrate parallel MECE metric execution into the C7 analyzer.

Purpose: Replace the sequential 4-task execution with parallel 5-metric execution, completing the C7 MECE implementation.

Output: Updated internal/analyzer/c7_agent/agent.go that orchestrates parallel metric execution with progress display.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/24-c7-mece-metrics-implementation/24-CONTEXT.md
@.planning/phases/24-c7-mece-metrics-implementation/24-RESEARCH.md
@.planning/phases/24-c7-mece-metrics-implementation/24-01-SUMMARY.md
@.planning/phases/24-c7-mece-metrics-implementation/24-02-SUMMARY.md
@.planning/phases/24-c7-mece-metrics-implementation/24-03-SUMMARY.md
@.planning/phases/24-c7-mece-metrics-implementation/24-04-SUMMARY.md

# File to modify
@internal/analyzer/c7_agent/agent.go
</context>

<tasks>

<task type="auto">
  <name>Task 1: Integrate parallel MECE metrics into C7 analyzer</name>
  <files>internal/analyzer/c7_agent/agent.go</files>
  <action>
Rewrite `internal/analyzer/c7_agent/agent.go` to use the new MECE metric system while preserving backward compatibility.

Key changes:
1. Import the new metrics and progress packages
2. Execute 5 MECE metrics in parallel instead of 4 sequential tasks
3. Use C7Progress for real-time display
4. Populate new C7Metrics fields while preserving legacy fields

```go
package c7

import (
    "context"
    "fmt"
    "os"
    "time"

    "github.com/ingo/agent-readyness/internal/agent"
    "github.com/ingo/agent-readyness/internal/agent/metrics"
    "github.com/ingo/agent-readyness/pkg/types"
)

// C7Analyzer implements the pipeline.Analyzer interface for C7: Agent Evaluation.
type C7Analyzer struct {
    evaluator *agent.Evaluator
    enabled   bool // only runs if explicitly enabled
}

// NewC7Analyzer creates a C7Analyzer. It's disabled by default.
func NewC7Analyzer() *C7Analyzer {
    return &C7Analyzer{enabled: false}
}

// Enable activates C7 analysis with the given CLI evaluator.
func (a *C7Analyzer) Enable(evaluator *agent.Evaluator) {
    a.evaluator = evaluator
    a.enabled = true
}

// Name returns the analyzer display name.
func (a *C7Analyzer) Name() string {
    return "C7: Agent Evaluation"
}

// Analyze runs C7 agent evaluation using 5 MECE metrics in parallel.
func (a *C7Analyzer) Analyze(targets []*types.AnalysisTarget) (*types.AnalysisResult, error) {
    if !a.enabled {
        return a.disabledResult(), nil
    }

    if len(targets) == 0 {
        return nil, fmt.Errorf("no targets provided")
    }
    rootDir := targets[0].RootDir

    // Check for Claude CLI
    if err := agent.CheckClaudeCLI(); err != nil {
        return a.disabledResult(), nil
    }

    // Create isolated workspace
    workDir, cleanup, err := agent.CreateWorkspace(rootDir)
    if err != nil {
        return nil, fmt.Errorf("create workspace: %w", err)
    }
    defer cleanup()

    // Initialize metrics
    allMetrics := metrics.AllMetrics()
    metricIDs := make([]string, len(allMetrics))
    metricNames := make([]string, len(allMetrics))
    for i, m := range allMetrics {
        metricIDs[i] = m.ID()
        metricNames[i] = m.Name()
    }

    // Create progress display
    progress := agent.NewC7Progress(os.Stderr, metricIDs, metricNames)
    progress.Start()
    defer progress.Stop()

    // Run metrics in parallel
    ctx := context.Background()
    startTime := time.Now()

    result := agent.RunMetricsParallel(ctx, workDir, targets, progress)

    // Build C7Metrics from results
    c7metrics := a.buildMetrics(result, startTime)

    return &types.AnalysisResult{
        Name:     "C7: Agent Evaluation",
        Category: "C7",
        Metrics:  map[string]interface{}{"c7": c7metrics},
    }, nil
}

// disabledResult returns a C7 result indicating the analyzer is disabled.
func (a *C7Analyzer) disabledResult() *types.AnalysisResult {
    return &types.AnalysisResult{
        Name:     "C7: Agent Evaluation",
        Category: "C7",
        Metrics:  map[string]interface{}{"c7": &types.C7Metrics{Available: false}},
    }
}

// buildMetrics constructs C7Metrics from parallel execution results.
func (a *C7Analyzer) buildMetrics(result agent.ParallelResult, startTime time.Time) *types.C7Metrics {
    m := &types.C7Metrics{
        Available:     true,
        MetricResults: make([]types.C7MetricResult, 0, len(result.Results)),
    }

    // Process each metric result
    var totalScore int
    completedCount := 0

    for _, mr := range result.Results {
        // Add to MetricResults
        metricResult := types.C7MetricResult{
            MetricID:   mr.MetricID,
            MetricName: mr.MetricName,
            Score:      mr.Score,
            Status:     "completed",
            Duration:   mr.Duration.Seconds(),
            Reasoning:  "", // Will be populated from samples
        }
        if mr.Error != "" {
            metricResult.Status = "error"
            metricResult.Reasoning = mr.Error
        }

        // Extract sample descriptions
        for _, s := range mr.Samples {
            metricResult.Samples = append(metricResult.Samples, s.Sample.Description)
        }

        m.MetricResults = append(m.MetricResults, metricResult)

        // Set individual metric scores
        switch mr.MetricID {
        case "task_execution_consistency":
            m.TaskExecutionConsistency = mr.Score
        case "code_behavior_comprehension":
            m.CodeBehaviorComprehension = mr.Score
        case "cross_file_navigation":
            m.CrossFileNavigation = mr.Score
        case "identifier_interpretability":
            m.IdentifierInterpretability = mr.Score
        case "documentation_accuracy_detection":
            m.DocumentationAccuracyDetection = mr.Score
        }

        // Track for average
        if mr.Error == "" && mr.Score > 0 {
            totalScore += mr.Score
            completedCount++
        }
    }

    // Calculate MECE score (weighted average)
    m.MECEScore = a.calculateWeightedScore(m)

    // Token and cost tracking
    m.TokensUsed = result.TotalTokens
    m.TotalDuration = time.Since(startTime).Seconds()
    // Sonnet 4.5 blended rate ~$5/MTok
    m.CostUSD = float64(m.TokensUsed) / 1_000_000 * 5.0

    return m
}

// calculateWeightedScore computes MECE score using research-based weights.
func (a *C7Analyzer) calculateWeightedScore(m *types.C7Metrics) float64 {
    // Weights from scoring config:
    // M1: 0.20, M2: 0.25, M3: 0.25, M4: 0.15, M5: 0.15
    weights := map[string]float64{
        "task_execution_consistency":       0.20,
        "code_behavior_comprehension":      0.25,
        "cross_file_navigation":            0.25,
        "identifier_interpretability":      0.15,
        "documentation_accuracy_detection": 0.15,
    }

    scores := map[string]int{
        "task_execution_consistency":       m.TaskExecutionConsistency,
        "code_behavior_comprehension":      m.CodeBehaviorComprehension,
        "cross_file_navigation":            m.CrossFileNavigation,
        "identifier_interpretability":      m.IdentifierInterpretability,
        "documentation_accuracy_detection": m.DocumentationAccuracyDetection,
    }

    totalWeight := 0.0
    weightedSum := 0.0

    for id, score := range scores {
        if score > 0 { // Only include completed metrics
            weight := weights[id]
            weightedSum += float64(score) * weight
            totalWeight += weight
        }
    }

    if totalWeight == 0 {
        return 0.0
    }
    return weightedSum / totalWeight
}
```

Key design decisions:
1. Parallel execution via RunMetricsParallel
2. Progress display on stderr (TTY-aware)
3. New fields populated: TaskExecutionConsistency through DocumentationAccuracyDetection
4. MECEScore calculated using research-based weights
5. Legacy fields not removed (but not populated by new metrics)
6. MetricResults array for detailed per-metric info
  </action>
  <verify>
File compiles: `go build ./internal/analyzer/c7_agent/...`
Uses parallel: `grep "RunMetricsParallel" internal/analyzer/c7_agent/agent.go`
Uses progress: `grep "NewC7Progress" internal/analyzer/c7_agent/agent.go`
Has weighted score: `grep "calculateWeightedScore" internal/analyzer/c7_agent/agent.go`
Sets all 5 metrics: `grep -c "m\.Task\|m\.Code\|m\.Cross\|m\.Identifier\|m\.Documentation" internal/analyzer/c7_agent/agent.go` should be >= 5
  </verify>
  <done>
C7 analyzer integrated with:
- Parallel MECE metric execution
- Real-time progress display
- All 5 metric scores populated
- Weighted MECE score calculation
- Token/cost tracking
- Backward compatible (legacy fields preserved)
  </done>
</task>

</tasks>

<verification>
```bash
# Package compiles
go build ./internal/analyzer/c7_agent/...

# Full build passes
go build ./...

# Tests pass
go test ./internal/analyzer/c7_agent/...

# Key integrations present
grep "agent.RunMetricsParallel" internal/analyzer/c7_agent/agent.go
grep "agent.NewC7Progress" internal/analyzer/c7_agent/agent.go
grep "metrics.AllMetrics" internal/analyzer/c7_agent/agent.go

# All 5 metrics assigned
grep "TaskExecutionConsistency\|CodeBehaviorComprehension\|CrossFileNavigation\|IdentifierInterpretability\|DocumentationAccuracyDetection" internal/analyzer/c7_agent/agent.go
```
</verification>

<success_criteria>
- C7 analyzer uses RunMetricsParallel for concurrent execution
- Progress display shows during evaluation
- All 5 MECE metric scores populated in C7Metrics
- MECEScore calculated with weighted average
- Package compiles and existing tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/24-c7-mece-metrics-implementation/24-05-SUMMARY.md`
</output>
