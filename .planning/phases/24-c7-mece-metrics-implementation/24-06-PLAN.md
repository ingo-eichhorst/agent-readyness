---
phase: 24-c7-mece-metrics-implementation
plan: 06
type: execute
wave: 4
depends_on: ["24-05"]
files_modified:
  - internal/agent/metrics/metric_test.go
  - internal/agent/progress_test.go
  - internal/agent/parallel_test.go
  - internal/analyzer/c7_agent/agent_test.go
autonomous: false  # Has checkpoint:human-verify task - executor will pause for user

must_haves:
  truths:
    - "Unit tests exist for metric interface and implementations"
    - "Unit tests exist for progress display"
    - "Integration test verifies parallel execution"
    - "All tests pass: go test ./... succeeds"
  artifacts:
    - path: "internal/agent/metrics/metric_test.go"
      provides: "Tests for metric implementations"
      min_lines: 100
    - path: "internal/agent/progress_test.go"
      provides: "Tests for C7Progress"
      min_lines: 50
    - path: "internal/agent/parallel_test.go"
      provides: "Tests for parallel execution"
      min_lines: 50
  key_links:
    - from: "test files"
      to: "implementations"
      via: "go test"
      pattern: "func Test"
---

<objective>
Add tests for the new MECE metric system and verify end-to-end functionality.

Purpose: Ensure all new components work correctly, both in isolation and integrated. Catch regressions before they reach production.

Output: Test files for metrics, progress, parallel execution, and updated analyzer tests.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/24-c7-mece-metrics-implementation/24-CONTEXT.md

# Existing test patterns
@internal/agent/executor_test.go
@internal/analyzer/c7_agent/agent_test.go

# Dependencies created by prior plans (will exist at execution time)
@internal/agent/metrics/metric.go
@internal/agent/metrics/registry.go
@internal/agent/progress.go
@internal/agent/parallel.go
@internal/analyzer/c7_agent/agent.go
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add unit tests for metrics package</name>
  <files>
    internal/agent/metrics/metric_test.go
    internal/agent/progress_test.go
    internal/agent/parallel_test.go
  </files>
  <action>
Create comprehensive tests following existing test patterns.

**internal/agent/metrics/metric_test.go:**
```go
package metrics

import (
    "testing"

    "github.com/ingo/agent-readyness/pkg/types"
)

func TestAllMetricsReturns5(t *testing.T) {
    metrics := AllMetrics()
    if len(metrics) != 5 {
        t.Errorf("AllMetrics() returned %d metrics, want 5", len(metrics))
    }
}

func TestMetricIDsAreUnique(t *testing.T) {
    metrics := AllMetrics()
    seen := make(map[string]bool)
    for _, m := range metrics {
        if seen[m.ID()] {
            t.Errorf("duplicate metric ID: %s", m.ID())
        }
        seen[m.ID()] = true
    }
}

func TestMetricNamesAreUnique(t *testing.T) {
    metrics := AllMetrics()
    seen := make(map[string]bool)
    for _, m := range metrics {
        if seen[m.Name()] {
            t.Errorf("duplicate metric Name: %s", m.Name())
        }
        seen[m.Name()] = true
    }
}

func TestGetMetricByID(t *testing.T) {
    tests := []struct {
        id   string
        want string
    }{
        {"task_execution_consistency", "Task Execution Consistency"},
        {"code_behavior_comprehension", "Code Behavior Comprehension"},
        {"cross_file_navigation", "Cross-File Navigation"},
        {"identifier_interpretability", "Identifier Interpretability"},
        {"documentation_accuracy_detection", "Documentation Accuracy Detection"},
    }

    for _, tc := range tests {
        m := GetMetric(tc.id)
        if m == nil {
            t.Errorf("GetMetric(%q) returned nil", tc.id)
            continue
        }
        if m.Name() != tc.want {
            t.Errorf("GetMetric(%q).Name() = %q, want %q", tc.id, m.Name(), tc.want)
        }
    }
}

func TestGetMetricUnknown(t *testing.T) {
    m := GetMetric("unknown_metric")
    if m != nil {
        t.Errorf("GetMetric(unknown) = %v, want nil", m)
    }
}

func TestM1Consistency_SelectSamples(t *testing.T) {
    m := NewM1Consistency()

    // Empty targets should return empty samples
    samples := m.SelectSamples(nil)
    if len(samples) > m.SampleCount() {
        t.Errorf("SelectSamples returned %d samples, max should be %d", len(samples), m.SampleCount())
    }
}

func TestM2Comprehension_SelectSamples(t *testing.T) {
    m := NewM2Comprehension()

    // With targets
    targets := []*types.AnalysisTarget{
        {
            RootDir:  "/test",
            Language: "go",
            Files: []types.SourceFile{
                {Path: "/test/main.go", Lines: 100, Class: types.ClassSource},
                {Path: "/test/util.go", Lines: 50, Class: types.ClassSource},
            },
        },
    }

    samples := m.SelectSamples(targets)
    if len(samples) > m.SampleCount() {
        t.Errorf("SelectSamples returned %d samples, max should be %d", len(samples), m.SampleCount())
    }
}

func TestMetricTimeoutsArePositive(t *testing.T) {
    for _, m := range AllMetrics() {
        if m.Timeout() <= 0 {
            t.Errorf("%s.Timeout() = %v, want > 0", m.ID(), m.Timeout())
        }
    }
}

func TestMetricSampleCountsArePositive(t *testing.T) {
    for _, m := range AllMetrics() {
        if m.SampleCount() <= 0 {
            t.Errorf("%s.SampleCount() = %d, want > 0", m.ID(), m.SampleCount())
        }
    }
}
```

**internal/agent/progress_test.go:**
```go
package agent

import (
    "os"
    "testing"
)

func TestNewC7Progress(t *testing.T) {
    ids := []string{"m1", "m2", "m3"}
    names := []string{"Metric 1", "Metric 2", "Metric 3"}

    p := NewC7Progress(os.Stderr, ids, names)
    if p == nil {
        t.Fatal("NewC7Progress returned nil")
    }

    if len(p.metrics) != 3 {
        t.Errorf("got %d metrics, want 3", len(p.metrics))
    }
}

func TestC7Progress_SetMetricRunning(t *testing.T) {
    ids := []string{"m1"}
    p := NewC7Progress(os.Stderr, ids, nil)

    p.SetMetricRunning("m1", 5)

    p.mu.Lock()
    defer p.mu.Unlock()

    if p.metrics["m1"].Status != StatusRunning {
        t.Errorf("status = %v, want Running", p.metrics["m1"].Status)
    }
    if p.metrics["m1"].TotalSamples != 5 {
        t.Errorf("TotalSamples = %d, want 5", p.metrics["m1"].TotalSamples)
    }
}

func TestC7Progress_SetMetricComplete(t *testing.T) {
    ids := []string{"m1"}
    p := NewC7Progress(os.Stderr, ids, nil)

    p.SetMetricComplete("m1", 8)

    p.mu.Lock()
    defer p.mu.Unlock()

    if p.metrics["m1"].Status != StatusComplete {
        t.Errorf("status = %v, want Complete", p.metrics["m1"].Status)
    }
    if p.metrics["m1"].Score != 8 {
        t.Errorf("Score = %d, want 8", p.metrics["m1"].Score)
    }
}

func TestC7Progress_AddTokens(t *testing.T) {
    p := NewC7Progress(os.Stderr, nil, nil)

    p.AddTokens(100)
    p.AddTokens(200)

    if got := p.TotalTokens(); got != 300 {
        t.Errorf("TotalTokens() = %d, want 300", got)
    }
}

func TestShortMetricID(t *testing.T) {
    tests := []struct {
        id   string
        want string
    }{
        {"task_execution_consistency", "M1"},
        {"code_behavior_comprehension", "M2"},
        {"cross_file_navigation", "M3"},
        {"identifier_interpretability", "M4"},
        {"documentation_accuracy_detection", "M5"},
        {"unknown", "un"},
    }

    for _, tc := range tests {
        got := shortMetricID(tc.id)
        if got != tc.want {
            t.Errorf("shortMetricID(%q) = %q, want %q", tc.id, got, tc.want)
        }
    }
}

func TestFormatTokens(t *testing.T) {
    tests := []struct {
        n    int
        want string
    }{
        {0, "0"},
        {999, "999"},
        {1000, "1,000"},
        {12345, "12,345"},
    }

    for _, tc := range tests {
        got := formatTokens(tc.n)
        if got != tc.want {
            t.Errorf("formatTokens(%d) = %q, want %q", tc.n, got, tc.want)
        }
    }
}
```

**internal/agent/parallel_test.go:**
```go
package agent

import (
    "context"
    "testing"

    "github.com/ingo/agent-readyness/pkg/types"
)

func TestRunMetricsParallel_NoTargets(t *testing.T) {
    ctx := context.Background()

    // Running with no targets should not panic
    result := RunMetricsParallel(ctx, "/tmp", nil, nil)

    // Should have 5 results (one per metric)
    if len(result.Results) != 5 {
        t.Errorf("got %d results, want 5", len(result.Results))
    }
}

func TestRunMetricsSequential_NoTargets(t *testing.T) {
    ctx := context.Background()

    result := RunMetricsSequential(ctx, "/tmp", nil, nil)

    if len(result.Results) != 5 {
        t.Errorf("got %d results, want 5", len(result.Results))
    }
}

func TestRunMetricsParallel_ContextCancellation(t *testing.T) {
    ctx, cancel := context.WithCancel(context.Background())
    cancel() // Cancel immediately

    // Should complete without hanging
    result := RunMetricsParallel(ctx, "/tmp", nil, nil)

    // Should still have results (possibly with errors)
    if len(result.Results) == 0 {
        t.Error("expected some results even with cancelled context")
    }
}
```
  </action>
  <verify>
Files exist: `ls internal/agent/metrics/metric_test.go internal/agent/progress_test.go internal/agent/parallel_test.go`
Tests pass: `go test ./internal/agent/... -v`
Metrics tests pass: `go test ./internal/agent/metrics/... -v`
  </verify>
  <done>
Unit tests created for metrics package (registry, selection, interface), progress display (status updates, token tracking), and parallel execution (basic operation, context cancellation).
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
Complete C7 MECE metrics implementation:
- 5 MECE metrics (M1-M5) with deterministic sample selection
- Parallel execution via errgroup
- Real-time progress display with token/cost tracking
- Updated types and scoring config
- Integrated C7 analyzer
- Comprehensive test suite
  </what-built>
  <how-to-verify>
1. Run all tests:
   ```bash
   go test ./... -v
   ```
   Expected: All tests pass

2. Build and verify help:
   ```bash
   go build -o ars .
   ./ars scan --help
   ```
   Expected: --enable-c7 flag visible

3. (Optional) Run C7 on a small project to verify progress display:
   ```bash
   ./ars scan . --enable-c7 --json | jq '.categories.C7'
   ```
   Expected: See 5 MECE metric scores (TaskExecutionConsistency through DocumentationAccuracyDetection)

4. Verify scoring config has 5 metrics:
   ```bash
   grep -c "task_execution_consistency\|code_behavior_comprehension\|cross_file_navigation\|identifier_interpretability\|documentation_accuracy_detection" internal/scoring/config.go
   ```
   Expected: 5
  </how-to-verify>
  <resume-signal>Type "approved" to proceed, or describe any issues found</resume-signal>
</task>

</tasks>

<verification>
```bash
# All tests pass
go test ./... -v

# Metrics tests specifically
go test ./internal/agent/metrics/... -v

# Progress tests
go test ./internal/agent/... -run TestC7Progress -v

# Full build
go build ./...

# Check test coverage
go test ./internal/agent/... -coverprofile=cover.out
go tool cover -func=cover.out | grep -E "agent/|metrics/"
```
</verification>

<success_criteria>
- All tests pass: `go test ./...` succeeds
- Test coverage exists for new packages (metrics, progress, parallel)
- C7 analyzer test passes
- Human verification confirms end-to-end functionality
</success_criteria>

<output>
After completion, create `.planning/phases/24-c7-mece-metrics-implementation/24-06-SUMMARY.md`
</output>
