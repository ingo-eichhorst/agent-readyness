---
phase: 25-c7-agent-evaluation-citations
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - internal/output/citations.go
  - internal/output/descriptions.go
autonomous: true

must_haves:
  truths:
    - "All 5 C7 metrics have inline citations in descriptions"
    - "C7 References section contains 9 complete citation entries"
    - "Every quantified claim in C7 metrics has explicit source attribution"
    - "Research novelty is acknowledged (preprints, 2024-2025 papers noted)"
  artifacts:
    - path: "internal/output/citations.go"
      provides: "C7 research citations"
      contains: "Category:    \"C7\""
    - path: "internal/output/descriptions.go"
      provides: "C7 metric descriptions with citations"
      contains: "task_execution_consistency"
  key_links:
    - from: "internal/output/descriptions.go"
      to: "internal/output/citations.go"
      via: "Citation references match entries"
      pattern: "Jimenez et al\\., 2024"
---

<objective>
Add research-backed citations to the 5 new C7 MECE metrics implemented in Phase 24, explicitly acknowledging the nascent state of AI agent code quality research.

Purpose: Complete the scientific foundations for v0.0.4 by documenting C7 metrics with the same rigor as C1-C6, while being transparent about the emerging nature of this research field.

Output: Updated citations.go with 9 C7 entries, updated descriptions.go with 5 metric descriptions including inline citations and Research Evidence sections.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/25-c7-agent-evaluation-citations/25-CONTEXT.md
@.planning/phases/25-c7-agent-evaluation-citations/25-RESEARCH.md

# Citation guide for format/style reference
@docs/CITATION-GUIDE.md

# Files to modify
@internal/output/citations.go
@internal/output/descriptions.go
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add C7 citations to citations.go</name>
  <files>internal/output/citations.go</files>
  <action>
Add 9 C7 citation entries to the `researchCitations` slice following the exact format of existing C1-C6 entries.

Citations to add (from 25-RESEARCH.md):

```go
// C7: Agent Evaluation Citations
{
    Category:    "C7",
    Title:       "SWE-bench: Can Language Models Resolve Real-World GitHub Issues?",
    Authors:     "Jimenez et al.",
    Year:        2024,
    URL:         "https://arxiv.org/abs/2310.06770",
    Description: "Agent evaluation methodology; established task completion benchmarks for LLMs",
},
{
    Category:    "C7",
    Title:       "AI Agents That Matter",
    Authors:     "Kapoor et al.",
    Year:        2024,
    URL:         "https://arxiv.org/abs/2407.01502",
    Description: "Identifies reproducibility gaps in agent evaluation; recommends variance reporting",
},
{
    Category:    "C7",
    Title:       "RepoGraph: Enhancing AI Software Engineering with Repository-level Code Graph",
    Authors:     "Ouyang et al.",
    Year:        2025,
    URL:         "https://arxiv.org/abs/2410.14684",
    Description: "32.8% improvement with repository-level understanding; validates cross-file navigation importance",
},
{
    Category:    "C7",
    Title:       "How Accurately Do Large Language Models Understand Code?",
    Authors:     "Haroon et al.",
    Year:        2025,
    URL:         "https://arxiv.org/abs/2504.04372",
    Description: "LLMs fail on 78% of bugs after semantic-preserving mutations; shallow comprehension finding",
},
{
    Category:    "C7",
    Title:       "A Code Comprehension Benchmark for Large Language Models for Code",
    Authors:     "Havare et al.",
    Year:        2025,
    URL:         "https://arxiv.org/abs/2507.10641",
    Description: "Code comprehension benchmark methodology; fine-tuning improves accuracy from 70% to 87.66%",
},
{
    Category:    "C7",
    Title:       "A Large-Scale Empirical Study on Code-Comment Inconsistencies",
    Authors:     "Wen et al.",
    Year:        2019,
    URL:         "https://doi.org/10.1109/ICPC.2019.00019",
    Description: "Taxonomy of 13 inconsistency types from 1.3B AST changes; foundational for M5",
},
{
    Category:    "C7",
    Title:       "Code Comment Inconsistency Detection Based on Confidence Learning",
    Authors:     "Xu et al.",
    Year:        2024,
    URL:         "https://doi.org/10.1109/TSE.2024.3358489",
    Description: "State-of-the-art CCI detection; 82.6% F1-score on 1,518 open-source projects",
},
{
    Category:    "C7",
    Title:       "Relating Identifier Naming Flaws and Code Quality: An Empirical Study",
    Authors:     "Butler et al.",
    Year:        2009,
    URL:         "https://doi.org/10.1109/WCRE.2009.50",
    Description: "Empirical correlation between identifier quality and code quality in 8 Java projects",
},
{
    Category:    "C7",
    Title:       "Code for Machines, Not Just Humans: Quantifying AI-Friendliness with Code Health Metrics",
    Authors:     "Borg et al.",
    Year:        2026,
    URL:         "https://arxiv.org/abs/2601.02200",
    Description: "Code health metrics predict AI agent reliability; 36-44% higher break rates on unhealthy code",
},
```

Note: Butler et al. 2009 and Wen et al. 2019 already exist in C2/C4 sections respectively. Following the established pattern (Phase 18-02 decision), duplicate entries per category for self-contained references.
  </action>
  <verify>
Run: `grep -c 'Category:    "C7"' internal/output/citations.go` should return 9.
Run: `go build ./...` compiles without errors.
  </verify>
  <done>9 C7 citation entries exist in citations.go with correct format matching C1-C6 patterns.</done>
</task>

<task type="auto">
  <name>Task 2: Add C7 metric descriptions to descriptions.go</name>
  <files>internal/output/descriptions.go</files>
  <action>
Add 5 C7 metric description entries to the `metricDescriptions` map after the C6 section. Follow the exact structure of existing C1-C6 entries.

Create a new section comment:
```go
// ============================================================================
// C7: Agent Evaluation Metrics
// ============================================================================
```

Add descriptions for each metric with these requirements:
1. Brief: 1-2 sentences with key citation in `<span class="citation">` format
2. Threshold: 6.0 (standard)
3. Detailed: Full HTML with Definition, Why It Matters for AI Agents, Research Evidence, Recommended Thresholds, How to Improve

For each metric, follow the patterns from 25-RESEARCH.md:

**M1 "task_execution_consistency":**
- Brief mentions 13% typical variance (Kapoor et al., 2024)
- Research Evidence cites SWE-bench (Jimenez et al., 2024) and reproducibility gaps (Kapoor et al., 2024)
- Note that variance thresholds (5%/15%/30%) are practitioner-derived heuristics

**M2 "code_behavior_comprehension":**
- Brief mentions 78% failure rate on mutations (Haroon et al., 2025)
- Research Evidence cites Haroon et al. 2025 and Havare et al. 2025
- Acknowledge that this research is emerging (2025 preprints)

**M3 "cross_file_navigation":**
- Brief mentions 32.8% improvement (Ouyang et al., 2025)
- Research Evidence cites RepoGraph (Ouyang et al., 2025) and SWE-bench (Jimenez et al., 2024)
- Note score boundaries are heuristic

**M4 "identifier_interpretability":**
- Brief cites identifier-quality correlation (Butler et al., 2009)
- Research Evidence cites Butler 2009, 2010 and Borg et al. 2026
- Acknowledge Java-specific origins of foundational research

**M5 "documentation_accuracy_detection":**
- Brief cites code-comment inconsistencies (Wen et al., 2019)
- Research Evidence cites Wen et al. 2019, Xu et al. 2024, and Borg et al. 2026
- Note 13 inconsistency types and 82.6% F1-score findings

Each description must include:
- `<em>Note:` disclaimer for heuristic thresholds or emerging research
- Proper HTML structure matching C1-C6 patterns
- Citations only in Research Evidence section and Brief (per style guide)
- No citations in "How to Improve" sections
  </action>
  <verify>
Run: `grep -c 'task_execution_consistency\|code_behavior_comprehension\|cross_file_navigation\|identifier_interpretability\|documentation_accuracy_detection' internal/output/descriptions.go` should return 5.
Run: `go build ./...` compiles without errors.
  </verify>
  <done>5 C7 metric descriptions exist with inline citations, Research Evidence sections, and heuristic disclaimer notes.</done>
</task>

<task type="auto">
  <name>Task 3: Verify citation URLs</name>
  <files></files>
  <action>
Verify all 9 C7 citation URLs are accessible using curl -I checks.

URLs to verify:
1. https://arxiv.org/abs/2310.06770 (SWE-bench)
2. https://arxiv.org/abs/2407.01502 (AI Agents That Matter)
3. https://arxiv.org/abs/2410.14684 (RepoGraph)
4. https://arxiv.org/abs/2504.04372 (LLM Code Understanding)
5. https://arxiv.org/abs/2507.10641 (Code Comprehension Benchmark)
6. https://doi.org/10.1109/ICPC.2019.00019 (Wen et al.)
7. https://doi.org/10.1109/TSE.2024.3358489 (Xu et al.)
8. https://doi.org/10.1109/WCRE.2009.50 (Butler et al.)
9. https://arxiv.org/abs/2601.02200 (Borg et al.)

For each URL:
- Run `curl -I -L --max-time 10 URL` and verify 200 OK response
- DOI URLs should redirect to publisher page (302 -> 200 is acceptable)
- ArXiv URLs should return 200 directly

If any URL fails:
- Check for typos in arxiv ID
- For DOIs, verify DOI format is correct
- Document any URLs that need manual browser verification
  </action>
  <verify>
All 9 URLs return HTTP 200 (directly or after redirect).
No broken links in citations.go.
  </verify>
  <done>All C7 citation URLs verified accessible, matching URL verification protocol from Phase 18.</done>
</task>

</tasks>

<verification>
1. `go build ./...` compiles without errors
2. `go test ./internal/output/...` passes (if tests exist)
3. `grep -c 'Category:    "C7"' internal/output/citations.go` returns 9
4. `grep 'task_execution_consistency' internal/output/descriptions.go` returns entry
5. All citation URLs verified accessible
6. Citations follow (Author, Year) format with `<span class="citation">` markup
</verification>

<success_criteria>
- [ ] 9 C7 citation entries added to citations.go
- [ ] 5 C7 metric descriptions added to descriptions.go
- [ ] All descriptions include Research Evidence section with inline citations
- [ ] Heuristic thresholds marked with disclaimer notes
- [ ] All URLs verified accessible
- [ ] Code compiles without errors
</success_criteria>

<output>
After completion, create `.planning/phases/25-c7-agent-evaluation-citations/25-01-SUMMARY.md`
</output>
