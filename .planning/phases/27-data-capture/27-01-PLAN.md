---
phase: 27-data-capture
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - internal/agent/metrics/metric.go
  - internal/agent/metrics/m1_consistency.go
  - internal/agent/metrics/m2_comprehension.go
  - internal/agent/metrics/m3_navigation.go
  - internal/agent/metrics/m4_identifiers.go
  - internal/agent/metrics/m5_documentation.go
  - internal/agent/metrics/metric_test.go
autonomous: true

must_haves:
  truths:
    - "Every metric's SampleResult contains the full prompt that was sent to Claude CLI"
    - "Every scoring function returns a ScoreTrace showing which indicators matched and their point contributions"
    - "Sum of ScoreTrace.Indicators deltas + BaseScore equals FinalScore (trace is source of truth)"
    - "Existing tests continue to pass -- scoring behavior unchanged"
  artifacts:
    - path: "internal/agent/metrics/metric.go"
      provides: "ScoreTrace, IndicatorMatch types and Prompt field on SampleResult"
      contains: "type ScoreTrace struct"
    - path: "internal/agent/metrics/m1_consistency.go"
      provides: "Prompt capture and score trace for M1"
      contains: "sr.Prompt"
    - path: "internal/agent/metrics/m2_comprehension.go"
      provides: "Prompt capture and score trace for M2"
      contains: "ScoreTrace"
    - path: "internal/agent/metrics/m3_navigation.go"
      provides: "Prompt capture and score trace for M3"
      contains: "ScoreTrace"
    - path: "internal/agent/metrics/m4_identifiers.go"
      provides: "Prompt capture and score trace for M4"
      contains: "ScoreTrace"
    - path: "internal/agent/metrics/m5_documentation.go"
      provides: "Prompt capture and score trace for M5"
      contains: "ScoreTrace"
  key_links:
    - from: "internal/agent/metrics/m2_comprehension.go"
      to: "internal/agent/metrics/metric.go"
      via: "ScoreTrace type used in SampleResult"
      pattern: "sr\\.ScoreTrace"
    - from: "internal/agent/metrics/m1_consistency.go"
      to: "internal/agent/metrics/metric.go"
      via: "Prompt field set on SampleResult"
      pattern: "sr\\.Prompt\\s*="
---

<objective>
Extend SampleResult with Prompt and ScoreTrace fields, then update all 5 metric scoring functions to populate them.

Purpose: Capture the data that flows through the metric evaluation pipeline so downstream phases (28: testing, 29: rendering) can inspect prompts, responses, and score breakdowns. This is the data layer that makes debug mode useful.

Output: All 5 metrics (M1-M5) store prompts in SampleResult.Prompt and return ScoreTrace structs from their scoring functions showing exact indicator contributions. Existing behavior and test results unchanged.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/27-data-capture/27-RESEARCH.md
@.planning/phases/26-debug-foundation/26-01-SUMMARY.md
@internal/agent/metrics/metric.go
@internal/agent/metrics/m1_consistency.go
@internal/agent/metrics/m2_comprehension.go
@internal/agent/metrics/m3_navigation.go
@internal/agent/metrics/m4_identifiers.go
@internal/agent/metrics/m5_documentation.go
@internal/agent/metrics/metric_test.go
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add ScoreTrace types and Prompt field to SampleResult</name>
  <files>internal/agent/metrics/metric.go</files>
  <action>
Add three new types and two new fields to `internal/agent/metrics/metric.go`:

1. Add `IndicatorMatch` struct:
```go
type IndicatorMatch struct {
    Name    string // e.g., "positive:returns", "negative:unclear", "length>100"
    Matched bool   // Whether the indicator was found in the response
    Delta   int    // Point contribution: +1, -1, +2, etc. (0 if !Matched)
}
```

2. Add `ScoreTrace` struct:
```go
type ScoreTrace struct {
    BaseScore  int              // Starting score before adjustments (typically 5)
    Indicators []IndicatorMatch // Each indicator checked and its result
    FinalScore int              // Score after clamping to 1-10
}
```

3. Extend `SampleResult` with two new fields (add AFTER the existing `Response` field, BEFORE `Duration`):
```go
type SampleResult struct {
    Sample     Sample
    Score      int
    Response   string        // Agent's response
    Prompt     string        // The prompt sent to the agent
    ScoreTrace ScoreTrace    // Heuristic scoring breakdown
    Duration   time.Duration
    Error      string
}
```

Do NOT modify the `Metric` interface, `Executor` interface, `Sample` type, or `MetricResult` type. Do NOT add json tags -- these are internal types, not serialized directly.
  </action>
  <verify>Run `go build ./internal/agent/metrics/...` -- should compile with no errors (existing code does not reference the new fields, so nothing breaks).</verify>
  <done>SampleResult has Prompt and ScoreTrace fields. ScoreTrace and IndicatorMatch types exist in metric.go.</done>
</task>

<task type="auto">
  <name>Task 2: Update M1-M5 to capture prompts and return ScoreTraces</name>
  <files>
    internal/agent/metrics/m1_consistency.go
    internal/agent/metrics/m2_comprehension.go
    internal/agent/metrics/m3_navigation.go
    internal/agent/metrics/m4_identifiers.go
    internal/agent/metrics/m5_documentation.go
    internal/agent/metrics/metric_test.go
  </files>
  <action>
For each of the 5 metrics, make two changes: (A) capture prompt in SampleResult, (B) refactor scoring to produce ScoreTrace.

**Important design principle:** The ScoreTrace must be the SOURCE OF TRUTH for the score, not a parallel record. The final score is computed FROM the trace indicators, not computed separately and then duplicated into the trace. This prevents the trace and score from diverging.

**A) Prompt capture (all 5 metrics):**

In each metric's `Execute()` method, add `Prompt: prompt` to the `SampleResult` struct literal where `sr` is constructed. The `prompt` variable already exists as a local before the `executor.ExecutePrompt()` call.

M1 (`m1_consistency.go`, line ~138): Add `Prompt: prompt` to the `sr := SampleResult{...}` literal.
M2 (`m2_comprehension.go`, line ~156): Add `Prompt: prompt` to the `sr := SampleResult{...}` literal.
M3 (`m3_navigation.go`, line ~149): Add `Prompt: prompt` to the `sr := SampleResult{...}` literal.
M4 (`m4_identifiers.go`, line ~235): Add `Prompt: prompt` to the `sr := SampleResult{...}` literal.
M5 (`m5_documentation.go`, line ~180): Add `Prompt: prompt` to the `sr := SampleResult{...}` literal.

**B) ScoreTrace return from scoring functions:**

Change each scoring function's signature from `func (...) int` to `func (...) (int, ScoreTrace)` and refactor the body so the trace IS the scoring mechanism.

**Pattern for M2-M5 (they all follow the same structure):**

For `scoreComprehensionResponse`, `scoreNavigationResponse`, `scoreIdentifierResponse`, `scoreDocumentationResponse`:

1. Change return type from `int` to `(int, ScoreTrace)`.
2. Initialize: `trace := ScoreTrace{BaseScore: 5}` (the base score each starts with).
3. For each indicator check, create an `IndicatorMatch` and append it. Compute `delta` based on whether matched:
   ```go
   matched := strings.Contains(responseLower, "returns")
   delta := 0
   if matched { delta = 1 }
   trace.Indicators = append(trace.Indicators, IndicatorMatch{
       Name: "positive:returns", Matched: matched, Delta: delta,
   })
   ```
4. After all indicators, compute the final score from the trace:
   ```go
   score := trace.BaseScore
   for _, ind := range trace.Indicators {
       score += ind.Delta
   }
   // Clamp to 1-10
   if score < 1 { score = 1 }
   if score > 10 { score = 10 }
   trace.FinalScore = score
   return score, trace
   ```
5. REMOVE the old inline score mutations (the `score++`, `score--` lines). The trace IS the scoring now.

**Specific per-metric details:**

**M2 (`scoreComprehensionResponse`):** 13 positive indicators (each +1): "returns", "return value", "returns the", "error", "handles", "handling", "if ", "when ", "condition", "loop", "iterate", "for each", "edge case", "corner case", "boundary", "side effect", "modifies", "updates", "validates", "checks", "ensures". 7 negative indicators (each -1): "i don't know", "unclear", "cannot determine", "might", "probably", "seems to", "not sure", "unsure". 2 length indicators: wordCount>100 (+1), wordCount>200 (+1). Name indicators like: "positive:returns", "positive:error", "negative:unclear", "length>100_words", "length>200_words".

**M3 (`scoreNavigationResponse`):** Positive indicators with weights (some are +2): "import" (+1), "from" (+1), "->" (+2), "calls" (+1), "returns" (+1), ".go" (+1), ".py" (+1), ".ts" (+1), ".js" (+1), "package" (+1), "module" (+1), "function" (+1), "exports" (+1), "provides" (+1), "dependency" (+1), "flow" (+1). Path count indicators: pathCount>3 (+1), pathCount>6 (+1). Negative indicators (each -1): "cannot find", "not found", "no file", "unable to", "cannot trace", "unknown". Length indicators: wordCount<50 (-1), wordCount>150 (+1).

**M4 (`scoreIdentifierResponse`):** Self-report accuracy: "accurate"/"correct" (+2), "mostly correct"/"partially" (+1), "incorrect"/"wrong"/"misunderstood" (-2). Positive indicators (each +1): "interpretation:", "purpose:", "function", "type", "handles", "manages", "creates", "processes", "returns", "validates", "converts", "parses". Structure indicators: "verification:" (+1), "accuracy:" (+1).

**M5 (`scoreDocumentationResponse`):** Structure indicators: "## summary" (+1), "accurate documentation"/"## accurate" (+1), "potential mismatch"/"## potential" (+1). Positive indicators with weights: "location:" (+1), "line" (+1), "comment says" (+1), "code does" (+1), "issue:" (+1), "accurate" (+1), "correctly" (+1), "describes" (+1), "matches" (+1), "documentation" (+1). Conclusion indicators: "all documentation appears accurate"/"no mismatches found"/"documentation is accurate" (+1), "mismatch" AND "line" (+1). Negative indicators (each -1): "cannot analyze", "unable to", "error reading", "no comments", "file not found". Length indicators: wordCount<50 (-1), wordCount>100 (+1).

**M1 special case (`m1_consistency.go`):** M1 does NOT have a separate scoring function. It scores inline in Execute(). Refactor the inline scoring (lines 148-158) to build a ScoreTrace. The per-run scoring checks JSON format:
- `strings.HasPrefix(response, "[") && strings.HasSuffix(response, "]")` -> score=10, indicator "json_array_exact" delta=10
- `strings.Contains(response, "[")` -> score=7, indicator "json_array_partial" delta=7
- `len(response) > 0` -> score=4, indicator "non_empty_response" delta=4
- else -> score=1, indicator "empty_response" delta=1

Use a base score of 0 for M1's per-run trace (since it uses absolute scoring, not base+adjustments).
Set `sr.ScoreTrace = trace` after computing the trace for each run.

For M1's aggregate variance scoring (lines 178-187), there's no SampleResult to attach it to -- this is a MetricResult-level concern. Leave the aggregate variance scoring as-is; the per-run traces are sufficient for Phase 27.

**Update call sites in Execute() methods:**

In each Execute() method where the scoring function is called, update from:
```go
sr.Score = m.scoreComprehensionResponse(response)
```
to:
```go
sr.Score, sr.ScoreTrace = m.scoreComprehensionResponse(response)
```

**Update existing tests:**

The existing test functions `TestM2_ScoreComprehensionResponse`, `TestM3_ScoreNavigationResponse`, `TestM4_ScoreIdentifierResponse`, `TestM5_ScoreDocumentationResponse` in `metric_test.go` call the scoring functions and check the int return. Update these to handle the `(int, ScoreTrace)` return. Change from:
```go
score := m.scoreComprehensionResponse(tc.response)
```
to:
```go
score, _ := m.scoreComprehensionResponse(tc.response)
```

**Add new ScoreTrace verification tests** in `metric_test.go`:

Add `TestScoreTrace_SumsCorrectly` that verifies for each metric (M2-M5):
1. Call the scoring function with a known response
2. Assert `trace.FinalScore == trace.BaseScore + sum(ind.Delta for ind in trace.Indicators)` (before clamping, if no clamping occurred)
3. Assert `len(trace.Indicators) > 0` for non-empty responses
4. Assert `trace.BaseScore == 5` for M2-M5

Add `TestAllMetrics_CapturePrompt` that:
1. Creates a mock executor (implement `Executor` interface returning a canned response)
2. Creates simple targets with at least one source file matching each metric's selection criteria
3. Calls each metric's Execute() method
4. Asserts every successful `SampleResult.Prompt != ""`

The mock executor:
```go
type mockExecutor struct {
    response string
}
func (m *mockExecutor) ExecutePrompt(ctx context.Context, workDir, prompt, tools string, timeout time.Duration) (string, error) {
    return m.response, nil
}
```
  </action>
  <verify>
Run `go test ./internal/agent/metrics/... -v` -- all existing tests pass plus new ScoreTrace and prompt tests pass. Run `go vet ./internal/agent/metrics/...` -- no warnings.
  </verify>
  <done>
All 5 metrics set sr.Prompt in Execute(). All 4 scoring functions (M2-M5) return (int, ScoreTrace). M1 builds ScoreTrace inline. Tests verify trace consistency (sum of deltas + base = final) and prompt capture for all 5 metrics.
  </done>
</task>

</tasks>

<verification>
1. `go build ./...` compiles cleanly
2. `go test ./internal/agent/metrics/... -v` passes all tests (old + new)
3. `go vet ./internal/agent/metrics/...` reports no issues
4. `go test ./... -count=1` full test suite passes (no regressions)
</verification>

<success_criteria>
- SampleResult has Prompt and ScoreTrace fields (metric.go)
- ScoreTrace and IndicatorMatch types defined (metric.go)
- All 5 metrics set sr.Prompt = prompt in their Execute() methods
- M2-M5 scoring functions return (int, ScoreTrace) with trace as source of truth
- M1 builds per-run ScoreTrace inline in Execute()
- New tests verify trace arithmetic (BaseScore + sum(Deltas) = FinalScore before clamp)
- New tests verify all 5 metrics capture non-empty prompts
- All existing tests pass unchanged (scoring behavior identical)
</success_criteria>

<output>
After completion, create `.planning/phases/27-data-capture/27-01-SUMMARY.md`
</output>
