---
phase: 28-heuristic-tests-scoring-fixes
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - internal/agent/metrics/testdata/c7_responses/m2_comprehension/good_go_explanation.txt
  - internal/agent/metrics/testdata/c7_responses/m2_comprehension/minimal_explanation.txt
  - internal/agent/metrics/testdata/c7_responses/m3_navigation/good_dependency_trace.txt
  - internal/agent/metrics/testdata/c7_responses/m3_navigation/shallow_trace.txt
  - internal/agent/metrics/testdata/c7_responses/m4_identifiers/accurate_interpretation.txt
  - internal/agent/metrics/testdata/c7_responses/m4_identifiers/partial_interpretation.txt
autonomous: true

must_haves:
  truths:
    - "testdata/c7_responses/ directory exists under internal/agent/metrics/ with subdirectories for M2, M3, M4"
    - "Each fixture file contains a real Claude CLI response (multi-paragraph, references actual code elements)"
    - "Fixture files are loadable by Go tests using os.ReadFile with simple relative paths"
  artifacts:
    - path: "internal/agent/metrics/testdata/c7_responses/m2_comprehension/good_go_explanation.txt"
      provides: "Real Claude response explaining code behavior of a complex Go file"
      contains: "func"
    - path: "internal/agent/metrics/testdata/c7_responses/m3_navigation/good_dependency_trace.txt"
      provides: "Real Claude response tracing dependencies across files"
      contains: ".go"
    - path: "internal/agent/metrics/testdata/c7_responses/m4_identifiers/accurate_interpretation.txt"
      provides: "Real Claude response interpreting an identifier name"
      contains: "interpretation"
  key_links:
    - from: "testdata fixture files"
      to: "Plan 03 test functions"
      via: "os.ReadFile in test setup"
      pattern: "ReadFile.*testdata.*c7_responses"
---

<objective>
Capture real Claude CLI response fixtures for M2, M3, and M4 metrics into testdata directory.

Purpose: Real response fixtures are essential for validating heuristic scoring functions against actual Claude output format. Fabricated responses mask bugs because they are written to match indicators perfectly. Real responses reveal format mismatches and scoring saturation.

Output: 6 fixture files (2 per metric) in internal/agent/metrics/testdata/c7_responses/
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/28-heuristic-tests-scoring-fixes/28-RESEARCH.md
@.planning/phases/27-data-capture/27-01-SUMMARY.md
@internal/agent/metrics/m2_comprehension.go (lines 143-151: prompt format for M2)
@internal/agent/metrics/m3_navigation.go (lines 132-144: prompt format for M3)
@internal/agent/metrics/m4_identifiers.go (lines 219-230: prompt format for M4)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Capture M2/M3/M4 real response fixtures via Claude CLI</name>
  <files>
    internal/agent/metrics/testdata/c7_responses/m2_comprehension/good_go_explanation.txt
    internal/agent/metrics/testdata/c7_responses/m2_comprehension/minimal_explanation.txt
    internal/agent/metrics/testdata/c7_responses/m3_navigation/good_dependency_trace.txt
    internal/agent/metrics/testdata/c7_responses/m3_navigation/shallow_trace.txt
    internal/agent/metrics/testdata/c7_responses/m4_identifiers/accurate_interpretation.txt
    internal/agent/metrics/testdata/c7_responses/m4_identifiers/partial_interpretation.txt
  </files>
  <action>
    Create directory structure:
    ```
    internal/agent/metrics/testdata/c7_responses/
      m2_comprehension/
      m3_navigation/
      m4_identifiers/
    ```

    Capture 6 real responses by running the Claude CLI with prompts that match the EXACT prompt templates used in the metric Execute() functions. The goal is fixtures that represent what the scoring functions actually receive in production.

    **M2 Comprehension (2 fixtures):**
    1. `good_go_explanation.txt` - Run the M2 prompt template against `internal/scoring/scorer.go` (a complex file with branching logic). Use:
       ```
       claude -p "Read the file at internal/scoring/scorer.go and explain what the code does.\n\nFocus on:\n1. The main purpose/behavior of the code\n2. Important control flow paths (branches, loops)\n3. Error handling and edge cases\n4. Return values and side effects\n\nBe specific and reference actual code elements." --allowedTools "Read,Grep" --output-format json
       ```
       Extract the `.result` field from the JSON output and save as the fixture.

    2. `minimal_explanation.txt` - Run against a simpler file `internal/agent/metrics/registry.go` (small, simple). Same prompt template but expect a shorter, less detailed response.

    **M3 Navigation (2 fixtures):**
    1. `good_dependency_trace.txt` - Run the M3 prompt template against `internal/pipeline/pipeline.go` (high import count, orchestration file). Use:
       ```
       claude -p "Examine the file at internal/pipeline/pipeline.go and trace its dependencies.\n\nYour task:\n1. List all imports/dependencies in this file\n2. For each imported module/package, identify what it provides\n3. Trace the data flow: pick one function and show how data flows from this file through other files\n\nFormat your response as:\n- Imports: [list of imports]\n- Dependency Purpose: [for each import, what it provides]\n- Data Flow Trace: [starting function] -> [calls in other files] -> [final destination]\n\nReference actual file paths and function names from the codebase." --allowedTools "Read,Glob,Grep" --output-format json
       ```

    2. `shallow_trace.txt` - Run against `internal/agent/metrics/registry.go` (minimal imports). Expect a shorter trace with fewer cross-file references.

    **M4 Identifiers (2 fixtures):**
    1. `accurate_interpretation.txt` - Run the M4 prompt template with identifier `NewM2ComprehensionMetric` in `internal/agent/metrics/m2_comprehension.go` line 26. Use:
       ```
       claude -p "Without reading the file, interpret what the identifier \"NewM2ComprehensionMetric\" means based ONLY on its name.\n\n1. What is the likely purpose of this identifier?\n2. What type of thing is it (function, type, variable, constant)?\n3. What domain/concern does it belong to?\n\nAfter your interpretation, read internal/agent/metrics/m2_comprehension.go (line 26) to verify your interpretation.\n\nFormat:\n- Interpretation: [your interpretation based on name alone]\n- Verification: [what you found in the code]\n- Accuracy: [how accurate was your interpretation?]" --allowedTools "Read" --output-format json
       ```

    2. `partial_interpretation.txt` - Run with identifier `scoreMetrics` from `internal/scoring/scorer.go`. This is a less descriptive name that may yield a less precise interpretation.

    **Important:** Each `claude` invocation returns JSON. Extract the `.result` field using `jq -r '.result'` and save that as the fixture. If `jq` is not available, parse the JSON to extract the result field. The fixture should be the raw text response, NOT the JSON wrapper.

    **If Claude CLI is not available:** Fall back to writing realistic fixture content that mimics the structure and style of real Claude responses. Each fixture should be multi-paragraph, reference actual code elements from this repository, and NOT be crafted to perfectly match scoring indicators. Include natural language variation, markdown formatting, and code references that a real agent would produce.

    Regardless of capture method, each fixture must be at least 50 words long.
  </action>
  <verify>
    ```bash
    # All 6 fixture files exist
    ls internal/agent/metrics/testdata/c7_responses/m2_comprehension/good_go_explanation.txt
    ls internal/agent/metrics/testdata/c7_responses/m2_comprehension/minimal_explanation.txt
    ls internal/agent/metrics/testdata/c7_responses/m3_navigation/good_dependency_trace.txt
    ls internal/agent/metrics/testdata/c7_responses/m3_navigation/shallow_trace.txt
    ls internal/agent/metrics/testdata/c7_responses/m4_identifiers/accurate_interpretation.txt
    ls internal/agent/metrics/testdata/c7_responses/m4_identifiers/partial_interpretation.txt

    # Each file is non-empty and substantial
    wc -w internal/agent/metrics/testdata/c7_responses/m2_comprehension/good_go_explanation.txt
    wc -w internal/agent/metrics/testdata/c7_responses/m3_navigation/good_dependency_trace.txt
    wc -w internal/agent/metrics/testdata/c7_responses/m4_identifiers/accurate_interpretation.txt

    # Existing tests still pass
    cd internal/agent/metrics && go test ./...
    ```
  </verify>
  <done>
    6 fixture files exist in internal/agent/metrics/testdata/c7_responses/ with real or realistic Claude response content. Each "good" fixture is at least 100 words. Each "minimal/shallow/partial" fixture is at least 50 words. All existing tests pass unchanged.
  </done>
</task>

</tasks>

<verification>
- `ls -R internal/agent/metrics/testdata/c7_responses/` shows 3 subdirectories with 2 files each
- `wc -w` on each fixture shows substantial content (not placeholder text)
- `go test ./internal/agent/metrics/...` passes (fixtures don't break existing tests)
</verification>

<success_criteria>
- 6 fixture files captured from real Claude CLI responses (or realistic fallbacks if CLI unavailable)
- Directory structure matches what Plan 03 tests will reference
- Fixtures are representative of actual production responses (varied quality, natural language)
</success_criteria>

<output>
After completion, create `.planning/phases/28-heuristic-tests-scoring-fixes/28-01-SUMMARY.md`
</output>
