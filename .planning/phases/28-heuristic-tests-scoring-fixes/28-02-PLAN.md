---
phase: 28-heuristic-tests-scoring-fixes
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - internal/scoring/scorer.go
  - internal/scoring/scorer_test.go
autonomous: true

must_haves:
  truths:
    - "extractC7 returns M1-M5 metric scores as float64 values in the raw values map"
    - "Formal scoring pipeline produces non-zero C7 sub-scores when C7Metrics has non-zero M1-M5 values"
    - "Legacy overall_score is still returned for backward compatibility"
  artifacts:
    - path: "internal/scoring/scorer.go"
      provides: "Updated extractC7 function returning all 6 C7 metric values"
      contains: "code_behavior_comprehension"
    - path: "internal/scoring/scorer_test.go"
      provides: "Test verifying extractC7 returns M1-M5 values and scoring produces non-zero sub-scores"
      contains: "TestExtractC7"
  key_links:
    - from: "internal/scoring/scorer.go extractC7"
      to: "internal/scoring/config.go C7 MetricThresholds"
      via: "metric name keys match between extractC7 return map and config.go Names"
      pattern: "code_behavior_comprehension.*cross_file_navigation.*identifier_interpretability"
---

<objective>
Fix extractC7 to return M1-M5 metric scores so the formal scoring pipeline produces non-zero C7 sub-scores.

Purpose: This is Bug 1 from the research -- the root cause of C7 always scoring 0/1 in the formal ScoredResult. The extractC7 function only returns `overall_score` (legacy) and does NOT return the 5 MECE metric values. The scoring pipeline therefore interpolates 0 for all M1-M5 metrics. This is a 2-line conceptual fix with a focused test.

Output: Fixed extractC7 function + test proving M1-M5 flow through scoring pipeline
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/28-heuristic-tests-scoring-fixes/28-RESEARCH.md
@internal/scoring/scorer.go (lines 345-366: current extractC7, lines 368-400: scoreMetrics)
@internal/scoring/config.go (lines 443-525: C7 category config with M1-M5 breakpoints)
@pkg/types/types.go (C7Metrics struct with M1-M5 int fields)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Fix extractC7 to return M1-M5 metric scores</name>
  <files>internal/scoring/scorer.go</files>
  <action>
    Update the `extractC7` function (around line 345) to return all M1-M5 metric scores in addition to the legacy `overall_score`.

    Current code returns only:
    ```go
    return map[string]float64{
        "overall_score": m.OverallScore,
    }, nil
    ```

    Change to:
    ```go
    return map[string]float64{
        "overall_score":                    m.OverallScore,
        "task_execution_consistency":       float64(m.TaskExecutionConsistency),
        "code_behavior_comprehension":      float64(m.CodeBehaviorComprehension),
        "cross_file_navigation":            float64(m.CrossFileNavigation),
        "identifier_interpretability":      float64(m.IdentifierInterpretability),
        "documentation_accuracy_detection": float64(m.DocumentationAccuracyDetection),
    }, nil
    ```

    The map keys MUST exactly match the `Name` fields in the C7 MetricThresholds in `config.go` (lines 464, 478, 490, 502, 514). These names are: `task_execution_consistency`, `code_behavior_comprehension`, `cross_file_navigation`, `identifier_interpretability`, `documentation_accuracy_detection`.

    Also update the `unavailable` map in the early return to list ALL metric names (not just `overall_score`), so that when C7 is not available, all 6 metrics are marked unavailable:
    ```go
    if !m.Available {
        unavailable := map[string]bool{
            "overall_score":                    true,
            "task_execution_consistency":       true,
            "code_behavior_comprehension":      true,
            "cross_file_navigation":            true,
            "identifier_interpretability":      true,
            "documentation_accuracy_detection": true,
        }
        return map[string]float64{}, unavailable
    }
    ```
  </action>
  <verify>
    ```bash
    go build ./internal/scoring/...
    ```
  </verify>
  <done>extractC7 returns all 6 C7 metric values (1 legacy + 5 MECE) as float64 in the raw values map, and marks all 6 as unavailable when C7 is not available.</done>
</task>

<task type="auto">
  <name>Task 2: Add tests for extractC7 and C7 scoring pipeline</name>
  <files>internal/scoring/scorer_test.go</files>
  <action>
    Add two test functions to `internal/scoring/scorer_test.go`:

    **Test 1: TestExtractC7_ReturnsAllMetrics**
    Verify extractC7 returns all 6 metric keys when C7Metrics is populated:
    ```go
    func TestExtractC7_ReturnsAllMetrics(t *testing.T) {
        ar := &types.AnalysisResult{
            Metrics: map[string]interface{}{
                "c7": &types.C7Metrics{
                    Available:                      true,
                    OverallScore:                   75.0,
                    TaskExecutionConsistency:       8,
                    CodeBehaviorComprehension:      7,
                    CrossFileNavigation:            6,
                    IdentifierInterpretability:     7,
                    DocumentationAccuracyDetection: 5,
                },
            },
        }

        rawValues, unavailable := extractC7(ar)

        if unavailable != nil {
            t.Errorf("expected no unavailable metrics, got %v", unavailable)
        }

        expectedKeys := []string{
            "overall_score",
            "task_execution_consistency",
            "code_behavior_comprehension",
            "cross_file_navigation",
            "identifier_interpretability",
            "documentation_accuracy_detection",
        }

        for _, key := range expectedKeys {
            val, ok := rawValues[key]
            if !ok {
                t.Errorf("missing key %q in rawValues", key)
                continue
            }
            if key != "overall_score" && val == 0 {
                t.Errorf("key %q has value 0, expected non-zero", key)
            }
        }

        // Verify specific values
        if rawValues["code_behavior_comprehension"] != 7.0 {
            t.Errorf("code_behavior_comprehension = %v, want 7.0", rawValues["code_behavior_comprehension"])
        }
    }
    ```

    **Test 2: TestExtractC7_UnavailableMarksAllMetrics**
    Verify that when C7 is not available, ALL metric names are in the unavailable set:
    ```go
    func TestExtractC7_UnavailableMarksAllMetrics(t *testing.T) {
        ar := &types.AnalysisResult{
            Metrics: map[string]interface{}{
                "c7": &types.C7Metrics{
                    Available: false,
                },
            },
        }

        _, unavailable := extractC7(ar)

        expectedUnavailable := []string{
            "overall_score",
            "task_execution_consistency",
            "code_behavior_comprehension",
            "cross_file_navigation",
            "identifier_interpretability",
            "documentation_accuracy_detection",
        }

        for _, key := range expectedUnavailable {
            if !unavailable[key] {
                t.Errorf("expected %q in unavailable set", key)
            }
        }
    }
    ```

    Ensure the test file's import block includes `"github.com/ingo/agent-readyness/pkg/types"` (it should already be there from existing tests).
  </action>
  <verify>
    ```bash
    go test ./internal/scoring/... -run TestExtractC7 -v
    go test ./internal/scoring/...
    ```
  </verify>
  <done>Both TestExtractC7_ReturnsAllMetrics and TestExtractC7_UnavailableMarksAllMetrics pass. All existing scoring tests still pass. The formal scoring pipeline now receives M1-M5 raw values and produces non-zero interpolated scores.</done>
</task>

</tasks>

<verification>
- `go test ./internal/scoring/... -v` passes all existing + 2 new tests
- `go test ./... ` passes project-wide
- Manual check: the 5 MECE metric key names in extractC7 match exactly the `Name` fields in config.go C7 MetricThresholds
</verification>

<success_criteria>
- extractC7 returns 6 keys (overall_score + 5 MECE metrics) when C7 is available
- extractC7 marks all 6 keys as unavailable when C7 is not available
- Formal scoring pipeline produces non-zero C7 sub-scores for non-zero input values
- All existing tests pass unchanged
</success_criteria>

<output>
After completion, create `.planning/phases/28-heuristic-tests-scoring-fixes/28-02-SUMMARY.md`
</output>
