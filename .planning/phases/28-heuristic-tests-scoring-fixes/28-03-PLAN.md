---
phase: 28-heuristic-tests-scoring-fixes
plan: 03
type: execute
wave: 2
depends_on: ["28-01"]
files_modified:
  - internal/agent/metrics/m2_comprehension.go
  - internal/agent/metrics/m3_navigation.go
  - internal/agent/metrics/m4_identifiers.go
  - internal/agent/metrics/m5_documentation.go
  - internal/agent/metrics/metric_test.go
autonomous: true

must_haves:
  truths:
    - "M2 scoreComprehensionResponse produces differentiated scores (not all 10/10) for varying quality fixtures"
    - "M3 scoreNavigationResponse produces differentiated scores for good vs shallow traces"
    - "M4 scoreIdentifierResponse produces differentiated scores for accurate vs partial interpretations"
    - "M5 scoreDocumentationResponse has consistent saturation fix applied"
    - "A good response scores 6-8, not 10/10"
    - "A minimal/shallow/partial response scores 4-6, not 10/10"
  artifacts:
    - path: "internal/agent/metrics/metric_test.go"
      provides: "Fixture-based tests for M2/M3/M4 scoring with real response content"
      contains: "TestM2_Score_Fixtures"
    - path: "internal/agent/metrics/m2_comprehension.go"
      provides: "Rebalanced scoring with fractional indicator weights"
      contains: "0.3"
    - path: "internal/agent/metrics/m3_navigation.go"
      provides: "Rebalanced scoring with fractional indicator weights"
      contains: "0.3"
    - path: "internal/agent/metrics/m4_identifiers.go"
      provides: "Rebalanced scoring with fractional indicator weights or grouped indicators"
      contains: "scoreIdentifierResponse"
  key_links:
    - from: "internal/agent/metrics/metric_test.go fixture tests"
      to: "internal/agent/metrics/testdata/c7_responses/"
      via: "os.ReadFile loading fixtures"
      pattern: "ReadFile.*testdata.*c7_responses"
    - from: "scoring functions"
      to: "ScoreTrace"
      via: "source-of-truth pattern"
      pattern: "trace\\.FinalScore"
---

<objective>
Write fixture-based unit tests for M2/M3/M4 scoring and fix scoring saturation so heuristics produce meaningful differentiation in the 1-10 range.

Purpose: Currently all M2/M3/M4 scoring functions over-saturate -- any decent response hits 10-15 positive indicators, pushing the score from BaseScore(5) to 15-20 before clamping to 10. This means zero differentiation between adequate and excellent responses. The fix uses fractional indicator weights so the effective scoring range spans 3-9 instead of always hitting 10.

Output: Rebalanced M2/M3/M4/M5 scoring functions + fixture-based tests proving differentiated scores
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/28-heuristic-tests-scoring-fixes/28-RESEARCH.md
@.planning/phases/27-data-capture/27-01-SUMMARY.md
@internal/agent/metrics/m2_comprehension.go (scoreComprehensionResponse function, lines 191-271)
@internal/agent/metrics/m3_navigation.go (scoreNavigationResponse function, lines 183-295)
@internal/agent/metrics/m4_identifiers.go (scoreIdentifierResponse function, lines 269-353)
@internal/agent/metrics/m5_documentation.go (scoreDocumentationResponse function, lines 214-348)
@internal/agent/metrics/metric.go (ScoreTrace, IndicatorMatch types)
@internal/agent/metrics/metric_test.go (existing test patterns including TestScoreTrace_SumsCorrectly)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add fixture-based scoring tests for M2, M3, M4</name>
  <files>internal/agent/metrics/metric_test.go</files>
  <action>
    Add 3 new test functions that load real fixture responses and assert score ranges. These tests will initially fail (showing saturation) until Task 2 fixes the scoring.

    **Add a test helper at the top of new test code:**
    ```go
    func loadFixture(t *testing.T, path string) string {
        t.Helper()
        data, err := os.ReadFile(filepath.Join("testdata", "c7_responses", path))
        if err != nil {
            t.Fatalf("failed to load fixture %s: %v", path, err)
        }
        return string(data)
    }
    ```
    Add `"os"` and `"path/filepath"` to the import block.

    **Test: TestM2_Score_Fixtures**
    ```go
    func TestM2_Score_Fixtures(t *testing.T) {
        m := NewM2ComprehensionMetric()
        tests := []struct {
            name     string
            fixture  string
            minScore int
            maxScore int
        }{
            {"good Go explanation", "m2_comprehension/good_go_explanation.txt", 6, 8},
            {"minimal explanation", "m2_comprehension/minimal_explanation.txt", 4, 6},
        }
        for _, tc := range tests {
            t.Run(tc.name, func(t *testing.T) {
                response := loadFixture(t, tc.fixture)
                score, trace := m.scoreComprehensionResponse(response)
                if score < tc.minScore || score > tc.maxScore {
                    t.Errorf("score = %d, want [%d, %d]", score, tc.minScore, tc.maxScore)
                    for _, ind := range trace.Indicators {
                        if ind.Matched {
                            t.Logf("  MATCHED: %s (delta=%.1f -> int %d)", ind.Name, float64(ind.Delta), ind.Delta)
                        }
                    }
                }
                // Verify trace integrity
                expected := trace.BaseScore
                for _, ind := range trace.Indicators {
                    expected += ind.Delta
                }
                if expected < 1 { expected = 1 }
                if expected > 10 { expected = 10 }
                if trace.FinalScore != expected {
                    t.Errorf("trace integrity: FinalScore=%d, computed=%d", trace.FinalScore, expected)
                }
                if score != trace.FinalScore {
                    t.Errorf("score %d != trace.FinalScore %d", score, trace.FinalScore)
                }
            })
        }
    }
    ```

    **Test: TestM3_Score_Fixtures** (same pattern with m3_navigation fixtures, good=6-8, shallow=4-6)

    **Test: TestM4_Score_Fixtures** (same pattern with m4_identifiers fixtures, accurate=6-8, partial=4-6)

    Run the tests BEFORE fixing the scoring functions. They will likely fail showing 10/10 for all fixtures, confirming the saturation bug. Log the test output for reference.
  </action>
  <verify>
    ```bash
    # Tests may fail before Task 2 fixes scoring - that's expected
    go test ./internal/agent/metrics/... -run "TestM[234]_Score_Fixtures" -v 2>&1 | head -80
    ```
  </verify>
  <done>3 fixture-based test functions exist (TestM2_Score_Fixtures, TestM3_Score_Fixtures, TestM4_Score_Fixtures) that load real response fixtures and assert score ranges. Tests may fail at this point due to saturation -- that is the expected state before Task 2.</done>
</task>

<task type="auto">
  <name>Task 2: Fix M2/M3/M4/M5 scoring saturation with fractional indicator weights</name>
  <files>
    internal/agent/metrics/m2_comprehension.go
    internal/agent/metrics/m3_navigation.go
    internal/agent/metrics/m4_identifiers.go
    internal/agent/metrics/m5_documentation.go
    internal/agent/metrics/metric.go
  </files>
  <action>
    The root cause of saturation: too many +1 indicators. M2 has 20 positive indicators each worth +1. A typical good response matches 10-15, pushing BaseScore(5) to 15-20 before clamping to 10. There is zero differentiation.

    **Strategy: Fractional weights + lower BaseScore**
    - Change BaseScore from 5 to 3 for all scoring functions (gives room for upward movement)
    - Change positive indicator deltas from +1 to fractional values that sum to ~5-6 when all match (target: 3 + 5 = 8, not 10)
    - Keep negative indicator deltas at -1 (they are rare and should have strong impact)
    - Keep the ScoreTrace source-of-truth pattern intact

    **Important: IndicatorMatch.Delta must change from int to float64** to support fractional weights. Update the struct in `metric.go`:
    ```go
    type IndicatorMatch struct {
        Name    string  // e.g., "positive:returns", "negative:unclear"
        Matched bool    // Whether the indicator was found
        Delta   float64 // Point contribution: +0.3, -1.0, etc. (0 if !Matched)
    }
    ```
    Also update `ScoreTrace.BaseScore` and `ScoreTrace.FinalScore` to remain as int BUT the summing logic must accumulate float64 and round at the end. Alternative: keep BaseScore as int, accumulate deltas as float64, then round the final score. The important thing is that FinalScore is still int (1-10 scale).

    Actually, simpler approach: Keep Delta as int but GROUP indicators into categories. Instead of 20 individual +1 indicators for M2, group them into 4-5 thematic groups, each worth +1, where the group fires if ANY indicator in it matches. This keeps the type system unchanged.

    **Recommended approach (grouping, preserves int types):**

    For **M2 scoreComprehensionResponse**:
    - BaseScore: 3 (was 5)
    - Group 1 "behavior_understanding" (+1): matches if ANY of ["returns", "return value", "returns the"] found
    - Group 2 "error_handling" (+1): matches if ANY of ["error", "handles", "handling"] found
    - Group 3 "control_flow" (+1): matches if ANY of ["if ", "when ", "condition", "loop", "iterate", "for each"] found
    - Group 4 "edge_awareness" (+1): matches if ANY of ["edge case", "corner case", "boundary"] found
    - Group 5 "side_effects" (+1): matches if ANY of ["side effect", "modifies", "updates"] found
    - Group 6 "validation" (+1): matches if ANY of ["validates", "checks", "ensures"] found
    - Length bonus: wordCount > 100 = +1, wordCount > 200 = +1 (keep as-is)
    - Negative indicators: keep all at -1 each (unchanged)
    - Maximum positive: BaseScore(3) + 6 groups + 2 length = 11 -> clamped to 10
    - Typical good response: 3 + 4-5 groups + 1 length = 8-9
    - Minimal response: 3 + 2-3 groups + 0 length = 5-6
    - Bad response: 3 + 1-2 groups - 2 negatives = 2-3

    Apply the same grouping pattern to **M3**, **M4**, and **M5**:

    For **M3 scoreNavigationResponse**:
    - BaseScore: 3
    - Group 1 "import_awareness" (+1): "import" or "from"
    - Group 2 "cross_file_refs" (+1): ".go" or ".py" or ".ts" or ".js"
    - Group 3 "data_flow" (+1): "->" or "calls" or "returns"
    - Group 4 "module_awareness" (+1): "package" or "module" or "exports"
    - Group 5 "purpose_mapping" (+1): "provides" or "dependency" or "flow"
    - Group 6 "depth" (+1): pathCount > 3
    - Group 7 "extensive_depth" (+1): pathCount > 6
    - Length: wordCount > 150 = +1
    - Short penalty: wordCount < 50 = -1
    - Negatives: keep at -1 each

    For **M4 scoreIdentifierResponse**:
    - BaseScore: 3
    - Group 1 "self_report_positive" (+2): "accurate" or "correct" (keep weight +2, it's the primary signal)
    - Group 2 "self_report_partial" (+1): "mostly correct" or "partially"
    - Group 3 "self_report_negative" (-2): "incorrect" or "wrong" or "misunderstood"
    - Group 4 "detailed_interpretation" (+1): ANY of ["interpretation:", "purpose:", "function", "type"]
    - Group 5 "action_words" (+1): ANY of ["handles", "manages", "creates", "processes", "returns", "validates", "converts", "parses"]
    - Group 6 "structure_verification" (+1): "verification:"
    - Group 7 "structure_accuracy" (+1): "accuracy:"

    For **M5 scoreDocumentationResponse**:
    - BaseScore: 3
    - Group 1 "structure_summary" (+1): "## summary"
    - Group 2 "accurate_section" (+1): "accurate documentation" or "## accurate"
    - Group 3 "mismatch_section" (+1): "potential mismatch" or "## potential"
    - Group 4 "specific_analysis" (+1): ANY of ["location:", "line", "comment says", "code does", "issue:"]
    - Group 5 "quality_language" (+1): ANY of ["accurate", "correctly", "describes", "matches", "documentation"]
    - Group 6 "conclusion" (+1): ANY of ["all documentation appears accurate", "no mismatches found", "documentation is accurate", "mismatch" AND "line"]
    - Length: wordCount > 100 = +1
    - Short penalty: wordCount < 50 = -1
    - Negatives: keep at -1 each

    **Implementation pattern for each group (preserves ScoreTrace visibility):**
    ```go
    // Group: behavior_understanding
    groupMatched := strings.Contains(responseLower, "returns") ||
        strings.Contains(responseLower, "return value") ||
        strings.Contains(responseLower, "returns the")
    delta := 0
    if groupMatched {
        delta = 1
    }
    trace.Indicators = append(trace.Indicators, IndicatorMatch{
        Name: "group:behavior_understanding", Matched: groupMatched, Delta: delta,
    })
    ```

    **Keep the ScoreTrace computation pattern exactly the same:**
    ```go
    score := trace.BaseScore
    for _, ind := range trace.Indicators {
        score += ind.Delta
    }
    if score < 1 { score = 1 }
    if score > 10 { score = 10 }
    trace.FinalScore = score
    ```

    After modifying all 4 scoring functions, update the `TestScoreTrace_SumsCorrectly` test's BaseScore assertions from `5` to `3` since all metrics now use BaseScore=3.

    Also update the existing synthetic-response tests (TestM2_ScoreComprehensionResponse, TestM3_ScoreNavigationResponse, TestM4_ScoreIdentifierResponse, TestM5_ScoreDocumentationResponse) to have correct expected score ranges for the new BaseScore=3 scheme:
    - Empty response: minScore=1, maxScore=3 (BaseScore 3, no indicators match)
    - Good synthetic response: minScore=5, maxScore=9 (some groups match)
    - Bad synthetic response: minScore=1, maxScore=4 (negatives push down)

    Run ALL tests to ensure everything passes with the new scoring.
  </action>
  <verify>
    ```bash
    # All tests pass including new fixture-based tests
    go test ./internal/agent/metrics/... -v

    # Fixture tests show differentiated scores (not all 10/10)
    go test ./internal/agent/metrics/... -run "TestM[234]_Score_Fixtures" -v

    # Full project test suite
    go test ./...
    ```
  </verify>
  <done>
    All 4 scoring functions (M2/M3/M4/M5) use grouped indicators with BaseScore=3. Fixture-based tests pass with differentiated scores: good fixtures score 6-8, minimal/shallow/partial fixtures score 4-6. No existing test is broken. TestScoreTrace_SumsCorrectly passes with updated BaseScore. `go test ./...` passes project-wide.
  </done>
</task>

</tasks>

<verification>
- `go test ./internal/agent/metrics/... -v` shows all tests passing including fixture-based tests
- Fixture-based tests demonstrate score differentiation (good vs minimal/shallow/partial responses)
- `go test ./...` passes project-wide (no regressions)
- ScoreTrace source-of-truth invariant maintained (verified by TestScoreTrace_SumsCorrectly)
</verification>

<success_criteria>
- M2 good fixture scores 6-8, minimal fixture scores 4-6
- M3 good fixture scores 6-8, shallow fixture scores 4-6
- M4 accurate fixture scores 6-8, partial fixture scores 4-6
- All existing tests pass (with updated score ranges where needed)
- ScoreTrace arithmetic is correct (BaseScore + sum(Deltas) = FinalScore before clamping)
</success_criteria>

<output>
After completion, create `.planning/phases/28-heuristic-tests-scoring-fixes/28-03-SUMMARY.md`
</output>
