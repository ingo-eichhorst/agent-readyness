---
phase: 29-debug-rendering-replay
plan: 02
type: execute
wave: 2
depends_on: ["29-01"]
files_modified:
  - cmd/scan.go
  - internal/agent/replay.go
  - internal/agent/replay_test.go
  - internal/agent/parallel.go
  - internal/pipeline/pipeline.go
  - internal/analyzer/c7_agent/agent.go
autonomous: true

must_haves:
  truths:
    - "Running ars scan . --debug-c7 --debug-dir ./debug-out saves responses as JSON files"
    - "Running the same command a second time replays saved responses without calling Claude CLI"
    - "--debug-dir implies --debug-c7 (no need to specify both)"
    - "Each JSON file contains metric_id, sample_index, file_path, prompt, response, duration, and error"
    - "Replay mode is detected automatically when debug-dir contains existing JSON files"
  artifacts:
    - path: "internal/agent/replay.go"
      provides: "ReplayExecutor, SaveResponses, LoadResponses"
      contains: "ReplayExecutor"
    - path: "internal/agent/replay_test.go"
      provides: "Tests for save/load/replay round-trip"
      contains: "TestSaveLoadResponses"
    - path: "cmd/scan.go"
      provides: "--debug-dir flag registration"
      contains: "debug-dir"
  key_links:
    - from: "cmd/scan.go"
      to: "internal/pipeline/pipeline.go"
      via: "p.SetDebugDir(debugDir)"
      pattern: "SetDebugDir"
    - from: "internal/pipeline/pipeline.go"
      to: "internal/analyzer/c7_agent/agent.go"
      via: "c7Analyzer.SetDebugDir or executor injection"
      pattern: "debugDir|SetDebugDir"
    - from: "internal/analyzer/c7_agent/agent.go"
      to: "internal/agent/parallel.go"
      via: "passes executor to RunMetricsParallel"
      pattern: "RunMetricsParallel"
    - from: "internal/agent/parallel.go"
      to: "internal/agent/replay.go"
      via: "uses ReplayExecutor when replay data available"
      pattern: "ReplayExecutor|replay"
---

<objective>
Implement --debug-dir flag for response persistence and replay. When --debug-dir is specified, captured C7 responses are saved as individual JSON files. On subsequent runs with the same --debug-dir, responses are loaded from disk and replayed without executing Claude CLI.

Purpose: Fulfills success criteria #2 and #3 -- response persistence and fast replay for heuristic iteration. Also satisfies RPL-01 through RPL-04.
Output: ReplayExecutor + save/load functions + CLI flag + pipeline/analyzer wiring
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/29-debug-rendering-replay/29-01-SUMMARY.md
@internal/agent/executor_adapter.go
@internal/agent/parallel.go
@internal/agent/metrics/metric.go
@internal/analyzer/c7_agent/agent.go
@internal/pipeline/pipeline.go
@cmd/scan.go
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create replay.go with DebugResponse, SaveResponses, LoadResponses, and ReplayExecutor</name>
  <files>internal/agent/replay.go, internal/agent/replay_test.go</files>
  <action>
Create `internal/agent/replay.go` in the `agent` package with the following:

**DebugResponse struct:**
```go
type DebugResponse struct {
    MetricID    string  `json:"metric_id"`
    SampleIndex int     `json:"sample_index"`
    FilePath    string  `json:"file_path"`
    Prompt      string  `json:"prompt"`
    Response    string  `json:"response"`
    Duration    float64 `json:"duration_seconds"`
    Error       string  `json:"error,omitempty"`
}
```

**SaveResponses(debugDir string, results []metrics.MetricResult) error:**
1. Call `os.MkdirAll(debugDir, 0755)` to create directory
2. Iterate over results, for each MetricResult iterate over its Samples ([]SampleResult)
3. For each sample, create a DebugResponse with:
   - MetricID from MetricResult.MetricID
   - SampleIndex from the loop index
   - FilePath from SampleResult.Sample.FilePath
   - Prompt from SampleResult.Prompt
   - Response from SampleResult.Response
   - Duration from SampleResult.Duration.Seconds()
   - Error from SampleResult.Error
4. Filename: `fmt.Sprintf("%s_%d.json", mr.MetricID, sampleIndex)`
5. Write with `json.MarshalIndent(resp, "", "  ")` and `os.WriteFile(path, data, 0644)`
6. Return wrapped errors with `fmt.Errorf`

**LoadResponses(debugDir string) (map[string]DebugResponse, error):**
1. Call `os.ReadDir(debugDir)` -- return error if directory does not exist
2. Iterate entries, skip non-.json files
3. Read each file with `os.ReadFile`, unmarshal into DebugResponse
4. Key: `fmt.Sprintf("%s_%d", resp.MetricID, resp.SampleIndex)`
5. Return the map

**ReplayExecutor struct:**
```go
type ReplayExecutor struct {
    responses map[string]DebugResponse
    callIndex map[string]int  // tracks per-metric call count for sample indexing
    mu        sync.Mutex
}
```

**NewReplayExecutor(responses map[string]DebugResponse) *ReplayExecutor** -- initialize both maps.

**ExecutePrompt(ctx, workDir, prompt, tools string, timeout time.Duration) (string, error):**
The key challenge: the Executor interface does not pass a metricID. Use the prompt text to identify the metric. Each metric has distinctive prompt patterns:
- M1 ("task_execution_consistency"): contains "list all function names" or "List all exported function"
- M2 ("code_behavior_comprehension"): contains "explain what the code does"
- M3 ("cross_file_navigation"): contains "Trace the dependencies" or "trace the complete dependency chain"
- M4 ("identifier_interpretability"): contains "interpret what the identifier" or "interpret what each identifier"
- M5 ("documentation_accuracy_detection"): contains "review the documentation" or "identify any inaccuracies"

Implement `identifyMetricFromPrompt(prompt string) string` as a helper that checks for these substrings (case-insensitive using `strings.Contains(strings.ToLower(prompt), ...)`) and returns the metric ID. If no match, return "unknown".

In ExecutePrompt:
1. Lock mutex
2. Call identifyMetricFromPrompt to get metricID
3. Get and increment callIndex[metricID]
4. Unlock mutex
5. Build key: `fmt.Sprintf("%s_%d", metricID, idx)`
6. Look up in responses map
7. If not found, return error: `fmt.Errorf("no replay data for %s", key)`
8. If resp.Error non-empty, return error: `fmt.Errorf("replayed error: %s", resp.Error)`
9. Return resp.Response, nil

Add compile-time check: `var _ metrics.Executor = (*ReplayExecutor)(nil)`

**Required imports:** `context`, `encoding/json`, `fmt`, `os`, `path/filepath`, `strings`, `sync`, `time`, and the metrics package.

**Test file (replay_test.go):**

1. `TestSaveLoadResponses` -- Create a temp dir, build mock MetricResult slice with 2 metrics, 1 sample each. Call SaveResponses. Verify files exist. Call LoadResponses. Verify map has 2 entries with correct keys and data.

2. `TestReplayExecutor` -- Build a response map with entries for M2 metric (key: "code_behavior_comprehension_0"). Create ReplayExecutor. Call ExecutePrompt with a prompt containing "explain what the code does". Assert response matches saved data. Call again -- should get "code_behavior_comprehension_1" (or error if not in map, which is expected).

3. `TestReplayExecutor_NotFound` -- Create empty ReplayExecutor. Call ExecutePrompt. Assert error contains "no replay data".

4. `TestIdentifyMetricFromPrompt` -- Table-driven test with prompts for each metric and expected IDs.
  </action>
  <verify>
`go build ./internal/agent/...` compiles.
`go test ./internal/agent/ -run TestSaveLoad -v` passes.
`go test ./internal/agent/ -run TestReplayExecutor -v` passes.
`go test ./internal/agent/ -run TestIdentifyMetric -v` passes.
  </verify>
  <done>replay.go provides SaveResponses, LoadResponses, and ReplayExecutor. Tests verify save/load round-trip and executor replay behavior.</done>
</task>

<task type="auto">
  <name>Task 2: Wire --debug-dir flag through CLI, pipeline, and C7 analyzer</name>
  <files>cmd/scan.go, internal/pipeline/pipeline.go, internal/agent/parallel.go, internal/analyzer/c7_agent/agent.go</files>
  <action>
**cmd/scan.go changes:**

1. Add variable: `var debugDir string` in the var block (after `debugC7 bool`)
2. In `init()`, add flag: `scanCmd.Flags().StringVar(&debugDir, "debug-dir", "", "directory for C7 response persistence and replay (implies --debug-c7)")`
3. In `RunE`, BEFORE the existing `if debugC7 {` block (around line 91), add:
```go
// --debug-dir implies --debug-c7
if debugDir != "" {
    debugC7 = true
    debugDir, err = filepath.Abs(debugDir)
    if err != nil {
        return fmt.Errorf("invalid debug-dir path: %w", err)
    }
}
```
Note: `err` needs to be declared (use `var err error` or `:=`). Since `err` is already used later in the function, use a simple assignment. Actually, `err` is not yet declared at this point in the function flow -- it gets declared later with `cfg, err := scoring.LoadConfig("")`. So declare it with `:=` or use a separate var. Cleanest: use a separate block:
```go
if debugDir != "" {
    debugC7 = true
    absDir, absErr := filepath.Abs(debugDir)
    if absErr != nil {
        return fmt.Errorf("invalid debug-dir path: %w", absErr)
    }
    debugDir = absDir
}
```
Place this BEFORE the scoring config load (before line 48).

4. After pipeline creation and the existing `if debugC7 { p.SetC7Debug(true) }` block, add:
```go
if debugDir != "" {
    p.SetDebugDir(debugDir)
}
```

**internal/pipeline/pipeline.go changes:**

1. Add `debugDir string` field to Pipeline struct
2. Add method:
```go
// SetDebugDir configures the directory for C7 response persistence and replay.
func (p *Pipeline) SetDebugDir(dir string) {
    p.debugDir = dir
}
```
3. Thread debugDir to C7 analyzer. In the `Run()` method, after `p.SetC7Enabled()` equivalent code (after c7Analyzer is set up), or better: in the C7 debug setup area. Add after the existing `SetC7Debug` call area:
```go
if p.debugDir != "" && p.c7Analyzer != nil {
    p.c7Analyzer.SetDebugDir(p.debugDir)
}
```
Place this near line 152 (after `SetC7Debug` method) in the method definitions. But for the Run() wiring, we need it to happen before `Analyze` is called. Actually, this should be set during pipeline configuration (not during Run). The SetDebugDir call from scan.go happens before Run() is called, so we need c7Analyzer.SetDebugDir to be called from Pipeline.SetDebugDir:

```go
func (p *Pipeline) SetDebugDir(dir string) {
    p.debugDir = dir
    if p.c7Analyzer != nil {
        p.c7Analyzer.SetDebugDir(dir)
    }
}
```

4. After `RunMetricsParallel` returns (in agent.go's Analyze method), save responses if debugDir is set. This happens in agent.go, not pipeline.go. See below.

**internal/analyzer/c7_agent/agent.go changes:**

1. Add `debugDir string` field to C7Analyzer struct
2. Add method:
```go
// SetDebugDir configures the directory for response persistence and replay.
func (a *C7Analyzer) SetDebugDir(dir string) {
    a.debugDir = dir
}
```

3. In the `Analyze()` method, after `workDir, cleanup, err := agent.CreateWorkspace(rootDir)`, determine the executor to use:
```go
// Determine executor: replay from files or live CLI
var executor metrics.Executor
if a.debugDir != "" {
    responses, loadErr := agent.LoadResponses(a.debugDir)
    if loadErr == nil && len(responses) > 0 {
        fmt.Fprintf(a.debugWriter, "[C7 DEBUG] Replay mode: loading %d responses from %s\n", len(responses), a.debugDir)
        executor = agent.NewReplayExecutor(responses)
    } else {
        fmt.Fprintf(a.debugWriter, "[C7 DEBUG] Capture mode: responses will be saved to %s\n", a.debugDir)
    }
}
```

4. Pass the executor to `RunMetricsParallel`. This requires modifying the function signature.

**internal/agent/parallel.go changes:**

Modify `RunMetricsParallel` to accept an optional executor parameter. Change signature to:
```go
func RunMetricsParallel(
    ctx context.Context,
    workDir string,
    targets []*types.AnalysisTarget,
    progress *C7Progress,
    executor metrics.Executor,  // nil = use default CLIExecutorAdapter
) ParallelResult {
```

Inside the function, change the executor creation:
```go
// Use provided executor or create default CLI adapter
if executor == nil {
    executor = NewCLIExecutorAdapter(workDir)
}
```

Remove the existing `executor := NewCLIExecutorAdapter(workDir)` line (line 35) and replace with the above.

Do the same for `RunMetricsSequential` for consistency (add executor parameter, same nil-check pattern).

5. Back in **agent.go's Analyze()**, update the call:
```go
result := agent.RunMetricsParallel(ctx, workDir, targets, progress, executor)
```
(Previously it passed nil implicitly since executor didn't exist as a param. Now pass the executor variable which may be nil for live CLI mode.)

6. After `RunMetricsParallel` returns, save responses if in capture mode (debugDir set but no replay executor):
```go
// Save responses for future replay
if a.debugDir != "" && executor == nil {
    // executor was nil means we used live CLI (not replay)
    if saveErr := agent.SaveResponses(a.debugDir, result.Results); saveErr != nil {
        fmt.Fprintf(a.debugWriter, "[C7 DEBUG] Warning: failed to save responses: %v\n", saveErr)
    } else {
        fmt.Fprintf(a.debugWriter, "[C7 DEBUG] Saved %d metric responses to %s\n", len(result.Results), a.debugDir)
    }
}
```

**Important:** The `executor` variable being `nil` indicates capture mode (live CLI). Non-nil means replay mode (we loaded from files). This is the discriminator for whether to save.
  </action>
  <verify>
`go build ./...` compiles without errors.
`go test ./... -count=1` full test suite passes (no regressions from parallel.go signature change -- check if any existing tests call RunMetricsParallel and update them).
`go test ./internal/agent/ -v` passes.
  </verify>
  <done>
--debug-dir flag registered in CLI. Pipeline threads debugDir to C7Analyzer. C7Analyzer detects replay vs capture mode. RunMetricsParallel accepts optional executor. Responses saved after live execution, loaded for replay on subsequent runs.
  </done>
</task>

</tasks>

<verification>
1. `go build ./...` compiles without errors
2. `go test ./... -count=1` full test suite passes
3. `go test ./internal/agent/ -run TestSaveLoad -v` save/load round-trip works
4. `go test ./internal/agent/ -run TestReplayExecutor -v` replay executor works
5. Verify no existing tests broke from RunMetricsParallel signature change (search for callers and update if needed)
</verification>

<success_criteria>
- `--debug-dir` flag exists and implies `--debug-c7`
- Running with `--debug-dir ./debug-out` on a real codebase creates JSON files in that directory
- JSON files contain metric_id, sample_index, file_path, prompt, response, duration, error
- Running the same command again loads responses from files (replay mode) without CLI execution
- Replay mode prints "[C7 DEBUG] Replay mode:" message on stderr
- Capture mode prints "[C7 DEBUG] Capture mode:" message on stderr
- All existing tests pass without regression
</success_criteria>

<output>
After completion, create `.planning/phases/29-debug-rendering-replay/29-02-SUMMARY.md`
</output>
