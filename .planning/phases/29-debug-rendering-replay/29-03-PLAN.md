---
phase: 29-debug-rendering-replay
plan: 03
type: execute
wave: 3
depends_on: ["29-01", "29-02"]
files_modified:
  - cmd/scan.go
  - cmd/root.go
  - README.md
autonomous: true

must_haves:
  truths:
    - "ars scan --help shows --debug-c7 flag with clear usage description"
    - "ars scan --help shows --debug-dir flag with clear usage description"
    - "README.md contains a debug mode usage section with examples"
    - "GitHub issue #55 is updated with root cause, fixes applied, and test results"
  artifacts:
    - path: "cmd/scan.go"
      provides: "Improved flag descriptions for --debug-c7 and --debug-dir"
      contains: "debug-c7"
    - path: "README.md"
      provides: "Debug mode documentation section"
      contains: "debug-c7"
  key_links:
    - from: "README.md"
      to: "cmd/scan.go"
      via: "documents the CLI flags defined in scan.go"
      pattern: "--debug-c7|--debug-dir"
---

<objective>
Update CLI help text for --debug-c7 and --debug-dir flags, add debug mode documentation to README, and update GitHub issue #55 with root cause analysis and resolution.

Purpose: Fulfills DOC-01 through DOC-04 -- documentation for the complete debug/replay feature set. This is the final plan of v0.0.5.
Output: Updated CLI help, README section, GitHub issue comment
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/29-debug-rendering-replay/29-01-SUMMARY.md
@.planning/phases/29-debug-rendering-replay/29-02-SUMMARY.md
@cmd/scan.go
@README.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Update CLI flag descriptions and README debug section</name>
  <files>cmd/scan.go, README.md</files>
  <action>
**cmd/scan.go flag description updates:**

Update the flag descriptions in `init()` to be more descriptive:

1. `--debug-c7` flag: Change description from `"enable C7 debug mode (implies --enable-c7; debug output on stderr)"` to:
   `"enable C7 debug mode: show per-metric prompts, responses, scores, and indicator traces on stderr (implies --enable-c7)"`

2. `--debug-dir` flag: Update description (from plan 02 it was set to something brief). Ensure it reads:
   `"directory for C7 response persistence and replay; saves responses on first run, replays from saved files on subsequent runs (implies --debug-c7)"`

3. Update the `scanCmd.Long` description to mention debug flags. Add a paragraph after the existing text:
```
Debug mode:
  --debug-c7            Show detailed C7 agent evaluation diagnostics on stderr
  --debug-c7 --debug-dir DIR  Save responses to DIR for offline analysis and replay
```

**README.md debug mode section:**

Find the appropriate location in README.md (after the "Usage" or "Flags" section, or in a "Debug Mode" subsection). Add a new section:

```markdown
### C7 Debug Mode

When investigating C7 Agent Evaluation scores, use debug mode to inspect what the agent sees and how responses are scored:

\`\`\`bash
# Show debug output on stderr (normal output unchanged on stdout)
ars scan . --debug-c7

# Pipe normal output to file while viewing debug on terminal
ars scan . --debug-c7 > results.json --json 2>debug.log

# Save responses for offline analysis
ars scan . --debug-c7 --debug-dir ./c7-debug

# Replay saved responses (fast, no Claude CLI calls)
ars scan . --debug-c7 --debug-dir ./c7-debug
\`\`\`

Debug output includes:
- Per-metric, per-sample prompt text (truncated)
- Full agent response (truncated in terminal, full in saved files)
- Score breakdown with heuristic indicator traces
- Timing data per sample and per metric

The `--debug-dir` flag enables response persistence:
- **First run**: Executes Claude CLI normally and saves all responses as JSON files
- **Subsequent runs**: Loads saved responses instead of calling Claude CLI (replay mode)
- Replay mode enables fast iteration on heuristic scoring without API costs

Debug output goes exclusively to stderr, so JSON output (`--json`) remains valid on stdout.
```

Place this section after the existing usage/flags documentation. Look for natural placement near the `--enable-c7` documentation if it exists, or create a new "### Advanced" or "### Debug Mode" subsection.
  </action>
  <verify>
`go build ./cmd/...` compiles.
`go run . scan --help` shows updated descriptions for --debug-c7 and --debug-dir.
Visually verify README.md has the new debug section with correct markdown formatting.
  </verify>
  <done>CLI help text clearly describes --debug-c7 and --debug-dir flags. README contains debug mode documentation with usage examples.</done>
</task>

<task type="auto">
  <name>Task 2: Update GitHub issue #55 with root cause and resolution</name>
  <files></files>
  <action>
Use `gh issue comment` to add a comprehensive update to GitHub issue #55. The comment should contain:

**Root Cause Analysis:**

The M2/M3/M4 scoring bug had two root causes:

1. **Bug 1 (extractC7):** The `extractC7()` function in `internal/scoring/extract.go` only returned the `overall_score` metric, not the individual M1-M5 MECE metrics. The scoring pipeline received zero values for all 5 metrics, producing 0/10 or 1/10 scores. Fixed in Phase 28-02 by updating extractC7 to return all 6 C7 metrics.

2. **Bug 2 (Scoring saturation):** Individual heuristic indicators (e.g., 15+ indicators for M2) caused premature score capping at 10/10 for any non-trivial response. The scoring function added +1 per matched indicator, but real responses matched many indicators simultaneously. Fixed in Phase 28-03 by switching to grouped indicators -- each thematic group (e.g., "code_structure" group) contributes at most +1 regardless of how many individual indicators within the group match. Variable BaseScore per metric (M2=2, M3=2, M4=1, M5=3) tuned to produce realistic score ranges.

**Fixes Applied:**
- Phase 28-02: `extractC7()` now returns `overall_score` + 5 MECE metrics
- Phase 28-03: Grouped indicator scoring with variable BaseScore per metric
- Phase 28-03: M4 uses "accurate" instead of "correct" to avoid false-positive on "partially correct"

**Test Results:**
- Fixture-based unit tests for M2, M3, M4 scoring with real Claude CLI response fixtures
- M2 scores: 5-8/10 range (was 0/10)
- M3 scores: 5-8/10 range (was 0/10)
- M4 scores: 4-7/10 range (was 0/10)
- All tests passing: `go test ./internal/agent/metrics/ -run TestM[234]_Score -v`

**Debug Infrastructure (v0.0.5):**
- `--debug-c7`: Show per-metric debug data on stderr (prompts, responses, scores, indicator traces)
- `--debug-dir DIR`: Save/replay responses for offline heuristic iteration
- Response fixtures in `testdata/c7_responses/` for regression testing

Run this command:
```bash
gh issue comment 55 --body "$(cat <<'GHEOF'
## Root Cause Analysis & Resolution

### Bug 1: extractC7 missing MECE metrics
`extractC7()` in `internal/scoring/extract.go` only returned `overall_score`, not the 5 individual MECE metrics (M1-M5). The scoring pipeline received zero values, producing 0/10 scores.

**Fix:** Updated extractC7 to return all 6 C7 metrics (Phase 28-02).

### Bug 2: Heuristic scoring saturation
Individual indicators (+1 each) caused premature capping at 10/10 for any real response. M2 had 15+ indicators that nearly all matched simultaneously.

**Fix:** Grouped indicators â€” each thematic group contributes max +1 regardless of individual matches. Variable BaseScore per metric (M2=2, M3=2, M4=1, M5=3) tuned to realistic ranges (Phase 28-03).

### Current Score Ranges (validated with fixture tests)
| Metric | Before Fix | After Fix | Test Command |
|--------|-----------|-----------|--------------|
| M2 (Comprehension) | 0/10 | 5-8/10 | `go test ./internal/agent/metrics/ -run TestM2` |
| M3 (Navigation) | 0/10 | 5-8/10 | `go test ./internal/agent/metrics/ -run TestM3` |
| M4 (Identifiers) | 0/10 | 4-7/10 | `go test ./internal/agent/metrics/ -run TestM4` |

### Debug Infrastructure Added (v0.0.5)
- `--debug-c7`: Shows per-metric prompts, responses, scores, and indicator traces on stderr
- `--debug-dir DIR`: Saves responses as JSON for offline analysis; replays on subsequent runs
- Fixture responses in `testdata/c7_responses/` for regression testing

All tests passing. Closing this issue.
GHEOF
)"
```

Then close the issue:
```bash
gh issue close 55
```

If issue #55 does not exist or the gh command fails due to auth, log the intended comment content to stderr and continue. Do NOT fail the plan over a GitHub API issue -- the code changes are the priority.
  </action>
  <verify>
`gh issue view 55` shows the comment was added (or log that it was skipped due to auth).
Issue #55 is closed (or logged as skipped).
  </verify>
  <done>GitHub issue #55 has root cause analysis, fix details, score ranges, and debug infrastructure summary. Issue is closed.</done>
</task>

</tasks>

<verification>
1. `go build ./...` compiles
2. `go run . scan --help` shows updated --debug-c7 and --debug-dir descriptions
3. README.md contains debug mode section with usage examples
4. GitHub issue #55 updated and closed (or logged as skipped if auth unavailable)
</verification>

<success_criteria>
- `ars scan --help` output includes clear descriptions for both --debug-c7 and --debug-dir
- README.md has a "C7 Debug Mode" section with 4 example commands
- GitHub issue #55 documents root cause (extractC7 + scoring saturation), fixes, and test results
- Issue #55 is closed
</success_criteria>

<output>
After completion, create `.planning/phases/29-debug-rendering-replay/29-03-SUMMARY.md`
</output>
