---
phase: 33-improvement-prompt-modals
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - internal/output/prompt.go
  - internal/output/prompt_test.go
autonomous: true

must_haves:
  truths:
    - "renderImprovementPrompt produces a 4-section prompt (Context, Build & Test, Task, Verification) for any C1-C6 metric"
    - "renderImprovementPrompt produces a prompt for C7 metrics using description-based guidance instead of breakpoints"
    - "Prompts interpolate the current score, target score, and evidence file paths"
    - "Metrics with no evidence produce prompts without a Files to Focus On section"
    - "nextTarget computes the next achievable breakpoint from current score"
  artifacts:
    - path: "internal/output/prompt.go"
      provides: "PromptParams struct, renderImprovementPrompt(), nextTarget(), languageBuildCommands()"
      min_lines: 120
    - path: "internal/output/prompt_test.go"
      provides: "Unit tests for prompt rendering and target calculation"
      min_lines: 60
  key_links:
    - from: "internal/output/prompt.go"
      to: "pkg/types/scoring.go"
      via: "EvidenceItem type usage"
      pattern: "types\\.EvidenceItem"
    - from: "internal/output/prompt.go"
      to: "internal/scoring/config.go"
      via: "Breakpoint type for target calculation"
      pattern: "scoring\\.Breakpoint"
---

<objective>
Create the prompt rendering engine that generates research-backed improvement prompts for all metrics.

Purpose: This is the data layer for the "Improve" button feature. Each prompt follows the 4-section structure (Context / Build & Test / Task / Verification) and interpolates project-specific data including current score, target score, and evidence file paths.

Output: `internal/output/prompt.go` with `renderImprovementPrompt()` function and `internal/output/prompt_test.go` with unit tests.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/33-improvement-prompt-modals/33-RESEARCH.md
@internal/output/trace.go
@internal/output/html.go
@internal/output/descriptions.go
@internal/scoring/config.go
@pkg/types/scoring.go
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create prompt rendering engine</name>
  <files>internal/output/prompt.go</files>
  <action>
Create `internal/output/prompt.go` in the `output` package with:

1. **PromptParams struct** with fields:
   - CategoryName string (e.g., "C1")
   - CategoryDisplay string (e.g., "C1: Code Health")
   - CategoryImpact string (from categoryImpact())
   - MetricName string (e.g., "complexity_avg")
   - MetricDisplay string (e.g., "Complexity avg")
   - RawValue float64
   - FormattedValue string
   - Score float64
   - TargetScore float64 (computed via nextTarget)
   - TargetValue float64 (computed via nextTarget)
   - HasBreakpoints bool (false for C7)
   - Evidence []types.EvidenceItem
   - Language string (detected language for build commands)

2. **renderImprovementPrompt(params PromptParams) string** function that builds HTML containing a copyable `<pre><code>` block with the 4-section prompt. Use `strings.Builder` (same pattern as trace.go).

   The prompt text (inside the code block) should be plain text (NOT HTML) that the user will copy. Use `template.HTMLEscapeString()` for all interpolated values in the HTML wrapper, but the actual prompt content inside `<code>` should read as natural plain text when copied via `.textContent`.

   **Section structure (rendered as plain text inside `<pre><code>`):**

   ```
   ## Context

   I'm working on improving the {MetricDisplay} metric in this codebase.
   Current score: {Score}/10 (raw value: {FormattedValue})
   Target score: {TargetScore}/10 (target value: {TargetValue})

   Category: {CategoryDisplay}
   Why it matters: {CategoryImpact}

   ## Build & Test Commands

   {language-appropriate commands from languageBuildCommands()}

   ## Task

   {metric-specific task from getMetricTaskGuidance()}

   ### Files to Focus On
   (only if len(Evidence) > 0)

   1. {filepath}:{line} - {description} (value: {value})
   ... up to 5 items

   ## Verification

   After making changes:
   {language-appropriate test command}
   Then re-scan: ars scan . --output-html /tmp/report.html
   Check that the {MetricDisplay} score has improved above {TargetScore}.
   ```

   Wrap the `<pre><code>` in a container div with class `prompt-copy-container`:
   ```html
   <div class="prompt-copy-container">
     <button class="trace-copy-btn" onclick="copyPromptText(this)">Copy</button>
     <pre><code>{escaped prompt text}</code></pre>
   </div>
   ```

3. **nextTarget(score float64, breakpoints []scoring.Breakpoint) (targetValue float64, targetScore float64)** function:
   - Find the breakpoint that gives the next higher score above current score.
   - Handle both ascending breakpoints (higher value = higher score, e.g., coverage) and descending breakpoints (higher value = lower score, e.g., complexity). Use the same ascending detection as `findCurrentBand()`: check if `breakpoints[0].Score < breakpoints[last].Score`.
   - If already at max score (10), return current values.
   - If no better breakpoint exists, target score 10 with the best breakpoint value.

4. **languageBuildCommands(lang string) string** helper:
   - "go" -> "go build ./...\ngo test ./..."
   - "python" -> "python -m pytest"
   - "typescript" -> "npm test"
   - Default -> "# (adjust build/test commands for your project)"
   - Add a note: "(adjust commands for your project if different)"

5. **getMetricTaskGuidance(metricName string) string** helper:
   - Extract the "How to Improve" bullet list from `metricDescriptions` in descriptions.go. Parse the `Detailed` field to find the `<h4>How to Improve</h4>` section and extract the `<li>` items as plain text bullet points.
   - If no "How to Improve" section found, return a generic: "Review the metric description for improvement guidance."
   - Prepend a metric-specific action line derived from the metric name and current/target values. For example, for complexity_avg: "Reduce the average cyclomatic complexity from {RawValue} to {TargetValue} or below."

Important implementation notes:
- Import `html/template` for HTMLEscapeString (not `text/template`)
- Import `scoring` package for Breakpoint type
- Import `types` package for EvidenceItem
- Use `fmt.Sprintf("%.1f", ...)` for scores, `fmt.Sprintf("%.4g", ...)` for raw values (matching trace.go formatting)
- For C7 metrics where HasBreakpoints is false: skip the target value line, use "Improve score above {current+2, max 10}" as target, and use description guidance only
  </action>
  <verify>
  Run `go build ./internal/output/...` -- must compile without errors.
  </verify>
  <done>
  `renderImprovementPrompt()` returns non-empty HTML string containing all 4 sections for a sample C1 metric with evidence, and returns a prompt without the "Files to Focus On" section when evidence is empty.
  </done>
</task>

<task type="auto">
  <name>Task 2: Unit tests for prompt rendering</name>
  <files>internal/output/prompt_test.go</files>
  <action>
Create `internal/output/prompt_test.go` with tests:

1. **TestRenderImprovementPrompt_C1Metric**: Create PromptParams for complexity_avg with score 4.2, raw value 18.3, 3 evidence items, Go language. Assert:
   - Result is non-empty
   - Contains "## Context"
   - Contains "## Build & Test Commands"
   - Contains "## Task"
   - Contains "## Verification"
   - Contains "4.2/10"
   - Contains "go test ./..."
   - Contains evidence file paths

2. **TestRenderImprovementPrompt_NoEvidence**: Create PromptParams with empty evidence slice. Assert:
   - Result is non-empty
   - Does NOT contain "### Files to Focus On"
   - Still contains all 4 sections

3. **TestRenderImprovementPrompt_C7Metric**: Create PromptParams for task_execution_consistency with HasBreakpoints=false, score 6.0. Assert:
   - Result is non-empty
   - Contains improvement guidance text
   - Does not crash or return empty

4. **TestNextTarget_Descending**: Test with complexity breakpoints (descending: higher value = lower score). Current score 4.0. Assert target score is higher than 4.0 and target value is lower than current raw value.

5. **TestNextTarget_Ascending**: Test with coverage breakpoints (ascending: higher value = higher score). Current score 5.0. Assert target score is higher than 5.0 and target value is higher than current raw value.

6. **TestNextTarget_MaxScore**: Test with score 10.0. Assert returns current values (no improvement needed).

7. **TestGetMetricTaskGuidance**: Test with "complexity_avg" -- assert returns non-empty string containing "complexity" or improvement guidance text.

Use the breakpoint data from `scoring.DefaultConfig()` for nextTarget tests to ensure realistic test data.
  </action>
  <verify>
  Run `go test ./internal/output/ -run TestRenderImprovementPrompt -v && go test ./internal/output/ -run TestNextTarget -v && go test ./internal/output/ -run TestGetMetricTaskGuidance -v` -- all tests pass.
  </verify>
  <done>
  7 test functions pass, covering C1-C6 prompts, C7 prompts, empty evidence, target calculation for ascending/descending/max, and task guidance extraction.
  </done>
</task>

</tasks>

<verification>
- `go build ./internal/output/...` compiles
- `go test ./internal/output/ -run "TestRenderImprovementPrompt|TestNextTarget|TestGetMetricTaskGuidance" -v` passes all tests
- `go test ./...` still passes (no regressions)
</verification>

<success_criteria>
- renderImprovementPrompt() produces well-structured 4-section prompts
- nextTarget() correctly computes improvement targets for both ascending and descending breakpoints
- C7 metrics get prompts based on description guidance (no breakpoints)
- Empty evidence arrays produce prompts without the files section
- All unit tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/33-improvement-prompt-modals/33-01-SUMMARY.md`
</output>
