---
phase: 33-improvement-prompt-modals
plan: 03
type: execute
wave: 2
depends_on: ["33-01"]
files_modified:
  - internal/output/html_test.go
autonomous: true

must_haves:
  truths:
    - "A test verifies that HTML reports contain Improve buttons for low-scoring metrics"
    - "A test verifies that prompts contain all 4 required sections"
    - "A test verifies that high-scoring metrics (>= 9.0) do not get Improve buttons"
    - "A test verifies prompt template coverage for all 7 categories"
  artifacts:
    - path: "internal/output/html_test.go"
      provides: "Integration tests for prompt rendering in HTML output"
      contains: "TestHTMLGenerator_PromptModals"
  key_links:
    - from: "internal/output/html_test.go"
      to: "internal/output/html.go"
      via: "GenerateReport call with trace data producing prompt HTML"
      pattern: "GenerateReport"
---

<objective>
Add integration tests validating that prompt modals render correctly in generated HTML reports.

Purpose: Ensures the end-to-end pipeline from score data through prompt rendering to HTML output works correctly, covering the prompt coverage requirement (PR-07: all 7 categories) and the progressive enhancement requirement (PR-09).

Output: New test functions in `internal/output/html_test.go`.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/33-improvement-prompt-modals/33-01-SUMMARY.md
@internal/output/html_test.go
@internal/output/html.go
@internal/output/prompt.go
@internal/scoring/config.go
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add HTML integration tests for prompt modals</name>
  <files>internal/output/html_test.go</files>
  <action>
Add the following test functions to `internal/output/html_test.go`:

1. **TestHTMLGenerator_PromptModals**: Create a ScoredResult with all 7 categories, each having at least one metric with score below 9.0 and evidence items. Create a TraceData with ScoringConfig from `scoring.DefaultConfig()` and Languages: []string{"go"}. Call GenerateReport and assert:
   - Output contains `"Improve"` button text
   - Output contains `"prompt-copy-container"` class
   - Output contains `"copyPromptText"` function
   - Output contains `"## Context"` (the prompt section header, HTML-escaped)
   - Output contains `"## Build"` (part of Build & Test section)
   - Output contains `"## Task"`
   - Output contains `"## Verification"`
   - Output contains `<template id="prompt-complexity_avg">` (or similar metric key)

   Build the ScoredResult using a helper or inline with categories C1-C7. For C1-C6, use real metric names from scoring.DefaultConfig(). For C7, use the 5 C7 metric names (task_execution_consistency, code_behavior_comprehension, cross_file_navigation, identifier_interpretability, documentation_accuracy_detection) with appropriate weights (0.20 each).

   For evidence, create at least 2 EvidenceItem entries per metric: `{FilePath: "internal/foo.go", Line: 42, Value: 15.0, Description: "high complexity"}`.

2. **TestHTMLGenerator_PromptModals_HighScore**: Create a ScoredResult where all metrics have score >= 9.0. Call GenerateReport and assert:
   - Output does NOT contain `<template id="prompt-`
   - Output does NOT contain `prompt-copy-container`
   (Verifies that high-scoring metrics correctly skip prompt generation)

3. **TestHTMLGenerator_PromptModals_AllCategories**: Create a ScoredResult with one representative metric per category (7 total), all with score 5.0. Assert that the output contains `<template id="prompt-` at least 7 times (one per metric). Use `strings.Count()` to verify.

For constructing test ScoredResults:
- Use `scoring.DefaultConfig()` to get category weights
- Set `Available: true` for all metrics
- Set `Evidence: []types.EvidenceItem{{FilePath: "test.go", Line: 1, Value: 10.0, Description: "test"}}` (at minimum)
- The TraceData must have a valid ScoringConfig for breakpoint lookup to work

Note: Existing tests in html_test.go pass `nil` for trace data. The new tests need non-nil TraceData with ScoringConfig and Languages populated.
  </action>
  <verify>
  Run `go test ./internal/output/ -run TestHTMLGenerator_PromptModals -v` -- all 3 test functions pass.
  Run `go test ./internal/output/...` -- all existing tests still pass.
  </verify>
  <done>
  3 integration tests validate: (1) prompt modals render with all 4 sections and copy button, (2) high-scoring metrics skip prompts, (3) all 7 categories have prompt templates in output.
  </done>
</task>

</tasks>

<verification>
- `go test ./internal/output/ -run TestHTMLGenerator_PromptModals -v` -- all pass
- `go test ./...` -- all pass (no regressions)
</verification>

<success_criteria>
- Tests prove all 7 categories produce prompt templates in HTML output
- Tests prove high-scoring metrics (>= 9.0) do not get prompt buttons
- Tests prove prompt HTML contains all 4 required sections
- No regressions in existing test suite
</success_criteria>

<output>
After completion, create `.planning/phases/33-improvement-prompt-modals/33-03-SUMMARY.md`
</output>
