---
phase: 34-testing-and-quality
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - internal/scoring/scorer_test.go
  - internal/output/json_test.go
autonomous: true

must_haves:
  truths:
    - "Evidence extraction tests verify non-empty evidence for all 7 categories when violations exist"
    - "Evidence arrays are always [] not nil for metrics without violations"
    - "JSON backward compatibility test confirms v1 JSON loads without error"
  artifacts:
    - path: "internal/scoring/scorer_test.go"
      provides: "Evidence extraction tests for C1-C7"
      contains: "TestExtractEvidence"
    - path: "internal/output/json_test.go"
      provides: "Enhanced backward compatibility test"
      contains: "TestJSONBaselineV1"
  key_links:
    - from: "internal/scoring/scorer_test.go"
      to: "internal/scoring/scorer.go"
      via: "extractC1..extractC7 function calls"
      pattern: "extractC[1-7]"
---

<objective>
Add evidence extraction tests for all 7 categories and enhance JSON schema backward compatibility testing.

Purpose: Validate that the evidence data flow (Phase 30) correctly populates non-empty evidence for all metric categories when violations exist, and that v0.0.5-era JSON still loads correctly in the current schema.
Output: New test functions in scorer_test.go and json_test.go
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/34-testing-and-quality/34-RESEARCH.md

@internal/scoring/scorer.go
@internal/scoring/scorer_test.go
@internal/scoring/config.go
@internal/output/json.go
@internal/output/json_test.go
@pkg/types/types.go
</context>

<tasks>

<task type="auto">
  <name>Task 1: Evidence extraction tests for all 7 categories</name>
  <files>internal/scoring/scorer_test.go</files>
  <action>
Add tests that verify extractC1 through extractC7 return non-empty evidence when given AnalysisResults with known violations. For each category:

1. Construct a synthetic `types.AnalysisResult` with the category's `*Metrics` struct populated with data that would produce evidence (e.g., high-complexity functions for C1, deep directories for C3, low coverage for C6).

2. Call the corresponding `extractCx` function and verify:
   - The returned evidence map is not nil
   - For metrics that have violations in the synthetic data, evidence is non-empty (`len(items) > 0`)
   - Each EvidenceItem has non-empty FilePath and Description fields
   - Evidence arrays are `[]types.EvidenceItem{}` (not nil) for metrics without violations

Use the existing test helpers (e.g., `makeHealthyC1()`) as reference for constructing AnalysisResults, but modify values to ensure violations exist (e.g., set complexity to 30, function length to 100).

Structure as a table-driven test `TestExtractEvidence_AllCategories` with subtests per category.

For C4 and C5, check the actual extractC4/extractC5 signatures -- they may produce evidence only for certain metrics (binary metrics like `changelog_present` may have empty evidence). That is acceptable; test that the evidence map itself is returned (not nil).

For C7, evidence extraction is different (score-based, no file-level evidence). Verify the function returns the expected evidence map structure even if empty.

Important: Look at the actual extractCx function signatures in scorer.go to understand what types they accept and return. The evidence is the third return value from these functions.
  </action>
  <verify>Run `go test ./internal/scoring/ -run TestExtractEvidence -v` -- all subtests pass</verify>
  <done>Tests verify evidence extraction for all 7 categories; non-nil maps returned; non-empty evidence for categories with synthetic violations</done>
</task>

<task type="auto">
  <name>Task 2: Enhanced JSON backward compatibility test</name>
  <files>internal/output/json_test.go</files>
  <action>
The existing `TestJSONBaselineBackwardCompatibility` test in json_test.go already covers the basic v1 JSON backward compatibility scenario. Enhance it or add a companion test `TestJSONBaselineV1FullRoundTrip` that:

1. Constructs a more complete v1-era JSON string with all 7 categories (C1-C7), each with at least one metric using the old "metrics" field name
2. Verifies `json.Unmarshal` succeeds without error
3. Verifies all category-level fields load correctly (name, score, weight)
4. Verifies composite_score and tier load correctly
5. Verifies that the code does NOT crash when accessing Categories that had "metrics" (old field name) instead of "sub_scores" (new field name)
6. Adds a test that marshals a current v2 JSONReport and verifies it uses "sub_scores" not "metrics"

Check the existing test first -- if it already covers these cases adequately, skip adding duplicate tests. Only add what is genuinely missing.

Note: The existing test already notes that SubScores will be empty when reading old "metrics" JSON because the tag changed. This is expected behavior since baseline loading only reads category-level scores.
  </action>
  <verify>Run `go test ./internal/output/ -run TestJSONBaseline -v` -- all tests pass</verify>
  <done>JSON backward compatibility test validates v1 JSON loads all category-level fields correctly; no crash on old "metrics" field name</done>
</task>

</tasks>

<verification>
```bash
go test ./internal/scoring/ -run TestExtractEvidence -v
go test ./internal/output/ -run TestJSONBaseline -v
go test ./... 2>&1 | tail -20
```
All tests pass. No regressions in existing tests.
</verification>

<success_criteria>
- Evidence extraction tests cover all 7 categories (C1-C7)
- Tests verify non-nil evidence maps and non-empty evidence for metrics with violations
- JSON backward compatibility confirmed for v1 schema
- `go test ./...` passes with no failures
</success_criteria>

<output>
After completion, create `.planning/phases/34-testing-and-quality/34-01-SUMMARY.md`
</output>
